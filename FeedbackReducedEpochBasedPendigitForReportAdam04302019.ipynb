{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeedbackReducedEpochBasedPendigitForReportAdam04302019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "kNvs6e3JChkM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/FeedbackReducedEpochBasedPendigitForReportAdam04302019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mF78qe2MQaox"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "y83_Jy4wQOX2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6IE0zjgLQOYE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ugYW9NELQOYN",
        "outputId": "dc6065c2-2c41-47d4-84d5-ea5c9944f6cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "train_data_pandas = pd.DataFrame(train_data)\n",
        "train_data_labels = pd.DataFrame(train_label)\n",
        "train_data_pandas.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.949582</td>\n",
              "      <td>-4.286268</td>\n",
              "      <td>-1.618116</td>\n",
              "      <td>-5.563660</td>\n",
              "      <td>-0.528531</td>\n",
              "      <td>-2.788612</td>\n",
              "      <td>-2.153607</td>\n",
              "      <td>2.321852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.895125</td>\n",
              "      <td>1.576207</td>\n",
              "      <td>-2.783810</td>\n",
              "      <td>7.369057</td>\n",
              "      <td>-0.803854</td>\n",
              "      <td>4.184283</td>\n",
              "      <td>0.250293</td>\n",
              "      <td>2.148146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.116622</td>\n",
              "      <td>-5.365335</td>\n",
              "      <td>-2.624452</td>\n",
              "      <td>-4.229593</td>\n",
              "      <td>-2.036412</td>\n",
              "      <td>-2.417719</td>\n",
              "      <td>-1.506112</td>\n",
              "      <td>3.154646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.294292</td>\n",
              "      <td>4.448064</td>\n",
              "      <td>1.300888</td>\n",
              "      <td>-0.489738</td>\n",
              "      <td>0.710722</td>\n",
              "      <td>-2.002546</td>\n",
              "      <td>-5.218903</td>\n",
              "      <td>-0.062628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.775319</td>\n",
              "      <td>2.801049</td>\n",
              "      <td>-0.430359</td>\n",
              "      <td>-6.618764</td>\n",
              "      <td>1.370674</td>\n",
              "      <td>-0.696267</td>\n",
              "      <td>-0.438141</td>\n",
              "      <td>-1.096400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3         4         5         6  \\\n",
              "0   8.949582 -4.286268 -1.618116 -5.563660 -0.528531 -2.788612 -2.153607   \n",
              "1   7.895125  1.576207 -2.783810  7.369057 -0.803854  4.184283  0.250293   \n",
              "2   9.116622 -5.365335 -2.624452 -4.229593 -2.036412 -2.417719 -1.506112   \n",
              "3  10.294292  4.448064  1.300888 -0.489738  0.710722 -2.002546 -5.218903   \n",
              "4  10.775319  2.801049 -0.430359 -6.618764  1.370674 -0.696267 -0.438141   \n",
              "\n",
              "          7  \n",
              "0  2.321852  \n",
              "1  2.148146  \n",
              "2  3.154646  \n",
              "3 -0.062628  \n",
              "4 -1.096400  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2AhK8UMpQOYc",
        "outputId": "43d10035-d8f3-49c3-ba97-8459da473cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5995, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fliwpya0QOYm"
      },
      "cell_type": "markdown",
      "source": [
        "#### Combine Validation and train data for MLP classifier - and set validation fraction to 4500/15000 = 0.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GWZy2-9oQOYo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2_xHZA9HQOYw",
        "outputId": "d888f61c-45ec-4892-c83d-c877b4d93993",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4403
        }
      },
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(40, 20, ), max_iter=500, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "\n",
        "\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.12229186\n",
            "Iteration 2, loss = 0.27630817\n",
            "Iteration 3, loss = 0.17940873\n",
            "Iteration 4, loss = 0.14870875\n",
            "Iteration 5, loss = 0.13422307\n",
            "Iteration 6, loss = 0.12491732\n",
            "Iteration 7, loss = 0.11561283\n",
            "Iteration 8, loss = 0.10954788\n",
            "Iteration 9, loss = 0.10444853\n",
            "Iteration 10, loss = 0.09997343\n",
            "Iteration 11, loss = 0.09542991\n",
            "Iteration 12, loss = 0.09315024\n",
            "Iteration 13, loss = 0.09086000\n",
            "Iteration 14, loss = 0.08797955\n",
            "Iteration 15, loss = 0.08514271\n",
            "Iteration 16, loss = 0.08348284\n",
            "Iteration 17, loss = 0.08288803\n",
            "Iteration 18, loss = 0.08186669\n",
            "Iteration 19, loss = 0.07878308\n",
            "Iteration 20, loss = 0.07808383\n",
            "Iteration 21, loss = 0.07576302\n",
            "Iteration 22, loss = 0.07428644\n",
            "Iteration 23, loss = 0.07352276\n",
            "Iteration 24, loss = 0.07310997\n",
            "Iteration 25, loss = 0.07200573\n",
            "Iteration 26, loss = 0.07080075\n",
            "Iteration 27, loss = 0.06966033\n",
            "Iteration 28, loss = 0.06799473\n",
            "Iteration 29, loss = 0.06854212\n",
            "Iteration 30, loss = 0.06804438\n",
            "Iteration 31, loss = 0.06642355\n",
            "Iteration 32, loss = 0.06480043\n",
            "Iteration 33, loss = 0.06382451\n",
            "Iteration 34, loss = 0.06395037\n",
            "Iteration 35, loss = 0.06303426\n",
            "Iteration 36, loss = 0.06253847\n",
            "Iteration 37, loss = 0.06187680\n",
            "Iteration 38, loss = 0.06203627\n",
            "Iteration 39, loss = 0.06096575\n",
            "Iteration 40, loss = 0.05912287\n",
            "Iteration 41, loss = 0.05871318\n",
            "Iteration 42, loss = 0.05839958\n",
            "Iteration 43, loss = 0.05873458\n",
            "Iteration 44, loss = 0.05841894\n",
            "Iteration 45, loss = 0.05703795\n",
            "Iteration 46, loss = 0.05711054\n",
            "Iteration 47, loss = 0.05628409\n",
            "Iteration 48, loss = 0.05574221\n",
            "Iteration 49, loss = 0.05569507\n",
            "Iteration 50, loss = 0.05493803\n",
            "Iteration 51, loss = 0.05378797\n",
            "Iteration 52, loss = 0.05353499\n",
            "Iteration 53, loss = 0.05476908\n",
            "Iteration 54, loss = 0.05238795\n",
            "Iteration 55, loss = 0.05293164\n",
            "Iteration 56, loss = 0.05355975\n",
            "Iteration 57, loss = 0.05224996\n",
            "Iteration 58, loss = 0.05271694\n",
            "Iteration 59, loss = 0.05160475\n",
            "Iteration 60, loss = 0.05206218\n",
            "Iteration 61, loss = 0.05048909\n",
            "Iteration 62, loss = 0.04961835\n",
            "Iteration 63, loss = 0.05083926\n",
            "Iteration 64, loss = 0.05034288\n",
            "Iteration 65, loss = 0.05162862\n",
            "Iteration 66, loss = 0.05090763\n",
            "Iteration 67, loss = 0.05057164\n",
            "Iteration 68, loss = 0.04880508\n",
            "Iteration 69, loss = 0.04904307\n",
            "Iteration 70, loss = 0.04928409\n",
            "Iteration 71, loss = 0.04789816\n",
            "Iteration 72, loss = 0.04898382\n",
            "Iteration 73, loss = 0.04876359\n",
            "Iteration 74, loss = 0.04953415\n",
            "Iteration 75, loss = 0.04775492\n",
            "Iteration 76, loss = 0.04749391\n",
            "Iteration 77, loss = 0.04780711\n",
            "Iteration 78, loss = 0.04810568\n",
            "Iteration 79, loss = 0.04646764\n",
            "Iteration 80, loss = 0.04810747\n",
            "Iteration 81, loss = 0.04716222\n",
            "Iteration 82, loss = 0.04676504\n",
            "Iteration 83, loss = 0.04679679\n",
            "Iteration 84, loss = 0.04667366\n",
            "Iteration 85, loss = 0.04543031\n",
            "Iteration 86, loss = 0.04776637\n",
            "Iteration 87, loss = 0.04588804\n",
            "Iteration 88, loss = 0.04496605\n",
            "Iteration 89, loss = 0.04587642\n",
            "Iteration 90, loss = 0.04407799\n",
            "Iteration 91, loss = 0.04475859\n",
            "Iteration 92, loss = 0.04465375\n",
            "Iteration 93, loss = 0.04509490\n",
            "Iteration 94, loss = 0.04407378\n",
            "Iteration 95, loss = 0.04421696\n",
            "Iteration 96, loss = 0.04520939\n",
            "Iteration 97, loss = 0.04351129\n",
            "Iteration 98, loss = 0.04478113\n",
            "Iteration 99, loss = 0.04514323\n",
            "Iteration 100, loss = 0.04494149\n",
            "Iteration 101, loss = 0.04711513\n",
            "Iteration 102, loss = 0.04368219\n",
            "Iteration 103, loss = 0.04302941\n",
            "Iteration 104, loss = 0.04313014\n",
            "Iteration 105, loss = 0.04499407\n",
            "Iteration 106, loss = 0.04263661\n",
            "Iteration 107, loss = 0.04297407\n",
            "Iteration 108, loss = 0.04255955\n",
            "Iteration 109, loss = 0.04308485\n",
            "Iteration 110, loss = 0.04546104\n",
            "Iteration 111, loss = 0.04228518\n",
            "Iteration 112, loss = 0.04163204\n",
            "Iteration 113, loss = 0.04165039\n",
            "Iteration 114, loss = 0.04156784\n",
            "Iteration 115, loss = 0.04140168\n",
            "Iteration 116, loss = 0.04134439\n",
            "Iteration 117, loss = 0.04327633\n",
            "Iteration 118, loss = 0.04109495\n",
            "Iteration 119, loss = 0.04173301\n",
            "Iteration 120, loss = 0.04153089\n",
            "Iteration 121, loss = 0.04149234\n",
            "Iteration 122, loss = 0.04079013\n",
            "Iteration 123, loss = 0.04301234\n",
            "Iteration 124, loss = 0.04181132\n",
            "Iteration 125, loss = 0.04177525\n",
            "Iteration 126, loss = 0.04241237\n",
            "Iteration 127, loss = 0.04054492\n",
            "Iteration 128, loss = 0.04093582\n",
            "Iteration 129, loss = 0.04000364\n",
            "Iteration 130, loss = 0.04042453\n",
            "Iteration 131, loss = 0.04171994\n",
            "Iteration 132, loss = 0.04016958\n",
            "Iteration 133, loss = 0.04018262\n",
            "Iteration 134, loss = 0.04134928\n",
            "Iteration 135, loss = 0.04015017\n",
            "Iteration 136, loss = 0.04017360\n",
            "Iteration 137, loss = 0.04137593\n",
            "Iteration 138, loss = 0.04152794\n",
            "Iteration 139, loss = 0.03963120\n",
            "Iteration 140, loss = 0.04260000\n",
            "Iteration 141, loss = 0.04003934\n",
            "Iteration 142, loss = 0.04009419\n",
            "Iteration 143, loss = 0.03963225\n",
            "Iteration 144, loss = 0.04007301\n",
            "Iteration 145, loss = 0.03890774\n",
            "Iteration 146, loss = 0.04068296\n",
            "Iteration 147, loss = 0.03943695\n",
            "Iteration 148, loss = 0.04028893\n",
            "Iteration 149, loss = 0.03883549\n",
            "Iteration 150, loss = 0.03939371\n",
            "Iteration 151, loss = 0.03958656\n",
            "Iteration 152, loss = 0.03839421\n",
            "Iteration 153, loss = 0.03820342\n",
            "Iteration 154, loss = 0.03967179\n",
            "Iteration 155, loss = 0.03818721\n",
            "Iteration 156, loss = 0.03878901\n",
            "Iteration 157, loss = 0.03752287\n",
            "Iteration 158, loss = 0.03953329\n",
            "Iteration 159, loss = 0.03962806\n",
            "Iteration 160, loss = 0.03861998\n",
            "Iteration 161, loss = 0.03782372\n",
            "Iteration 162, loss = 0.03946599\n",
            "Iteration 163, loss = 0.03783930\n",
            "Iteration 164, loss = 0.03691389\n",
            "Iteration 165, loss = 0.03734587\n",
            "Iteration 166, loss = 0.03736659\n",
            "Iteration 167, loss = 0.03762873\n",
            "Iteration 168, loss = 0.03837839\n",
            "Iteration 169, loss = 0.03707094\n",
            "Iteration 170, loss = 0.03704182\n",
            "Iteration 171, loss = 0.03665978\n",
            "Iteration 172, loss = 0.03690557\n",
            "Iteration 173, loss = 0.03873433\n",
            "Iteration 174, loss = 0.03774919\n",
            "Iteration 175, loss = 0.03603050\n",
            "Iteration 176, loss = 0.03780329\n",
            "Iteration 177, loss = 0.03670278\n",
            "Iteration 178, loss = 0.03803341\n",
            "Iteration 179, loss = 0.03663487\n",
            "Iteration 180, loss = 0.03644912\n",
            "Iteration 181, loss = 0.03677084\n",
            "Iteration 182, loss = 0.03647322\n",
            "Iteration 183, loss = 0.03673921\n",
            "Iteration 184, loss = 0.03667555\n",
            "Iteration 185, loss = 0.03601972\n",
            "Iteration 186, loss = 0.03701488\n",
            "Iteration 187, loss = 0.03614083\n",
            "Iteration 188, loss = 0.03614320\n",
            "Iteration 189, loss = 0.03643272\n",
            "Iteration 190, loss = 0.03581577\n",
            "Iteration 191, loss = 0.03555400\n",
            "Iteration 192, loss = 0.03691407\n",
            "Iteration 193, loss = 0.03635127\n",
            "Iteration 194, loss = 0.03513516\n",
            "Iteration 195, loss = 0.03748860\n",
            "Iteration 196, loss = 0.03537683\n",
            "Iteration 197, loss = 0.03697657\n",
            "Iteration 198, loss = 0.03728009\n",
            "Iteration 199, loss = 0.03608641\n",
            "Iteration 200, loss = 0.03555540\n",
            "Iteration 201, loss = 0.03586443\n",
            "Iteration 202, loss = 0.03496716\n",
            "Iteration 203, loss = 0.03539961\n",
            "Iteration 204, loss = 0.03486847\n",
            "Iteration 205, loss = 0.03519890\n",
            "Iteration 206, loss = 0.03533117\n",
            "Iteration 207, loss = 0.03394689\n",
            "Iteration 208, loss = 0.03451001\n",
            "Iteration 209, loss = 0.03657145\n",
            "Iteration 210, loss = 0.03471450\n",
            "Iteration 211, loss = 0.03629280\n",
            "Iteration 212, loss = 0.03725743\n",
            "Iteration 213, loss = 0.03418068\n",
            "Iteration 214, loss = 0.03592074\n",
            "Iteration 215, loss = 0.03680712\n",
            "Iteration 216, loss = 0.03429884\n",
            "Iteration 217, loss = 0.03370677\n",
            "Iteration 218, loss = 0.03432734\n",
            "Iteration 219, loss = 0.03696363\n",
            "Iteration 220, loss = 0.03493227\n",
            "Iteration 221, loss = 0.03579457\n",
            "Iteration 222, loss = 0.03442442\n",
            "Iteration 223, loss = 0.03416824\n",
            "Iteration 224, loss = 0.03426877\n",
            "Iteration 225, loss = 0.03425624\n",
            "Iteration 226, loss = 0.03495616\n",
            "Iteration 227, loss = 0.03409632\n",
            "Iteration 228, loss = 0.03332544\n",
            "Iteration 229, loss = 0.03425785\n",
            "Iteration 230, loss = 0.03291441\n",
            "Iteration 231, loss = 0.03494392\n",
            "Iteration 232, loss = 0.03519032\n",
            "Iteration 233, loss = 0.03319317\n",
            "Iteration 234, loss = 0.03332386\n",
            "Iteration 235, loss = 0.03430392\n",
            "Iteration 236, loss = 0.03603113\n",
            "Iteration 237, loss = 0.03380661\n",
            "Iteration 238, loss = 0.03396583\n",
            "Iteration 239, loss = 0.03260597\n",
            "Iteration 240, loss = 0.03425424\n",
            "Iteration 241, loss = 0.03306291\n",
            "Iteration 242, loss = 0.03283005\n",
            "Iteration 243, loss = 0.03376364\n",
            "Iteration 244, loss = 0.03440181\n",
            "Iteration 245, loss = 0.03313323\n",
            "Iteration 246, loss = 0.03395346\n",
            "Iteration 247, loss = 0.03350116\n",
            "Iteration 248, loss = 0.03349362\n",
            "Iteration 249, loss = 0.03347422\n",
            "Iteration 250, loss = 0.03402542\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(40, 20), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DchLz1PNQOZF"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yvvp0-twQOZH",
        "outputId": "7bd27252-db59-45e3-e854-0506513e6175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9896580483736447"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ME0XH5iJQOZV",
        "outputId": "507830ad-0189-44e7-9e7b-ce5ab7fd995a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Validation Accuracy/\n",
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9839893262174784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qQBlnhMCQOZi",
        "outputId": "3dc136be-9ab5-4b65-e9ad-ffd54980a570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Test accuracy\n",
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9748427672955975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UvcdQCMhQOZo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rvCVtPq4QOZu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_tzPHDX8QOZ2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sDaagC0gQOZ9",
        "outputId": "3e04afac-1036-4a0e-ea4e-9d1152e387ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z-j8e4a-ypY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Rerun the same thing in tensorflow"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_mPgmrt2QOaG",
        "outputId": "8eaf91a1-b7b1-4b06-deae-38e139adaadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Baseline\n",
        "saver = tf.train.Saver()\n",
        "learning_rate = 0.001\n",
        "num_steps = 10000\n",
        "batch_size = 200\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    return out_layer\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X), labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X), 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "  ### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % 1000 == 0:\n",
        "            train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "            print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "            train_losses.append(train_loss)\n",
        "            validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "            if step%1000 == 0:\n",
        "              print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "              print()\n",
        "              if (validation_accuracy >= best_accuracy_valid):\n",
        "                best_accuracy_valid = validation_accuracy\n",
        "                saver.save(sess, './statlog_letter')\n",
        "                test_Accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_Accuracy), \"%\")\n",
        "    print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "\n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-14-5df073d8ab8d>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step 0, training loss= 0.0007878761, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 1000, training loss= 0.00027250333, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 2000, training loss= 0.00087376055, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 3000, training loss= 6.208709e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 4000, training loss= 3.6624704e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 5000, training loss= 0.00016827443, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 6000, training loss= 0.0021800278, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7000, training loss= 0.00023414326, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 8000, training loss= 0.0017441899, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9000, training loss= 1.8004113e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "Test acc= 97.57004 %\n",
            "Valid acc= 99.59973 %\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D56WT7Q7ypZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Divide valid in two parts for validation and validation-test"
      ]
    },
    {
      "metadata": {
        "id": "xQDivBFuypZH",
        "colab_type": "code",
        "outputId": "c6670581-acc5-4caf-d762-9b6a3099f95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1499, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "NnuxpRfoypZM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data = validation_data[0:1000,:]\n",
        "valid_validation_data_label = validation_label_one_hot[0:1000,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fz6cKjLHypZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_test_data = validation_data[1000:,:]\n",
        "valid_test_data_label = validation_label_one_hot[1000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5uoFbH73ypZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define weights for modified architecture"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4ON1VfM1QOaT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "G_w_out_h1 = tf.Variable(xavier_init([10,40]))\n",
        "G_b_out_h1 = tf.Variable(xavier_init([40]))\n",
        "\n",
        "G_w_h2_h1 = tf.Variable(xavier_init([20,40]))\n",
        "G_b_h2_h1 = tf.Variable(xavier_init([40]))\n",
        "\n",
        "\n",
        "G_w_h1_input = tf.Variable(xavier_init([40,8]))\n",
        "G_b_h1_input = tf.Variable(xavier_init([8]))\n",
        "\n",
        "\n",
        "G_w_input_h1_h2 = tf.Variable(xavier_init([8,20]))\n",
        "G_b_h1_input = tf.Variable(xavier_init([20]))\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pInVCW51CHfd",
        "colab_type": "code",
        "outputId": "013fb98c-1c3a-4d8f-e559-1441c2e14f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5995, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "rEu2QGtPCRaQ",
        "colab_type": "code",
        "outputId": "a4cdccd5-c3f4-47bd-ab0d-f84404c74335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "5995/200"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "dWMPlIkQCTKk",
        "colab_type": "code",
        "outputId": "a909f19d-14db-4c1b-e120-cf61e3f67889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "100000/30"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3333.3333333333335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "94L9z8xkCVZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 3333"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UZqq2BfgLuhu"
      },
      "cell_type": "markdown",
      "source": [
        "## Best Tuned, Use W1 = 3, W2 = 1, W3 = 1 from best validation accuracy found below"
      ]
    },
    {
      "metadata": {
        "id": "3QHfYBjFEMAq",
        "colab_type": "code",
        "outputId": "8b2a58fa-b3ac-4dcb-8e64-f55b4a77ac8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5995, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "g08MS9ToEUZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X_train = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zqxeiID0FiAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle  #train_data, train_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOSx-8PQChwv",
        "colab_type": "code",
        "outputId": "77656111-dce4-43dc-b905-a5f0b7f55c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17187
        }
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "hid_neuron = [104]\n",
        "num_steps = 100000\n",
        "batch_size = 200\n",
        "BATCH_SIZE = batch_size\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 10\n",
        "\n",
        "\n",
        "###\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = train_data.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "epoch_track = []\n",
        "###\n",
        "learning_rate = 0.001\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "wLoss1 = 3\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "\n",
        "#############\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for i in range(EPOCHS):\n",
        "      X_train, y_train = shuffle(train_data, train_label_one_hot)\n",
        "      \n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "            step = 0\n",
        "          else:\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "          sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "      if i % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: X_train,Y: y_train})\n",
        "          train_accuracy.append(train_acc)\n",
        "          print(\"Epoch \" + str(i) + '/' + str(EPOCHS) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          train_losses.append(train_loss)\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "          val_accuracy.append(validation_accuracy)\n",
        "          if step%plot_every == 0:\n",
        "            print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "            print()\n",
        "            if (validation_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validation_accuracy\n",
        "              saver.save(sess, './statlog_letter2')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np, G_W3np, G_b3np = sess.run([G_W1, G_b1, G_W2, G_b2, G_W3, G_b3])\n",
        "    print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "    ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/3333, training loss= 1.3385851, training acc= 93.728107213974%\n",
            "Validation Accuracy 94.1294174194336 ...\n",
            "\n",
            "Epoch 10/3333, training loss= 0.0832698, training acc= 98.51542711257935%\n",
            "Validation Accuracy 97.93196105957031 ...\n",
            "\n",
            "Epoch 20/3333, training loss= 0.05007368, training acc= 98.7322747707367%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "Epoch 30/3333, training loss= 0.041123293, training acc= 98.83236289024353%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 40/3333, training loss= 0.036575433, training acc= 98.86572360992432%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 50/3333, training loss= 0.03325339, training acc= 98.98248314857483%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 60/3333, training loss= 0.03164795, training acc= 98.94912242889404%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 70/3333, training loss= 0.029906044, training acc= 98.99916648864746%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 80/3333, training loss= 0.02966851, training acc= 98.96580576896667%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 90/3333, training loss= 0.028324852, training acc= 98.96580576896667%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 100/3333, training loss= 0.028128812, training acc= 99.01584386825562%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 110/3333, training loss= 0.026767364, training acc= 98.96580576896667%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 120/3333, training loss= 0.025764853, training acc= 99.01584386825562%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 130/3333, training loss= 0.025992123, training acc= 99.01584386825562%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 140/3333, training loss= 0.024855591, training acc= 99.01584386825562%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 150/3333, training loss= 0.023894688, training acc= 99.01584386825562%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 160/3333, training loss= 0.023709072, training acc= 99.08257126808167%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 170/3333, training loss= 0.024059437, training acc= 99.03252720832825%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 180/3333, training loss= 0.023441402, training acc= 99.08257126808167%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 190/3333, training loss= 0.02181137, training acc= 99.08257126808167%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 200/3333, training loss= 0.022107646, training acc= 99.09924864768982%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 210/3333, training loss= 0.02084186, training acc= 99.16597008705139%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 220/3333, training loss= 0.020692922, training acc= 99.14929270744324%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 230/3333, training loss= 0.020718567, training acc= 99.11593198776245%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 240/3333, training loss= 0.019699002, training acc= 99.18265342712402%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 250/3333, training loss= 0.01923389, training acc= 99.1326093673706%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 260/3333, training loss= 0.019143451, training acc= 99.16597008705139%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 270/3333, training loss= 0.019021587, training acc= 99.11593198776245%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 280/3333, training loss= 0.018610734, training acc= 99.18265342712402%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 290/3333, training loss= 0.018195242, training acc= 99.21601414680481%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 300/3333, training loss= 0.01814877, training acc= 99.31609630584717%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 310/3333, training loss= 0.01768577, training acc= 99.29941892623901%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 320/3333, training loss= 0.017392538, training acc= 99.28273558616638%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 330/3333, training loss= 0.016853688, training acc= 99.3327796459198%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 340/3333, training loss= 0.01737455, training acc= 99.3327796459198%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 350/3333, training loss= 0.016617259, training acc= 99.36614036560059%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 360/3333, training loss= 0.01641588, training acc= 99.29941892623901%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 370/3333, training loss= 0.017773688, training acc= 99.28273558616638%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 380/3333, training loss= 0.016020926, training acc= 99.3327796459198%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 390/3333, training loss= 0.01582293, training acc= 99.29941892623901%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 400/3333, training loss= 0.015093557, training acc= 99.34945702552795%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 410/3333, training loss= 0.016139401, training acc= 99.3327796459198%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 420/3333, training loss= 0.015339876, training acc= 99.36614036560059%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 430/3333, training loss= 0.015942825, training acc= 99.31609630584717%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 440/3333, training loss= 0.014683652, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 450/3333, training loss= 0.01454823, training acc= 99.36614036560059%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 460/3333, training loss= 0.015027995, training acc= 99.36614036560059%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 470/3333, training loss= 0.014894507, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 480/3333, training loss= 0.014303249, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 490/3333, training loss= 0.014480925, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 500/3333, training loss= 0.014048055, training acc= 99.39950108528137%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 510/3333, training loss= 0.014078692, training acc= 99.39950108528137%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 520/3333, training loss= 0.013722693, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 530/3333, training loss= 0.013996228, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 540/3333, training loss= 0.014083452, training acc= 99.39950108528137%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 550/3333, training loss= 0.013741776, training acc= 99.36614036560059%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 560/3333, training loss= 0.013612228, training acc= 99.36614036560059%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 570/3333, training loss= 0.013198102, training acc= 99.39950108528137%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 580/3333, training loss= 0.013276179, training acc= 99.3327796459198%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 590/3333, training loss= 0.013082831, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 600/3333, training loss= 0.013024012, training acc= 99.34945702552795%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "Epoch 610/3333, training loss= 0.01854703, training acc= 99.34945702552795%\n",
            "Validation Accuracy 98.93262481689453 ...\n",
            "\n",
            "Epoch 620/3333, training loss= 0.013392791, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 630/3333, training loss= 0.013345889, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 640/3333, training loss= 0.012672213, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 650/3333, training loss= 0.012876043, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 660/3333, training loss= 0.012609703, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 670/3333, training loss= 0.012782641, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 680/3333, training loss= 0.012588999, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 690/3333, training loss= 0.012296371, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 700/3333, training loss= 0.012683391, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 710/3333, training loss= 0.012778865, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 720/3333, training loss= 0.012894668, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 730/3333, training loss= 0.019314988, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 740/3333, training loss= 0.013057955, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 750/3333, training loss= 0.012122118, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 760/3333, training loss= 0.011914722, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 770/3333, training loss= 0.012350534, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 780/3333, training loss= 0.012186273, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 790/3333, training loss= 0.012109637, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 800/3333, training loss= 0.011976785, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 810/3333, training loss= 0.012173596, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 820/3333, training loss= 0.012270901, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 830/3333, training loss= 0.011812487, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 840/3333, training loss= 0.012027848, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 850/3333, training loss= 0.013217479, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 860/3333, training loss= 0.011676463, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 870/3333, training loss= 0.011889095, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 880/3333, training loss= 0.011648316, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 890/3333, training loss= 0.011658794, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 900/3333, training loss= 0.011682702, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 910/3333, training loss= 0.011648997, training acc= 99.39950108528137%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 920/3333, training loss= 0.011477349, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 930/3333, training loss= 0.011817465, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 940/3333, training loss= 0.01140283, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 950/3333, training loss= 0.011673366, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 960/3333, training loss= 0.011316824, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 970/3333, training loss= 0.011368482, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 980/3333, training loss= 0.015699299, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 990/3333, training loss= 0.0112354625, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 1000/3333, training loss= 0.011883106, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1010/3333, training loss= 0.011769926, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1020/3333, training loss= 0.012569052, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1030/3333, training loss= 0.011234059, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1040/3333, training loss= 0.011214858, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1050/3333, training loss= 0.011199508, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1060/3333, training loss= 0.011568769, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 1070/3333, training loss= 0.01592827, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1080/3333, training loss= 0.011104783, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1090/3333, training loss= 0.011040983, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1100/3333, training loss= 0.011107093, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1110/3333, training loss= 0.01313819, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 1120/3333, training loss= 0.011095873, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1130/3333, training loss= 0.011053403, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 1140/3333, training loss= 0.011413707, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 1150/3333, training loss= 0.011371689, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 1160/3333, training loss= 0.010990857, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1170/3333, training loss= 0.011418377, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1180/3333, training loss= 0.0111358585, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 1190/3333, training loss= 0.010833065, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1200/3333, training loss= 0.011282145, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1210/3333, training loss= 0.010840931, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1220/3333, training loss= 0.010798137, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1230/3333, training loss= 0.010689264, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1240/3333, training loss= 0.010978962, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1250/3333, training loss= 0.01113374, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1260/3333, training loss= 0.010722555, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 1270/3333, training loss= 0.013477184, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1280/3333, training loss= 0.010751906, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 1290/3333, training loss= 0.010904625, training acc= 99.41617846488953%\n",
            "Validation Accuracy 99.0660400390625 ...\n",
            "\n",
            "Epoch 1300/3333, training loss= 0.0106563885, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 1310/3333, training loss= 0.010823238, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1320/3333, training loss= 0.010873047, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1330/3333, training loss= 0.011155329, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 1340/3333, training loss= 0.010785363, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1350/3333, training loss= 0.010668749, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1360/3333, training loss= 0.011190473, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1370/3333, training loss= 0.010791554, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1380/3333, training loss= 0.010642779, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1390/3333, training loss= 0.01115685, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1400/3333, training loss= 0.010665808, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 1410/3333, training loss= 0.010741712, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1420/3333, training loss= 0.010490955, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 1430/3333, training loss= 0.011030326, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1440/3333, training loss= 0.010681066, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.19879913330078 ...\n",
            "\n",
            "Epoch 1450/3333, training loss= 0.010761655, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1460/3333, training loss= 0.01046124, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1470/3333, training loss= 0.022548867, training acc= 99.2493748664856%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "Epoch 1480/3333, training loss= 0.010855021, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1490/3333, training loss= 0.010870115, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1500/3333, training loss= 0.010783309, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1510/3333, training loss= 0.010985494, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1520/3333, training loss= 0.010826656, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1530/3333, training loss= 0.011237028, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1540/3333, training loss= 0.010923587, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1550/3333, training loss= 0.011332486, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1560/3333, training loss= 0.011049601, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1570/3333, training loss= 0.010730675, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 1580/3333, training loss= 0.010561547, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1590/3333, training loss= 0.010754967, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1600/3333, training loss= 0.010613813, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1610/3333, training loss= 0.010713211, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1620/3333, training loss= 0.010753746, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1630/3333, training loss= 0.010397921, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1640/3333, training loss= 0.010746928, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 1650/3333, training loss= 0.010560373, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1660/3333, training loss= 0.010650414, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1670/3333, training loss= 0.010545671, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.93262481689453 ...\n",
            "\n",
            "Epoch 1680/3333, training loss= 0.010537341, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 1690/3333, training loss= 0.010425832, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1700/3333, training loss= 0.010696279, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 1710/3333, training loss= 0.010665038, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1720/3333, training loss= 0.0104388865, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 1730/3333, training loss= 0.010387815, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1740/3333, training loss= 0.010436739, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1750/3333, training loss= 0.010385201, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1760/3333, training loss= 0.010641134, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1770/3333, training loss= 0.010386354, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1780/3333, training loss= 0.010711135, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1790/3333, training loss= 0.010354097, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1800/3333, training loss= 0.010745683, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1810/3333, training loss= 0.010442527, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 1820/3333, training loss= 0.0102966, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1830/3333, training loss= 0.01017432, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1840/3333, training loss= 0.010341551, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 1850/3333, training loss= 0.011345203, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.19879913330078 ...\n",
            "\n",
            "Epoch 1860/3333, training loss= 0.010362191, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1870/3333, training loss= 0.01031139, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 1880/3333, training loss= 0.010190724, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1890/3333, training loss= 0.010980936, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.19879913330078 ...\n",
            "\n",
            "Epoch 1900/3333, training loss= 0.010153932, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1910/3333, training loss= 0.010145112, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 1920/3333, training loss= 0.010079567, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1930/3333, training loss= 0.010080131, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1940/3333, training loss= 0.010417751, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 1950/3333, training loss= 0.01180161, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 1960/3333, training loss= 0.01193658, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1970/3333, training loss= 0.011127539, training acc= 99.39950108528137%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 1980/3333, training loss= 0.0108533, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 1990/3333, training loss= 0.010831616, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2000/3333, training loss= 0.012386389, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2010/3333, training loss= 0.010803654, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 2020/3333, training loss= 0.011098705, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2030/3333, training loss= 0.01078538, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 2040/3333, training loss= 0.010648948, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2050/3333, training loss= 0.010580591, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2060/3333, training loss= 0.049177174, training acc= 99.38281774520874%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2070/3333, training loss= 0.0105592795, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2080/3333, training loss= 0.01061655, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2090/3333, training loss= 0.01047157, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2100/3333, training loss= 0.010510474, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2110/3333, training loss= 0.0105154095, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2120/3333, training loss= 0.010411038, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2130/3333, training loss= 0.010098655, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2140/3333, training loss= 0.0105571775, training acc= 99.46622252464294%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2150/3333, training loss= 0.010800611, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2160/3333, training loss= 0.010340812, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2170/3333, training loss= 0.010217517, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "Epoch 2180/3333, training loss= 0.010244601, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2190/3333, training loss= 0.010103041, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2200/3333, training loss= 0.009977689, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 2210/3333, training loss= 0.010135471, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2220/3333, training loss= 0.01002588, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2230/3333, training loss= 0.010328995, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2240/3333, training loss= 0.010143246, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2250/3333, training loss= 0.010446954, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 2260/3333, training loss= 0.010317875, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 2270/3333, training loss= 0.009863101, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2280/3333, training loss= 0.010410944, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "Epoch 2290/3333, training loss= 0.010963194, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2300/3333, training loss= 0.010388515, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 2310/3333, training loss= 0.010011965, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 2320/3333, training loss= 0.010042572, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 2330/3333, training loss= 0.010597387, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2340/3333, training loss= 0.010003994, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2350/3333, training loss= 0.009968724, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2360/3333, training loss= 0.010000492, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2370/3333, training loss= 0.010013822, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2380/3333, training loss= 0.010678962, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2390/3333, training loss= 0.010192331, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 2400/3333, training loss= 0.009908312, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 2410/3333, training loss= 0.010761628, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 2420/3333, training loss= 0.010380114, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 2430/3333, training loss= 0.010231582, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2440/3333, training loss= 0.010085796, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2450/3333, training loss= 0.010048244, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2460/3333, training loss= 0.010121047, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2470/3333, training loss= 0.0100412695, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2480/3333, training loss= 0.010133362, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 2490/3333, training loss= 0.0101492405, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2500/3333, training loss= 0.0103514725, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2510/3333, training loss= 0.010039578, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2520/3333, training loss= 0.010703375, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 2530/3333, training loss= 0.009961987, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.19879913330078 ...\n",
            "\n",
            "Epoch 2540/3333, training loss= 0.0106094135, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 2550/3333, training loss= 0.010036342, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2560/3333, training loss= 0.010261081, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 2570/3333, training loss= 0.010271034, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2580/3333, training loss= 0.010232027, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 2590/3333, training loss= 0.010281287, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 2600/3333, training loss= 0.010314582, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.19879913330078 ...\n",
            "\n",
            "Epoch 2610/3333, training loss= 0.010214748, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2620/3333, training loss= 0.011030809, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2630/3333, training loss= 0.010208527, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 2640/3333, training loss= 0.00989345, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2650/3333, training loss= 0.010059816, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2660/3333, training loss= 0.010055469, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2670/3333, training loss= 0.009922464, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2680/3333, training loss= 0.016002886, training acc= 99.44953918457031%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2690/3333, training loss= 0.010288841, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "Epoch 2700/3333, training loss= 0.010032591, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2710/3333, training loss= 0.0100169, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2720/3333, training loss= 0.0100609055, training acc= 99.49958324432373%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2730/3333, training loss= 0.009851132, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2740/3333, training loss= 0.010050661, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 2750/3333, training loss= 0.014995779, training acc= 99.41617846488953%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 2760/3333, training loss= 0.0099744415, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2770/3333, training loss= 0.009857709, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 2780/3333, training loss= 0.009869744, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2790/3333, training loss= 0.012376092, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2800/3333, training loss= 0.010057474, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 2810/3333, training loss= 0.009906487, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "Epoch 2820/3333, training loss= 0.010016322, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2830/3333, training loss= 0.00989253, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 2840/3333, training loss= 0.01165308, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2850/3333, training loss= 0.00997237, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2860/3333, training loss= 0.009972072, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2870/3333, training loss= 0.009959351, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2880/3333, training loss= 0.009945923, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2890/3333, training loss= 0.009745757, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 2900/3333, training loss= 0.009790803, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 2910/3333, training loss= 0.0098708365, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 2920/3333, training loss= 0.009948452, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2930/3333, training loss= 0.009935958, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2940/3333, training loss= 0.010063499, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "Epoch 2950/3333, training loss= 0.009947103, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 2960/3333, training loss= 0.009809896, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 2970/3333, training loss= 0.009865471, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2980/3333, training loss= 0.009907252, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 2990/3333, training loss= 0.010015563, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 3000/3333, training loss= 0.01012865, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.53235626220703 ...\n",
            "\n",
            "Epoch 3010/3333, training loss= 0.009759073, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3020/3333, training loss= 0.009747691, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3030/3333, training loss= 0.009796058, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 3040/3333, training loss= 0.009670551, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3050/3333, training loss= 0.010003241, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "Epoch 3060/3333, training loss= 0.009953876, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3070/3333, training loss= 0.010170258, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 3080/3333, training loss= 0.009843201, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3090/3333, training loss= 0.009769618, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3100/3333, training loss= 0.009883274, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3110/3333, training loss= 0.009832881, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3120/3333, training loss= 0.009875181, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3130/3333, training loss= 0.010267228, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3140/3333, training loss= 0.009783229, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3150/3333, training loss= 0.011041791, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3160/3333, training loss= 0.0110976575, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3170/3333, training loss= 0.009872812, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3180/3333, training loss= 0.010150055, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.19879913330078 ...\n",
            "\n",
            "Epoch 3190/3333, training loss= 0.009927562, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "Epoch 3200/3333, training loss= 0.0098387245, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.13208770751953 ...\n",
            "\n",
            "Epoch 3210/3333, training loss= 0.009739068, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3220/3333, training loss= 0.009973896, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3230/3333, training loss= 0.009814768, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3240/3333, training loss= 0.009804971, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.26551055908203 ...\n",
            "\n",
            "Epoch 3250/3333, training loss= 0.009959514, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.13208770751953 ...\n",
            "\n",
            "Epoch 3260/3333, training loss= 0.011168452, training acc= 99.43286180496216%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 3270/3333, training loss= 0.009801939, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 3280/3333, training loss= 0.0096272845, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 3290/3333, training loss= 0.009880959, training acc= 99.53294396400452%\n",
            "Validation Accuracy 98.46564483642578 ...\n",
            "\n",
            "Epoch 3300/3333, training loss= 0.016173061, training acc= 99.4828999042511%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3310/3333, training loss= 0.009847356, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3320/3333, training loss= 0.009870369, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Epoch 3330/3333, training loss= 0.009718145, training acc= 99.51626062393188%\n",
            "Validation Accuracy 98.33222198486328 ...\n",
            "\n",
            "Valid acc= 99.06604 %\n",
            "==================================================\n",
            "W1\n",
            "3\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e_t8dXEfJE-2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import spline\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a5c0qO5sVdQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "6c9f9837-7ea4-4eb4-940e-d3b041ae1c78"
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, 3331, 10)]\n",
        "plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "# plt.plot(steps_plot, savgol_filter(100*np.asarray(train_accuracy),11,1))  \n",
        "# plt.plot(steps_plot, savgol_filter(np.asarray(val_accuracy),11,1))\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train Accuracy','Validation Accuracy'])\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEmCAYAAAAOb7UzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MpSIPT1bJG8_",
        "colab_type": "code",
        "outputId": "a06a7ac8-0a9f-4dc3-d979-5e88da0ac9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, 3331, 10)]\n",
        "# plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "# plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "plt.plot(steps_plot, savgol_filter(100*np.asarray(train_accuracy),3,1))  \n",
        "plt.plot(steps_plot, savgol_filter(np.asarray(val_accuracy),3,1))\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train Accuracy','Validation Accuracy'])\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEmCAYAAAAOb7UzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "f2Gwyv-0JsW2",
        "colab_type": "code",
        "outputId": "4366a868-3059-4254-d82f-597ba7717237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "valid_accuracy_filtered = val_accuracy#savgol_filter(np.asarray(val_accuracy),51,1)\n",
        "print(max(valid_accuracy_filtered))\n",
        "valid_accuracy_filtered_np = np.asarray(valid_accuracy_filtered)\n",
        "print(np.argmax(valid_accuracy_filtered))\n",
        "print(steps_plot[np.argmax(valid_accuracy_filtered)])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.06604\n",
            "129\n",
            "1290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uW8d_LNXKBSE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now retrain till 1290 epoch with complete data"
      ]
    },
    {
      "metadata": {
        "id": "Ef0_rfG0KFb4",
        "colab_type": "code",
        "outputId": "dfa4ce82-cf0e-4ad4-a47e-13c7de5babaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_label_one_hot.shape)\n",
        "print(train_valid_combined.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5995, 10)\n",
            "(7494, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fs1yco26KJnx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 1\n",
        "aside_valid_test = train_valid_combined[-aside_examples:]\n",
        "aside_valid_test_label = validation_test_label_one_hot[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = validation_test_label_one_hot[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OONINtwVKUrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 1290"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OHS9YzgMKNFY",
        "colab_type": "code",
        "outputId": "9137afb6-29a6-4792-d73b-2c7be2f64fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6732
        }
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "hid_neuron = [104]\n",
        "num_steps = 100000\n",
        "batch_size = 200\n",
        "BATCH_SIZE = batch_size\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 10\n",
        "\n",
        "\n",
        "###\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "epoch_track = []\n",
        "###\n",
        "learning_rate = 0.001\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "wLoss1 = 3\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "\n",
        "#############\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for i in range(EPOCHS):\n",
        "      X_train, y_train = shuffle(combined_train_valid, combined_train_valid_label)\n",
        "      \n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "            step = 0\n",
        "          else:\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "          sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "      if i % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: X_train,Y: y_train})\n",
        "          train_accuracy.append(train_acc)\n",
        "          print(\"Epoch \" + str(i) + '/' + str(EPOCHS) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          train_losses.append(train_loss)\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "          val_accuracy.append(validation_accuracy)\n",
        "          if step%plot_every == 0:\n",
        "            print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "            print()\n",
        "            if (validation_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validation_accuracy\n",
        "    saver.save(sess, './PenDigitFinalReduced')\n",
        "    G_W1np, G_b1np, G_W2np, G_b2np, G_W3np, G_b3np = sess.run([G_W1, G_b1, G_W2, G_b2, G_W3, G_b3])\n",
        "    print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "    ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1290, training loss= 0.74340725, training acc= 94.247967004776%\n",
            "Validation Accuracy 94.9299545288086 ...\n",
            "\n",
            "Epoch 10/1290, training loss= 0.07485091, training acc= 98.55865240097046%\n",
            "Validation Accuracy 98.66577911376953 ...\n",
            "\n",
            "Epoch 20/1290, training loss= 0.047829717, training acc= 98.75884056091309%\n",
            "Validation Accuracy 98.93262481689453 ...\n",
            "\n",
            "Epoch 30/1290, training loss= 0.03938057, training acc= 98.90564680099487%\n",
            "Validation Accuracy 99.0660400390625 ...\n",
            "\n",
            "Epoch 40/1290, training loss= 0.03586432, training acc= 98.91899228096008%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 50/1290, training loss= 0.034051318, training acc= 98.85226488113403%\n",
            "Validation Accuracy 99.0660400390625 ...\n",
            "\n",
            "Epoch 60/1290, training loss= 0.031557813, training acc= 98.86561036109924%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "Epoch 70/1290, training loss= 0.030412475, training acc= 98.95902872085571%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "Epoch 80/1290, training loss= 0.028961232, training acc= 98.89230132102966%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "Epoch 90/1290, training loss= 0.027152706, training acc= 98.98571968078613%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "Epoch 100/1290, training loss= 0.02585931, training acc= 98.98571968078613%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "Epoch 110/1290, training loss= 0.0256099, training acc= 99.01241064071655%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "Epoch 120/1290, training loss= 0.024403175, training acc= 99.02575612068176%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "Epoch 130/1290, training loss= 0.023817688, training acc= 99.03910160064697%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "Epoch 140/1290, training loss= 0.028108, training acc= 99.01241064071655%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "Epoch 150/1290, training loss= 0.02178147, training acc= 99.0791380405426%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 160/1290, training loss= 0.023724347, training acc= 99.06579256057739%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 170/1290, training loss= 0.022173008, training acc= 99.15921688079834%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "Epoch 180/1290, training loss= 0.020598112, training acc= 99.13252592086792%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "Epoch 190/1290, training loss= 0.020995067, training acc= 99.15921688079834%\n",
            "Validation Accuracy 99.0660400390625 ...\n",
            "\n",
            "Epoch 200/1290, training loss= 0.018880757, training acc= 99.14587140083313%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 210/1290, training loss= 0.018550634, training acc= 99.13252592086792%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "Epoch 220/1290, training loss= 0.018218232, training acc= 99.15921688079834%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "Epoch 230/1290, training loss= 0.01769138, training acc= 99.2392897605896%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "Epoch 240/1290, training loss= 0.017409015, training acc= 99.25263524055481%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "Epoch 250/1290, training loss= 0.017289517, training acc= 99.2392897605896%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "Epoch 260/1290, training loss= 0.01918046, training acc= 99.22594428062439%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "Epoch 270/1290, training loss= 0.016809821, training acc= 99.27932620048523%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "Epoch 280/1290, training loss= 0.016365679, training acc= 99.26598072052002%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "Epoch 290/1290, training loss= 0.017151123, training acc= 99.30601716041565%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 300/1290, training loss= 0.016332824, training acc= 99.29267168045044%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "Epoch 310/1290, training loss= 0.018497504, training acc= 99.22594428062439%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 320/1290, training loss= 0.015444008, training acc= 99.33270812034607%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 330/1290, training loss= 0.014901849, training acc= 99.31936264038086%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "Epoch 340/1290, training loss= 0.01495177, training acc= 99.33270812034607%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "Epoch 350/1290, training loss= 0.015396873, training acc= 99.27932620048523%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 360/1290, training loss= 0.01486522, training acc= 99.30601716041565%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 370/1290, training loss= 0.014589079, training acc= 99.31936264038086%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 380/1290, training loss= 0.017163953, training acc= 99.26598072052002%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "Epoch 390/1290, training loss= 0.01435812, training acc= 99.34605360031128%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 400/1290, training loss= 0.014226134, training acc= 99.37275052070618%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 410/1290, training loss= 0.013716846, training acc= 99.37275052070618%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 420/1290, training loss= 0.015070751, training acc= 99.30601716041565%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "Epoch 430/1290, training loss= 0.013622344, training acc= 99.38609600067139%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 440/1290, training loss= 0.013348751, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 450/1290, training loss= 0.013620967, training acc= 99.38609600067139%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 460/1290, training loss= 0.01298214, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 470/1290, training loss= 0.013258767, training acc= 99.38609600067139%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 480/1290, training loss= 0.013834012, training acc= 99.38609600067139%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 490/1290, training loss= 0.013142705, training acc= 99.3994414806366%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 500/1290, training loss= 0.013316795, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 510/1290, training loss= 0.019297967, training acc= 99.27932620048523%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "Epoch 520/1290, training loss= 0.013900356, training acc= 99.38609600067139%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 530/1290, training loss= 0.013515405, training acc= 99.42613244056702%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 540/1290, training loss= 0.013006078, training acc= 99.42613244056702%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 550/1290, training loss= 0.01575271, training acc= 99.30601716041565%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "Epoch 560/1290, training loss= 0.013943325, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 570/1290, training loss= 0.012464818, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 580/1290, training loss= 0.01212287, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 590/1290, training loss= 0.013233093, training acc= 99.38609600067139%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 600/1290, training loss= 0.013346431, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 610/1290, training loss= 0.012414723, training acc= 99.3994414806366%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "Epoch 620/1290, training loss= 0.012738461, training acc= 99.3994414806366%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 630/1290, training loss= 0.012262647, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 640/1290, training loss= 0.012225575, training acc= 99.42613244056702%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 650/1290, training loss= 0.011921122, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 660/1290, training loss= 0.012508309, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 670/1290, training loss= 0.013244385, training acc= 99.38609600067139%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 680/1290, training loss= 0.012248336, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "Epoch 690/1290, training loss= 0.012095792, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 700/1290, training loss= 0.011563516, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 710/1290, training loss= 0.011536492, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 720/1290, training loss= 0.011215349, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 730/1290, training loss= 0.011510326, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 740/1290, training loss= 0.011422572, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 750/1290, training loss= 0.0114116, training acc= 99.45282340049744%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "Epoch 760/1290, training loss= 0.011243942, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 770/1290, training loss= 0.0111812195, training acc= 99.45282340049744%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 780/1290, training loss= 0.011082083, training acc= 99.45282340049744%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 790/1290, training loss= 0.011805298, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 800/1290, training loss= 0.011008354, training acc= 99.45282340049744%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 810/1290, training loss= 0.011561337, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 820/1290, training loss= 0.011240016, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 830/1290, training loss= 0.011167303, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 840/1290, training loss= 0.011216934, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 850/1290, training loss= 0.01073451, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 860/1290, training loss= 0.011335813, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 870/1290, training loss= 0.010816327, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 880/1290, training loss= 0.01100752, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 890/1290, training loss= 0.011933164, training acc= 99.3994414806366%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 900/1290, training loss= 0.012242976, training acc= 99.4127869606018%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 910/1290, training loss= 0.0115004415, training acc= 99.43947792053223%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 920/1290, training loss= 0.011061919, training acc= 99.45282340049744%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 930/1290, training loss= 0.011032499, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 940/1290, training loss= 0.016097609, training acc= 99.30601716041565%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 950/1290, training loss= 0.011037474, training acc= 99.45282340049744%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 960/1290, training loss= 0.010717913, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 970/1290, training loss= 0.010885993, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 980/1290, training loss= 0.010869863, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 990/1290, training loss= 0.0108149275, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1000/1290, training loss= 0.010667812, training acc= 99.49285984039307%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1010/1290, training loss= 0.010579887, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1020/1290, training loss= 0.010683141, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1030/1290, training loss= 0.010868332, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1040/1290, training loss= 0.010519051, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1050/1290, training loss= 0.010593629, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1060/1290, training loss= 0.010998162, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1070/1290, training loss= 0.01066554, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "Epoch 1080/1290, training loss= 0.010469445, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1090/1290, training loss= 0.010592774, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1100/1290, training loss= 0.0104006985, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1110/1290, training loss= 0.010313321, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1120/1290, training loss= 0.011056169, training acc= 99.45282340049744%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1130/1290, training loss= 0.010787145, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1140/1290, training loss= 0.01040567, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1150/1290, training loss= 0.0105584925, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1160/1290, training loss= 0.010372503, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1170/1290, training loss= 0.010365353, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1180/1290, training loss= 0.010286771, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1190/1290, training loss= 0.010312056, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Epoch 1200/1290, training loss= 0.010099984, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1210/1290, training loss= 0.01035634, training acc= 99.49285984039307%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1220/1290, training loss= 0.010023645, training acc= 99.46616888046265%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1230/1290, training loss= 0.010277178, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1240/1290, training loss= 0.010163802, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1250/1290, training loss= 0.009983209, training acc= 99.49285984039307%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1260/1290, training loss= 0.010245599, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.79986572265625 ...\n",
            "\n",
            "Epoch 1270/1290, training loss= 0.010185603, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "Epoch 1280/1290, training loss= 0.010182607, training acc= 99.47951436042786%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "Valid acc= 99.799866 %\n",
            "==================================================\n",
            "W1\n",
            "3\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2oEJ6yqELgp2",
        "colab_type": "code",
        "outputId": "c5508eaf-e5ec-4baa-c6c0-cd773801c657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './PenDigitFinalReduced')\n",
        "    train_accuracy = sess.run(accuracy*100, feed_dict={X: X_train,Y: y_train})\n",
        "    print(\"Train acc=\",str(train_accuracy), \"%\")\n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_accuracy), \"%\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./PenDigitFinalReduced\n",
            "Train acc= 99.49286 %\n",
            "ValidValid acc= 99.799866 %\n",
            "Test acc= 97.71298 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MZD2D7FSKJOp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "8hp89NMVKBPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "50jgkGr2KBNA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "MsIkDOvFKBHF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "kNvs6e3JChkM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Old code - Hyper Tuning below"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "dXIQF74aypZh",
        "colab_type": "code",
        "outputId": "a2141ee0-39b3-4798-a106-1bf2453961d5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "hid_neuron = [104]\n",
        "num_steps = 100000\n",
        "batch_size = 200\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "wLoss1 = 6\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % plot_every == 0:\n",
        "            train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "            train_accuracy.append(train_acc)\n",
        "            print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "            train_losses.append(train_loss)\n",
        "            validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "            val_accuracy.append(validation_accuracy)\n",
        "            if step%plot_every == 0:\n",
        "              print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "              print()\n",
        "              if (validation_accuracy >= best_accuracy_valid):\n",
        "                best_accuracy_valid = validation_accuracy\n",
        "                saver.save(sess, './statlog_letter2')\n",
        "                G_W1np, G_b1np, G_W2np, G_b2np, G_W3np, G_b3np = sess.run([G_W1, G_b1, G_W2, G_b2, G_W3, G_b3])\n",
        "    print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "    ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-22-20e9c5308af7>:47: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step 0, training loss= 109.98173, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50, training loss= 2.9851856, training acc= 95.99999785423279%\n",
            "Validation Accuracy 96.66444396972656 ...\n",
            "\n",
            "step 100, training loss= 1.204541, training acc= 98.50000143051147%\n",
            "Validation Accuracy 96.66444396972656 ...\n",
            "\n",
            "step 150, training loss= 0.4234264, training acc= 98.00000190734863%\n",
            "Validation Accuracy 97.13141632080078 ...\n",
            "\n",
            "step 200, training loss= 0.2155815, training acc= 100.0%\n",
            "Validation Accuracy 97.79853057861328 ...\n",
            "\n",
            "step 250, training loss= 0.26538092, training acc= 98.00000190734863%\n",
            "Validation Accuracy 97.99866485595703 ...\n",
            "\n",
            "step 300, training loss= 0.089434944, training acc= 99.00000095367432%\n",
            "Validation Accuracy 98.13208770751953 ...\n",
            "\n",
            "step 350, training loss= 0.09699867, training acc= 97.50000238418579%\n",
            "Validation Accuracy 98.39893341064453 ...\n",
            "\n",
            "step 400, training loss= 0.08207041, training acc= 100.0%\n",
            "Validation Accuracy 98.59906768798828 ...\n",
            "\n",
            "step 450, training loss= 0.12828861, training acc= 99.00000095367432%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "step 500, training loss= 0.0337054, training acc= 100.0%\n",
            "Validation Accuracy 98.73249053955078 ...\n",
            "\n",
            "step 550, training loss= 0.035193503, training acc= 99.00000095367432%\n",
            "Validation Accuracy 98.93262481689453 ...\n",
            "\n",
            "step 600, training loss= 0.034307912, training acc= 98.50000143051147%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "step 650, training loss= 0.0057984856, training acc= 100.0%\n",
            "Validation Accuracy 98.86590576171875 ...\n",
            "\n",
            "step 700, training loss= 0.046027496, training acc= 99.50000047683716%\n",
            "Validation Accuracy 98.93262481689453 ...\n",
            "\n",
            "step 750, training loss= 0.059952807, training acc= 99.50000047683716%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 800, training loss= 0.023224508, training acc= 99.50000047683716%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 850, training loss= 0.03757449, training acc= 100.0%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 900, training loss= 0.009147925, training acc= 100.0%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 950, training loss= 0.0036250958, training acc= 100.0%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 1000, training loss= 0.0030607635, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1050, training loss= 0.0631515, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.0660400390625 ...\n",
            "\n",
            "step 1100, training loss= 0.027653806, training acc= 100.0%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 1150, training loss= 0.04887722, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1200, training loss= 0.028914742, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1250, training loss= 0.026249338, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 1300, training loss= 0.007915103, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 1350, training loss= 0.0048822034, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1400, training loss= 0.00950739, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 1450, training loss= 0.0037202314, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 1500, training loss= 0.015764944, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 1550, training loss= 0.0065427534, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 1600, training loss= 0.0057849926, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 1650, training loss= 0.0026489347, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 1700, training loss= 0.0016985023, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1750, training loss= 0.0048939707, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1800, training loss= 0.006183743, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1850, training loss= 0.004099639, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1900, training loss= 0.012571454, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 1950, training loss= 0.0060145166, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 2000, training loss= 0.02245107, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 2050, training loss= 0.0037075672, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 2100, training loss= 0.0031748805, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 2150, training loss= 0.0053376295, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2200, training loss= 0.0026726872, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2250, training loss= 0.010035805, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2300, training loss= 0.002643262, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2350, training loss= 0.0026757934, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2400, training loss= 0.003150687, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2450, training loss= 0.013925527, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 2500, training loss= 0.0019927653, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2550, training loss= 0.0030613393, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 2600, training loss= 0.003971531, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 2650, training loss= 0.0066127907, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 2700, training loss= 0.005084242, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 2750, training loss= 0.0023595076, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 2800, training loss= 0.005206835, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 2850, training loss= 0.002915146, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 2900, training loss= 0.025540743, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 2950, training loss= 0.0023107436, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 3000, training loss= 0.005476523, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 3050, training loss= 0.00903118, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 3100, training loss= 0.004214019, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3150, training loss= 0.011189755, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 3200, training loss= 0.0030444062, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 3250, training loss= 0.0030957388, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 3300, training loss= 0.0020779849, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3350, training loss= 0.0031580208, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 3400, training loss= 0.0079008415, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 3450, training loss= 0.003966007, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3500, training loss= 0.0023614257, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 3550, training loss= 0.0032933655, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3600, training loss= 0.0021565598, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 3650, training loss= 0.002364118, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 3700, training loss= 0.0017326789, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3750, training loss= 0.020443223, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3800, training loss= 0.0031589298, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3850, training loss= 0.021816887, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3900, training loss= 0.0019288776, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 3950, training loss= 0.003070686, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 4000, training loss= 0.0020186144, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 4050, training loss= 0.003893922, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 4100, training loss= 0.003914307, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 4150, training loss= 0.0027375175, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 4200, training loss= 0.028090766, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 4250, training loss= 0.0025873454, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 4300, training loss= 0.0011968504, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 4350, training loss= 0.0036152985, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 4400, training loss= 0.0025130515, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 4450, training loss= 0.0021255242, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 4500, training loss= 0.0016291067, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 4550, training loss= 0.0028085574, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 4600, training loss= 0.010152089, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 4650, training loss= 0.0316701, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 4700, training loss= 0.0014833789, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 4750, training loss= 0.0022901413, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 4800, training loss= 0.0033891536, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 4850, training loss= 0.001939687, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 4900, training loss= 0.0037563196, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 4950, training loss= 0.0015403094, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5000, training loss= 0.0051362496, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5050, training loss= 0.0019035288, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 5100, training loss= 0.00834436, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5150, training loss= 0.0016188925, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 5200, training loss= 0.0016201718, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5250, training loss= 0.0014821853, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5300, training loss= 0.0016548595, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5350, training loss= 0.17280722, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5400, training loss= 0.0030481094, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5450, training loss= 0.0020315228, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 5500, training loss= 0.0026891667, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5550, training loss= 0.0055081574, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5600, training loss= 0.0023796465, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5650, training loss= 0.0016841043, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5700, training loss= 0.0021571205, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 5750, training loss= 0.0032764974, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5800, training loss= 0.0027017694, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5850, training loss= 0.04909158, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 5900, training loss= 0.0014568155, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 5950, training loss= 0.0023754959, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 6000, training loss= 0.012733467, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 6050, training loss= 0.0022829124, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 6100, training loss= 0.018593851, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 6150, training loss= 0.018803837, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 6200, training loss= 0.0035198047, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 6250, training loss= 0.0030250598, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 6300, training loss= 0.012120858, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 6350, training loss= 0.0019602214, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 6400, training loss= 0.002062161, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 6450, training loss= 0.004233729, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 6500, training loss= 0.0021346644, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 6550, training loss= 0.0023392716, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 6600, training loss= 0.0015952246, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 6650, training loss= 0.0013851412, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 6700, training loss= 0.0019117002, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 6750, training loss= 0.0023316843, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 6800, training loss= 0.001924954, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 6850, training loss= 0.0010734076, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 6900, training loss= 0.005217763, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 6950, training loss= 0.0019893944, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 7000, training loss= 0.0064020758, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 7050, training loss= 0.06416498, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7100, training loss= 0.0023450865, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7150, training loss= 0.0012746453, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 7200, training loss= 0.0019001312, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7250, training loss= 0.009457495, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 7300, training loss= 0.0022336843, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7350, training loss= 0.001673892, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 7400, training loss= 0.0020515625, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7450, training loss= 0.0021057893, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 7500, training loss= 0.001510759, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 7550, training loss= 0.0034952813, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7600, training loss= 0.008999852, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 7650, training loss= 0.0012546959, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7700, training loss= 0.0027946872, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7750, training loss= 0.008926385, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 7800, training loss= 0.007084446, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 7850, training loss= 0.0019090291, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 7900, training loss= 0.0029436874, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 7950, training loss= 0.0020611836, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 8000, training loss= 0.001377817, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 8050, training loss= 0.0012259203, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8100, training loss= 0.002394261, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 8150, training loss= 0.0019675242, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8200, training loss= 0.0014403276, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8250, training loss= 0.0026848149, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 8300, training loss= 0.0013557645, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 8350, training loss= 0.001150791, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8400, training loss= 0.00093236327, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8450, training loss= 0.011210198, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8500, training loss= 0.002705517, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8550, training loss= 0.0030704653, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 8600, training loss= 0.0031092642, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 8650, training loss= 0.0011361605, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 8700, training loss= 0.0025259077, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 8750, training loss= 0.0033543278, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 8800, training loss= 0.0016805009, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 8850, training loss= 0.0012079357, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 8900, training loss= 0.001654467, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 8950, training loss= 0.00478984, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 9000, training loss= 0.002195489, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 9050, training loss= 0.0021761267, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 9100, training loss= 0.0029886814, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 9150, training loss= 0.0017874893, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 9200, training loss= 0.0011045153, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 9250, training loss= 0.0018795012, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 9300, training loss= 0.000951224, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9350, training loss= 0.0014263934, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9400, training loss= 0.0017990564, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9450, training loss= 0.0025312817, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9500, training loss= 0.0011621132, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 9550, training loss= 0.00072543696, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 9600, training loss= 0.0012196414, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 9650, training loss= 0.0017521055, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9700, training loss= 0.0010543138, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9750, training loss= 0.00084153004, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9800, training loss= 0.002239137, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 9850, training loss= 0.0012970744, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 9900, training loss= 0.0013453105, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 9950, training loss= 0.021488031, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 10000, training loss= 0.005335997, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 10050, training loss= 0.0027506556, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 10100, training loss= 0.0030365407, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 10150, training loss= 0.0024042567, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 10200, training loss= 0.0024990202, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 10250, training loss= 0.0015072358, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 10300, training loss= 0.0018119779, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 10350, training loss= 0.0020172196, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 10400, training loss= 0.003037645, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 10450, training loss= 0.0005084953, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 10500, training loss= 0.0016995923, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 10550, training loss= 0.0013965932, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 10600, training loss= 0.0005741748, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 10650, training loss= 0.0014505753, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 10700, training loss= 0.0023001279, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 10750, training loss= 0.000982011, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 10800, training loss= 0.002350778, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 10850, training loss= 0.0010549454, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 10900, training loss= 0.0030783683, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 10950, training loss= 0.0013109687, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 11000, training loss= 0.00077209785, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 11050, training loss= 0.0010940298, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 11100, training loss= 0.00085297937, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 11150, training loss= 0.00085421273, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 11200, training loss= 0.0005030793, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 11250, training loss= 0.0016050882, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 11300, training loss= 0.000992893, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 11350, training loss= 0.00084363145, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 11400, training loss= 0.00077118754, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 11450, training loss= 0.001000626, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 11500, training loss= 0.0029364952, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 11550, training loss= 0.0014990503, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 11600, training loss= 0.0014143672, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 11650, training loss= 0.0013063938, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11700, training loss= 0.0012291945, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 11750, training loss= 0.0020973568, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 11800, training loss= 0.0031509835, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 11850, training loss= 0.00077393407, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 11900, training loss= 0.001463656, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 11950, training loss= 0.0014289782, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 12000, training loss= 0.0017870718, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 12050, training loss= 0.00079788995, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 12100, training loss= 0.0016537891, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12150, training loss= 0.0016806563, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12200, training loss= 0.0010353039, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 12250, training loss= 0.0007019014, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 12300, training loss= 0.0032550583, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12350, training loss= 0.0012092018, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 12400, training loss= 0.0005043214, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 12450, training loss= 0.0010211217, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12500, training loss= 0.0008957734, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12550, training loss= 0.0031822422, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12600, training loss= 0.0013540098, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12650, training loss= 0.0027991699, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12700, training loss= 0.0010104853, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12750, training loss= 0.00083401497, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12800, training loss= 0.0009541646, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12850, training loss= 0.0019065855, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12900, training loss= 0.0014751669, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 12950, training loss= 0.0019363734, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13000, training loss= 0.0007378526, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13050, training loss= 0.0003490519, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13100, training loss= 0.00085236673, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13150, training loss= 0.00055159384, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13200, training loss= 0.0010923883, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13250, training loss= 0.001208178, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13300, training loss= 0.00042696815, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 13350, training loss= 0.0033591767, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 13400, training loss= 0.0011368694, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 13450, training loss= 0.040969986, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 13500, training loss= 0.005796723, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 13550, training loss= 0.0019229129, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 13600, training loss= 0.0030047484, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 13650, training loss= 0.0019816565, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 13700, training loss= 0.0037245555, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 13750, training loss= 0.022232587, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 13800, training loss= 0.0018387487, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 13850, training loss= 0.00065679394, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 13900, training loss= 0.0004943128, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 13950, training loss= 0.019617911, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 14000, training loss= 0.010772498, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 14050, training loss= 0.001665297, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14100, training loss= 0.0011829925, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14150, training loss= 0.0003815359, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 14200, training loss= 0.000648205, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14250, training loss= 0.0005634982, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 14300, training loss= 0.0007216334, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14350, training loss= 0.0010710966, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14400, training loss= 0.00085871277, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 14450, training loss= 0.00071569026, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 14500, training loss= 0.0007186553, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 14550, training loss= 0.0005987828, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 14600, training loss= 0.0005754601, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14650, training loss= 0.0004899651, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14700, training loss= 0.00048646634, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14750, training loss= 0.0012643222, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14800, training loss= 0.0007627193, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14850, training loss= 0.00048044496, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 14900, training loss= 0.0078121945, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 14950, training loss= 0.0005586174, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15000, training loss= 0.0009365213, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15050, training loss= 0.0011082364, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15100, training loss= 0.00294142, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15150, training loss= 0.00044144198, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15200, training loss= 0.00047322834, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15250, training loss= 0.00057164434, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15300, training loss= 0.0018620166, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15350, training loss= 0.00069129677, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15400, training loss= 0.0010840332, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15450, training loss= 0.0007831856, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15500, training loss= 0.0010906054, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15550, training loss= 0.0013302848, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15600, training loss= 0.00081793306, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15650, training loss= 0.0010081913, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15700, training loss= 0.0006320703, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15750, training loss= 0.00042395716, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15800, training loss= 0.008197133, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15850, training loss= 0.00062605215, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 15900, training loss= 0.0028816727, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 15950, training loss= 0.0004817937, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 16000, training loss= 0.000926494, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 16050, training loss= 0.0011067715, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 16100, training loss= 0.00050403544, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 16150, training loss= 0.015982663, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 16200, training loss= 0.004660771, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 16250, training loss= 0.0017903032, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 16300, training loss= 0.00068143645, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16350, training loss= 0.0035983364, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16400, training loss= 0.0011262784, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16450, training loss= 0.0015099648, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16500, training loss= 0.0004482108, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16550, training loss= 0.00085029885, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16600, training loss= 0.0012688178, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16650, training loss= 0.0056229085, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16700, training loss= 0.0007112155, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16750, training loss= 0.00053784304, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16800, training loss= 0.00058734417, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16850, training loss= 0.0012528824, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16900, training loss= 0.0016161213, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 16950, training loss= 0.0006346174, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17000, training loss= 0.000749089, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17050, training loss= 0.0010582173, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17100, training loss= 0.00060868886, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17150, training loss= 0.0020716707, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17200, training loss= 0.0007709621, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17250, training loss= 0.0007293117, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17300, training loss= 0.00053193787, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17350, training loss= 0.0009935467, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17400, training loss= 0.0022087712, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17450, training loss= 0.000644005, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17500, training loss= 0.00055404456, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17550, training loss= 0.0040633334, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17600, training loss= 0.0009443614, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17650, training loss= 0.000516073, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17700, training loss= 0.0004066031, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17750, training loss= 0.0012130202, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17800, training loss= 0.0005801866, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 17850, training loss= 0.00075629563, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 17900, training loss= 0.0010549356, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 17950, training loss= 0.000498008, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 18000, training loss= 0.0030124418, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 18050, training loss= 0.0017604777, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 18100, training loss= 0.0007104318, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 18150, training loss= 0.00036332692, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 18200, training loss= 0.0016815048, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 18250, training loss= 0.0012377964, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 18300, training loss= 0.0019186438, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 18350, training loss= 0.018120471, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 18400, training loss= 0.0010980364, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 18450, training loss= 0.0015401277, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 18500, training loss= 0.00038075238, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 18550, training loss= 0.009951229, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 18600, training loss= 0.0019372166, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 18650, training loss= 0.00046809224, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 18700, training loss= 0.0010725736, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 18750, training loss= 0.0007111618, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 18800, training loss= 0.0006935227, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 18850, training loss= 0.0010562391, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 18900, training loss= 0.001434422, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 18950, training loss= 0.00038175442, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19000, training loss= 0.0006379902, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 19050, training loss= 0.0005656681, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 19100, training loss= 0.0057424535, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19150, training loss= 0.0009209512, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19200, training loss= 0.0008035012, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19250, training loss= 0.0011148853, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 19300, training loss= 0.00022618897, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 19350, training loss= 0.00062257046, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19400, training loss= 0.00048831676, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 19450, training loss= 0.0015404333, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 19500, training loss= 0.0007234548, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19550, training loss= 0.0012934528, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19600, training loss= 0.00067132356, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19650, training loss= 0.0006708908, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19700, training loss= 0.00051356864, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19750, training loss= 0.0005716739, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19800, training loss= 0.0015756874, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 19850, training loss= 0.00044441404, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 19900, training loss= 0.00046472985, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 19950, training loss= 0.0019085231, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 20000, training loss= 0.00064424466, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 20050, training loss= 0.0005200402, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 20100, training loss= 0.00038962037, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 20150, training loss= 0.0014370246, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 20200, training loss= 0.0038537574, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 20250, training loss= 0.022368118, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 20300, training loss= 0.008695549, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 20350, training loss= 0.0027819711, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 20400, training loss= 0.0015051344, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 20450, training loss= 0.0017254599, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 20500, training loss= 0.0016327845, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 20550, training loss= 0.0010750615, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 20600, training loss= 0.0010043376, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 20650, training loss= 0.0007132036, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 20700, training loss= 0.00030399743, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 20750, training loss= 0.0066359458, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 20800, training loss= 0.0007588896, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 20850, training loss= 0.0008689584, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 20900, training loss= 0.00069176656, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 20950, training loss= 0.00025322562, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 21000, training loss= 0.00038077065, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 21050, training loss= 0.0006642036, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 21100, training loss= 0.00047962897, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 21150, training loss= 0.00041000987, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 21200, training loss= 0.00045707202, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 21250, training loss= 0.00045417345, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21300, training loss= 0.0029628356, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 21350, training loss= 0.000566669, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21400, training loss= 0.00021796465, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21450, training loss= 0.00023307896, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21500, training loss= 0.00049310667, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21550, training loss= 0.00070949784, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21600, training loss= 0.0005494635, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21650, training loss= 0.0004082037, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21700, training loss= 0.00042060396, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21750, training loss= 0.00038289296, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21800, training loss= 0.00033951597, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21850, training loss= 0.0005856078, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21900, training loss= 0.00041670608, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 21950, training loss= 0.0005032339, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22000, training loss= 0.00032728323, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22050, training loss= 0.00049915037, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22100, training loss= 0.0007743795, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22150, training loss= 0.00023703052, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22200, training loss= 0.016358593, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22250, training loss= 0.000556097, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 22300, training loss= 0.0003244071, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22350, training loss= 0.00096622785, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22400, training loss= 0.0033109535, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22450, training loss= 0.00026577013, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 22500, training loss= 0.0012066998, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22550, training loss= 0.0002911295, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22600, training loss= 0.00036990514, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22650, training loss= 0.00947869, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22700, training loss= 0.00045162838, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22750, training loss= 0.00021159217, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22800, training loss= 0.0007158611, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22850, training loss= 0.00070479454, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22900, training loss= 0.0005238024, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 22950, training loss= 0.00075831276, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23000, training loss= 0.000121066965, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23050, training loss= 0.0003534196, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23100, training loss= 0.0005514348, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23150, training loss= 0.00032725808, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23200, training loss= 0.0004150658, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23250, training loss= 0.00035112962, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23300, training loss= 0.00020506022, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23350, training loss= 0.00031895915, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23400, training loss= 0.00032738977, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23450, training loss= 0.0002452064, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23500, training loss= 0.0004905822, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23550, training loss= 0.0011501253, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23600, training loss= 0.0006442656, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23650, training loss= 0.001140686, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23700, training loss= 0.00035088966, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23750, training loss= 0.0001861612, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 23800, training loss= 0.00036639103, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 23850, training loss= 0.0047737495, training acc= 99.50000047683716%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 23900, training loss= 0.0019784146, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 23950, training loss= 0.0008128238, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 24000, training loss= 0.001792754, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24050, training loss= 0.00041966728, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24100, training loss= 0.00091702107, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24150, training loss= 0.0016582588, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24200, training loss= 0.0007087962, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24250, training loss= 0.00078420574, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24300, training loss= 0.00027863722, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24350, training loss= 0.00063481077, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24400, training loss= 0.00045317042, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24450, training loss= 0.00034060283, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24500, training loss= 0.00029165117, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24550, training loss= 0.0006527491, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24600, training loss= 0.0006833723, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24650, training loss= 0.0004130642, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24700, training loss= 0.0003669159, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24750, training loss= 0.00027561295, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24800, training loss= 0.0004082187, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24850, training loss= 0.00045327639, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24900, training loss= 0.0028504864, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 24950, training loss= 0.0006604238, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25000, training loss= 0.0007366961, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25050, training loss= 0.0006604702, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25100, training loss= 0.00025838456, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25150, training loss= 0.0003056742, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25200, training loss= 0.00035505262, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25250, training loss= 0.00047478708, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25300, training loss= 0.0006428804, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25350, training loss= 0.00039613008, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25400, training loss= 0.0012784081, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 25450, training loss= 0.00027174648, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25500, training loss= 0.00039680555, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25550, training loss= 0.00024114555, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 25600, training loss= 0.00028214118, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25650, training loss= 0.00035306407, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25700, training loss= 0.0001871728, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25750, training loss= 0.00027361166, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25800, training loss= 0.00029128155, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 25850, training loss= 0.0031904972, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 25900, training loss= 0.0015670701, training acc= 100.0%\n",
            "Validation Accuracy 99.0660400390625 ...\n",
            "\n",
            "step 25950, training loss= 0.045251377, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 26000, training loss= 0.00035300918, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26050, training loss= 0.0024241095, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26100, training loss= 0.00036955293, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26150, training loss= 0.0046628853, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26200, training loss= 0.002075889, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26250, training loss= 0.0029968265, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26300, training loss= 0.00034367508, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26350, training loss= 0.00028253658, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26400, training loss= 0.0004473677, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26450, training loss= 0.0068716845, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26500, training loss= 0.0004750223, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26550, training loss= 0.00023024404, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26600, training loss= 0.0003728685, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26650, training loss= 0.000411643, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26700, training loss= 0.0006346696, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26750, training loss= 0.00012732182, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26800, training loss= 0.0004124301, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26850, training loss= 0.00018641942, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 26900, training loss= 0.000107494576, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 26950, training loss= 0.00022757209, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27000, training loss= 0.00087748055, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 27050, training loss= 0.00040129738, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27100, training loss= 0.0005055184, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 27150, training loss= 0.0003171102, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27200, training loss= 0.00010433992, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 27250, training loss= 0.00029932082, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27300, training loss= 0.000266865, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27350, training loss= 0.00043832892, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 27400, training loss= 0.00015114578, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27450, training loss= 0.0002530059, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 27500, training loss= 0.0005573732, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 27550, training loss= 0.002913527, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27600, training loss= 0.00021032675, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 27650, training loss= 0.00029163313, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 27700, training loss= 0.00017352428, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 27750, training loss= 0.00036021558, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27800, training loss= 0.0003691757, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27850, training loss= 0.0003064989, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 27900, training loss= 0.0017280288, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 27950, training loss= 0.00023316998, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28000, training loss= 0.0005336227, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28050, training loss= 8.257629e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28100, training loss= 0.00030937698, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28150, training loss= 0.00021651508, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28200, training loss= 0.0001748155, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28250, training loss= 0.00017380662, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28300, training loss= 0.0002042316, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28350, training loss= 0.00025515116, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28400, training loss= 0.00024791478, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 28450, training loss= 0.00049472705, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 28500, training loss= 0.0005911983, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 28550, training loss= 0.00058766664, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 28600, training loss= 0.0018549237, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 28650, training loss= 0.0018563851, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28700, training loss= 0.00046125642, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 28750, training loss= 0.0004402328, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 28800, training loss= 0.00057255436, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28850, training loss= 0.00051874714, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28900, training loss= 0.002713199, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 28950, training loss= 0.00038666758, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29000, training loss= 0.0007954843, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29050, training loss= 0.0005484377, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 29100, training loss= 0.00023745437, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 29150, training loss= 0.0007118599, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 29200, training loss= 0.00029573747, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 29250, training loss= 0.00042129654, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29300, training loss= 0.0004027814, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 29350, training loss= 0.0003450378, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29400, training loss= 0.00023203381, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29450, training loss= 0.00023850347, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29500, training loss= 0.00041044576, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29550, training loss= 0.0006113456, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29600, training loss= 0.0018978714, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29650, training loss= 0.00018796886, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29700, training loss= 0.0002054109, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29750, training loss= 0.00017032164, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29800, training loss= 0.00018556308, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29850, training loss= 0.0006191759, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29900, training loss= 0.00038910462, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 29950, training loss= 0.00042756775, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30000, training loss= 0.00035513297, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30050, training loss= 0.00026268297, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30100, training loss= 0.0002291757, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30150, training loss= 0.0001968074, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30200, training loss= 0.0022247161, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30250, training loss= 0.00020266729, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30300, training loss= 0.0001533084, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30350, training loss= 0.00030640746, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30400, training loss= 0.0002449892, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30450, training loss= 0.00012832526, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 30500, training loss= 0.00025600847, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30550, training loss= 0.00028566277, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30600, training loss= 0.0005698714, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 30650, training loss= 0.0007151739, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 30700, training loss= 5.6537763e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30750, training loss= 0.0004219206, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30800, training loss= 0.00023450165, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30850, training loss= 0.00023526044, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 30900, training loss= 0.00021314164, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 30950, training loss= 0.0001874056, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31000, training loss= 0.00022275548, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31050, training loss= 0.00013055318, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31100, training loss= 0.0006383408, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31150, training loss= 0.00034036188, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31200, training loss= 0.0003522303, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31250, training loss= 0.00023724845, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31300, training loss= 0.00024245618, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31350, training loss= 0.00026811467, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31400, training loss= 0.00027533175, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31450, training loss= 0.00021013702, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31500, training loss= 0.00055247394, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31550, training loss= 9.206993e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31600, training loss= 0.0005603596, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31650, training loss= 6.6824265e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31700, training loss= 0.00024058984, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31750, training loss= 0.00030530727, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31800, training loss= 0.00024443908, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31850, training loss= 6.537367e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 31900, training loss= 0.00016851089, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 31950, training loss= 0.00017862626, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32000, training loss= 0.00026334595, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32050, training loss= 0.0002227382, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32100, training loss= 0.00020539365, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32150, training loss= 0.001163012, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32200, training loss= 0.00021814143, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32250, training loss= 0.00025579397, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32300, training loss= 0.00023023249, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 32350, training loss= 5.1398918e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 32400, training loss= 0.00013007155, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 32450, training loss= 0.00073359883, training acc= 100.0%\n",
            "Validation Accuracy 99.0660400390625 ...\n",
            "\n",
            "step 32500, training loss= 0.0009031743, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 32550, training loss= 0.0009226467, training acc= 100.0%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "step 32600, training loss= 0.0003704914, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 32650, training loss= 0.008203011, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 32700, training loss= 0.0008858126, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 32750, training loss= 0.00047248628, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 32800, training loss= 0.0007181139, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 32850, training loss= 0.00029806126, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 32900, training loss= 0.00086053595, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 32950, training loss= 0.00019975459, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33000, training loss= 0.00018896742, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33050, training loss= 8.315617e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33100, training loss= 0.0002590309, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33150, training loss= 0.0003313195, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33200, training loss= 0.00045744912, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33250, training loss= 0.00026017096, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33300, training loss= 0.00017559205, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33350, training loss= 0.00050662295, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33400, training loss= 0.00018104588, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33450, training loss= 0.0003777845, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33500, training loss= 0.00023504978, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33550, training loss= 0.00040273284, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33600, training loss= 0.0003585549, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33650, training loss= 0.00030764422, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33700, training loss= 0.00015651446, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33750, training loss= 0.0001653436, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33800, training loss= 0.00030660574, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33850, training loss= 0.000218823, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33900, training loss= 0.00010604049, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 33950, training loss= 0.00011606469, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34000, training loss= 0.00017615253, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34050, training loss= 0.0002475092, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34100, training loss= 0.00030297396, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34150, training loss= 8.376207e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34200, training loss= 0.0009890319, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34250, training loss= 9.4765855e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34300, training loss= 0.000120442215, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34350, training loss= 0.00011274157, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34400, training loss= 0.0001371643, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34450, training loss= 0.00017581286, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34500, training loss= 0.00035442415, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34550, training loss= 0.00022059632, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34600, training loss= 0.00014201541, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34650, training loss= 0.000103085054, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 34700, training loss= 0.00013172263, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 34750, training loss= 5.96852e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 34800, training loss= 0.00013256713, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 34850, training loss= 0.00024128734, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 34900, training loss= 0.00029524305, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 34950, training loss= 0.000138714, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35000, training loss= 0.00013436408, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35050, training loss= 0.0003599051, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35100, training loss= 0.00010278829, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35150, training loss= 0.00019544137, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35200, training loss= 0.0007167558, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35250, training loss= 6.580281e-05, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35300, training loss= 0.0001355987, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 35350, training loss= 0.00019360453, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35400, training loss= 7.418265e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 35450, training loss= 4.0403505e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35500, training loss= 0.00010569965, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35550, training loss= 0.0013812288, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 35600, training loss= 6.756264e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 35650, training loss= 0.00010093002, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 35700, training loss= 0.0002853288, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35750, training loss= 0.00010874113, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35800, training loss= 7.8532525e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35850, training loss= 8.300185e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35900, training loss= 0.00013170038, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 35950, training loss= 0.00010031294, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36000, training loss= 0.00016287295, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36050, training loss= 0.0006452085, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36100, training loss= 0.0011762879, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36150, training loss= 4.6371206e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36200, training loss= 8.011209e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36250, training loss= 0.000114672905, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 36300, training loss= 7.850358e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36350, training loss= 0.00017133406, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36400, training loss= 4.2165244e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36450, training loss= 7.214633e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 36500, training loss= 0.00010026377, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36550, training loss= 0.000102045655, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36600, training loss= 4.8430407e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36650, training loss= 0.00025670746, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36700, training loss= 9.075886e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36750, training loss= 5.9604765e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36800, training loss= 9.875565e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36850, training loss= 6.114093e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36900, training loss= 0.0006091722, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 36950, training loss= 0.0013145079, training acc= 100.0%\n",
            "Validation Accuracy 98.99933624267578 ...\n",
            "\n",
            "step 37000, training loss= 0.00078237965, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 37050, training loss= 0.0005242865, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 37100, training loss= 0.00040388975, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 37150, training loss= 0.0006506585, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 37200, training loss= 0.00024260048, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37250, training loss= 0.00023173922, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37300, training loss= 0.00011187092, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37350, training loss= 0.00068294903, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37400, training loss= 0.00023820781, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37450, training loss= 0.00056968414, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37500, training loss= 0.00016099768, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37550, training loss= 0.00016321927, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37600, training loss= 0.00025927948, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37650, training loss= 0.00019619071, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37700, training loss= 0.00012093968, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37750, training loss= 0.00020452974, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 37800, training loss= 0.00010977308, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 37850, training loss= 0.0003291137, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 37900, training loss= 0.00023495714, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 37950, training loss= 7.023775e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 38000, training loss= 0.00012379822, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 38050, training loss= 0.0002348393, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 38100, training loss= 0.0010304481, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 38150, training loss= 0.0002512446, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 38200, training loss= 0.00020759566, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 38250, training loss= 0.00028369992, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38300, training loss= 0.0001717642, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38350, training loss= 0.000118492026, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38400, training loss= 0.000836581, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38450, training loss= 9.6357464e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38500, training loss= 7.571944e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38550, training loss= 0.00017737168, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38600, training loss= 0.00021806746, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38650, training loss= 7.8127254e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38700, training loss= 7.858137e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38750, training loss= 0.00015063718, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38800, training loss= 0.00015525907, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38850, training loss= 9.9992874e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38900, training loss= 7.616337e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 38950, training loss= 0.000103641934, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39000, training loss= 4.793093e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39050, training loss= 4.8621587e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39100, training loss= 0.00018078998, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39150, training loss= 0.000102932725, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39200, training loss= 8.512596e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39250, training loss= 3.898871e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39300, training loss= 0.00010826381, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39350, training loss= 0.00011156293, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39400, training loss= 0.00017524915, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39450, training loss= 8.207859e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39500, training loss= 0.000100693855, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39550, training loss= 0.00013879225, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39600, training loss= 5.205572e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 39650, training loss= 6.939973e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 39700, training loss= 0.00010925757, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 39750, training loss= 0.00051799696, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39800, training loss= 0.0007874803, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39850, training loss= 8.449173e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 39900, training loss= 4.3331172e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 39950, training loss= 8.2574596e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40000, training loss= 5.1720577e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40050, training loss= 6.317602e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40100, training loss= 4.888557e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40150, training loss= 0.00010105539, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40200, training loss= 0.00011536422, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40250, training loss= 7.198538e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40300, training loss= 0.00013073701, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40350, training loss= 6.0363465e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40400, training loss= 6.094218e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40450, training loss= 0.00023161212, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 40500, training loss= 0.00011080251, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 40550, training loss= 0.0029314724, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 40600, training loss= 0.0017402737, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 40650, training loss= 0.00040403634, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 40700, training loss= 0.00042928118, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 40750, training loss= 0.00011531971, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 40800, training loss= 0.00041146905, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 40850, training loss= 0.0005887217, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 40900, training loss= 0.00021515497, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 40950, training loss= 0.00013836639, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41000, training loss= 0.00025775962, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41050, training loss= 0.00020548177, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41100, training loss= 0.0001836515, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41150, training loss= 0.00021041639, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41200, training loss= 0.00018740994, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41250, training loss= 9.594872e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41300, training loss= 0.0002187906, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41350, training loss= 0.0005377145, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41400, training loss= 0.000253692, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41450, training loss= 6.6230794e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41500, training loss= 0.00024213095, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41550, training loss= 0.0002000435, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41600, training loss= 0.00016764228, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 41650, training loss= 0.00010362015, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 41700, training loss= 0.00014591958, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41750, training loss= 0.00012553569, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 41800, training loss= 0.00015085979, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 41850, training loss= 9.043879e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 41900, training loss= 9.922996e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 41950, training loss= 0.00019595522, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 42000, training loss= 0.00010024498, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 42050, training loss= 5.8124162e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 42100, training loss= 0.00023520892, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 42150, training loss= 0.00010830803, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 42200, training loss= 7.383522e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42250, training loss= 5.0536186e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42300, training loss= 3.1616004e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42350, training loss= 0.00011495728, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42400, training loss= 0.00012863179, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42450, training loss= 5.555506e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42500, training loss= 0.00011657312, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42550, training loss= 3.6157355e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42600, training loss= 0.00023611778, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42650, training loss= 2.2469601e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42700, training loss= 9.9999874e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42750, training loss= 0.00012685132, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42800, training loss= 6.516625e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42850, training loss= 6.582781e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42900, training loss= 0.0001767561, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 42950, training loss= 7.7397926e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43000, training loss= 4.3390803e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43050, training loss= 7.650673e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43100, training loss= 5.84611e-05, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43150, training loss= 0.00017356718, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 43200, training loss= 8.69128e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43250, training loss= 5.999961e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43300, training loss= 2.5281532e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43350, training loss= 9.84981e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43400, training loss= 3.78385e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 43450, training loss= 9.178153e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 43500, training loss= 0.0003126404, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 43550, training loss= 0.0001608723, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 43600, training loss= 0.000102440266, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43650, training loss= 3.163923e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43700, training loss= 2.7548724e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43750, training loss= 4.443513e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 43800, training loss= 4.412981e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43850, training loss= 0.00011551518, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43900, training loss= 3.626057e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 43950, training loss= 2.6438933e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44000, training loss= 2.8247407e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44050, training loss= 6.177815e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 44100, training loss= 3.273028e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44150, training loss= 4.2626823e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44200, training loss= 9.2676804e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44250, training loss= 3.3787095e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44300, training loss= 4.931859e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44350, training loss= 8.7015826e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44400, training loss= 0.00014493831, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44450, training loss= 0.00035627044, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 44500, training loss= 5.8035308e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 44550, training loss= 3.3068485e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 44600, training loss= 8.912147e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44650, training loss= 3.535591e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44700, training loss= 1.3974192e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 44750, training loss= 2.7603324e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44800, training loss= 1.4563923e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44850, training loss= 4.20331e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 44900, training loss= 0.0006963075, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 44950, training loss= 2.0589241e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45000, training loss= 2.3739241e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45050, training loss= 6.291773e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45100, training loss= 0.00010376078, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 45150, training loss= 3.2206088e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 45200, training loss= 2.4106374e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 45250, training loss= 3.5692316e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 45300, training loss= 0.0029752632, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 45350, training loss= 0.03583721, training acc= 99.50000047683716%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45400, training loss= 0.0003887049, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 45450, training loss= 0.0003040541, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45500, training loss= 0.00041186198, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45550, training loss= 0.001171918, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 45600, training loss= 9.4405e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45650, training loss= 0.00059006165, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 45700, training loss= 0.000100649006, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 45750, training loss= 0.00019448498, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45800, training loss= 0.00010938024, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 45850, training loss= 6.515737e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 45900, training loss= 4.933025e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 45950, training loss= 0.00015604141, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 46000, training loss= 0.00025573582, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 46050, training loss= 0.000117134565, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46100, training loss= 0.00010393098, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46150, training loss= 0.00032345564, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 46200, training loss= 0.0002757827, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46250, training loss= 7.253796e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46300, training loss= 0.00010236348, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46350, training loss= 0.00012575978, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46400, training loss= 8.741321e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46450, training loss= 8.184772e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46500, training loss= 0.00011293985, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46550, training loss= 0.00016406632, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46600, training loss= 2.9632885e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46650, training loss= 9.677881e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46700, training loss= 0.00015575284, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46750, training loss= 9.85855e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46800, training loss= 0.000102033686, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46850, training loss= 6.0827457e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46900, training loss= 0.00014865352, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 46950, training loss= 5.9769292e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47000, training loss= 6.8235604e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 47050, training loss= 3.8523525e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47100, training loss= 6.451976e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47150, training loss= 0.00013282984, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 47200, training loss= 4.904021e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47250, training loss= 4.1567488e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47300, training loss= 8.75449e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47350, training loss= 6.389235e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47400, training loss= 6.1479324e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47450, training loss= 7.134329e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47500, training loss= 6.657223e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47550, training loss= 6.535087e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47600, training loss= 0.00022575058, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47650, training loss= 6.860949e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47700, training loss= 0.00014921946, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47750, training loss= 2.338156e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47800, training loss= 7.904406e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47850, training loss= 0.00011648633, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47900, training loss= 3.779095e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 47950, training loss= 2.9022593e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48000, training loss= 0.00012721131, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48050, training loss= 3.074876e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48100, training loss= 0.00036901113, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48150, training loss= 3.7446258e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48200, training loss= 4.0787865e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48250, training loss= 6.0773553e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48300, training loss= 6.6499895e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48350, training loss= 6.896359e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48400, training loss= 4.7772297e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48450, training loss= 8.1753526e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48500, training loss= 2.4758532e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48550, training loss= 3.0898005e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48600, training loss= 1.679027e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48650, training loss= 7.894833e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 48700, training loss= 3.9823706e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 48750, training loss= 0.00023208093, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 48800, training loss= 2.8246159e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 48850, training loss= 5.280683e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48900, training loss= 2.3760767e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 48950, training loss= 8.91109e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49000, training loss= 3.81516e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49050, training loss= 6.495289e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49100, training loss= 5.3934775e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49150, training loss= 4.8880527e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49200, training loss= 4.3162716e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49250, training loss= 8.630342e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49300, training loss= 2.9817385e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49350, training loss= 3.738104e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49400, training loss= 1.6453527e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49450, training loss= 3.1001124e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49500, training loss= 0.00014532852, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49550, training loss= 3.9607025e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49600, training loss= 3.798663e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49650, training loss= 3.1357336e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49700, training loss= 2.6192678e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49750, training loss= 2.6916445e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49800, training loss= 3.3225748e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49850, training loss= 2.3036693e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49900, training loss= 2.6958029e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 49950, training loss= 9.47903e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50000, training loss= 1.7367778e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50050, training loss= 1.3268736e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50100, training loss= 0.00015164042, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50150, training loss= 2.4336836e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50200, training loss= 2.100258e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50250, training loss= 2.5816136e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50300, training loss= 3.911355e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50350, training loss= 1.2721188e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50400, training loss= 2.1841724e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50450, training loss= 1.5122408e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 50500, training loss= 2.4628062e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50550, training loss= 4.014248e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50600, training loss= 4.7922877e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50650, training loss= 1.05858835e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50700, training loss= 4.2487023e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50750, training loss= 1.4839757e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50800, training loss= 1.7004337e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50850, training loss= 1.7178867e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50900, training loss= 1.395958e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 50950, training loss= 0.0014074658, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 51000, training loss= 0.00022281412, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51050, training loss= 0.0001668612, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 51100, training loss= 7.978783e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 51150, training loss= 0.00013462124, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 51200, training loss= 0.00013785163, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 51250, training loss= 0.00018963232, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 51300, training loss= 6.282311e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 51350, training loss= 0.00016580711, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 51400, training loss= 0.000199106, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 51450, training loss= 0.00013546049, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 51500, training loss= 6.934358e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51550, training loss= 0.0013954496, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51600, training loss= 8.981665e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 51650, training loss= 4.697399e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51700, training loss= 0.00023646686, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51750, training loss= 2.1953616e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 51800, training loss= 0.00013449855, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51850, training loss= 3.7449256e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51900, training loss= 4.374684e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 51950, training loss= 0.000119099765, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52000, training loss= 4.55777e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52050, training loss= 0.0003624518, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52100, training loss= 6.715287e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52150, training loss= 5.8901587e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52200, training loss= 5.850704e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52250, training loss= 3.8406688e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52300, training loss= 0.00010098371, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52350, training loss= 6.4160486e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52400, training loss= 3.677152e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52450, training loss= 0.00018661015, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52500, training loss= 0.00012843726, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52550, training loss= 1.4119821e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52600, training loss= 0.00019468222, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52650, training loss= 4.360257e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52700, training loss= 2.084779e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52750, training loss= 0.00011566669, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 52800, training loss= 6.231444e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52850, training loss= 0.00019165117, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52900, training loss= 4.640033e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 52950, training loss= 7.5943e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53000, training loss= 2.2035616e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 53050, training loss= 1.2481049e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53100, training loss= 4.1506893e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53150, training loss= 0.00010278296, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53200, training loss= 8.008029e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53250, training loss= 4.8234633e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53300, training loss= 3.4078013e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53350, training loss= 2.9725561e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53400, training loss= 0.00013755148, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53450, training loss= 4.7216246e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53500, training loss= 1.5699692e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53550, training loss= 4.8058762e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53600, training loss= 2.6436934e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53650, training loss= 6.316918e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53700, training loss= 1.9405263e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53750, training loss= 4.2887837e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53800, training loss= 3.5623623e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53850, training loss= 7.8967656e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53900, training loss= 5.190593e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 53950, training loss= 8.6364984e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54000, training loss= 0.000115370116, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54050, training loss= 3.435911e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54100, training loss= 0.00012863603, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54150, training loss= 5.449478e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54200, training loss= 3.2064858e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54250, training loss= 4.2933327e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54300, training loss= 4.379454e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54350, training loss= 6.145741e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54400, training loss= 5.6785673e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54450, training loss= 1.5304844e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54500, training loss= 2.8207722e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54550, training loss= 2.4662646e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54600, training loss= 2.4858584e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54650, training loss= 5.5790973e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54700, training loss= 1.496009e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54750, training loss= 4.0647672e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54800, training loss= 6.5190405e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54850, training loss= 2.5176085e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54900, training loss= 3.931392e-05, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 54950, training loss= 1.0240141e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55000, training loss= 3.2522315e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55050, training loss= 2.814707e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55100, training loss= 3.2359338e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55150, training loss= 2.4374725e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 55200, training loss= 7.4953605e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55250, training loss= 5.0625113e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55300, training loss= 1.8525583e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55350, training loss= 9.84969e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55400, training loss= 0.00014320013, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55450, training loss= 1.9097035e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55500, training loss= 5.317661e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55550, training loss= 1.778876e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55600, training loss= 2.6314276e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55650, training loss= 2.0508138e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55700, training loss= 2.0002364e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55750, training loss= 1.3652023e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55800, training loss= 8.9773006e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55850, training loss= 2.0805715e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55900, training loss= 1.779162e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 55950, training loss= 8.219552e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 56000, training loss= 3.230603e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 56050, training loss= 2.9429817e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 56100, training loss= 6.254835e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 56150, training loss= 1.034978e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 56200, training loss= 0.0036801193, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 56250, training loss= 0.0007552993, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 56300, training loss= 0.019628044, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 56350, training loss= 0.00041205192, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56400, training loss= 0.00015837523, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56450, training loss= 0.00016582839, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56500, training loss= 0.00013100276, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56550, training loss= 5.7263533e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56600, training loss= 7.531841e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56650, training loss= 0.000107064814, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56700, training loss= 8.487678e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56750, training loss= 0.00010153537, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56800, training loss= 5.3485106e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56850, training loss= 4.4184566e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56900, training loss= 0.0006395608, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 56950, training loss= 4.0827726e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 57000, training loss= 0.00013028023, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 57050, training loss= 0.0002363056, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 57100, training loss= 0.000107181324, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57150, training loss= 0.00012243589, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57200, training loss= 5.8166268e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57250, training loss= 8.462744e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57300, training loss= 9.597121e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57350, training loss= 0.0001722244, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 57400, training loss= 2.8232553e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57450, training loss= 7.310068e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 57500, training loss= 3.188885e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 57550, training loss= 2.0207386e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57600, training loss= 0.000109263085, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57650, training loss= 0.00012533317, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57700, training loss= 1.6551374e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57750, training loss= 8.2680686e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57800, training loss= 1.6376227e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 57850, training loss= 6.362387e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 57900, training loss= 0.00012080745, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 57950, training loss= 3.6171637e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58000, training loss= 5.614956e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58050, training loss= 6.156573e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58100, training loss= 8.166076e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58150, training loss= 0.00014031485, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58200, training loss= 4.1146304e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58250, training loss= 3.5546153e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58300, training loss= 2.7791173e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58350, training loss= 2.7514197e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58400, training loss= 2.7510783e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58450, training loss= 2.6783986e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58500, training loss= 0.00025166498, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58550, training loss= 4.6293702e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58600, training loss= 6.53851e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58650, training loss= 2.6702324e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58700, training loss= 6.952455e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58750, training loss= 1.8989167e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58800, training loss= 2.9852717e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58850, training loss= 7.683915e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 58900, training loss= 4.747023e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 58950, training loss= 2.4939007e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 59000, training loss= 5.2840096e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 59050, training loss= 3.0037698e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 59100, training loss= 1.9948182e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 59150, training loss= 2.259097e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59200, training loss= 4.832419e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 59250, training loss= 3.734016e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 59300, training loss= 3.113109e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 59350, training loss= 0.00010672514, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59400, training loss= 4.8585665e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59450, training loss= 4.5456163e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59500, training loss= 2.8914534e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59550, training loss= 2.8253282e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59600, training loss= 5.4502227e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59650, training loss= 5.1535386e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59700, training loss= 1.9963976e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59750, training loss= 1.1435965e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59800, training loss= 6.145254e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59850, training loss= 2.1092324e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59900, training loss= 1.4284e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 59950, training loss= 2.6330743e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60000, training loss= 4.563507e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60050, training loss= 7.4471072e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60100, training loss= 3.71719e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60150, training loss= 4.5696972e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60200, training loss= 2.405239e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60250, training loss= 0.00021044008, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60300, training loss= 2.9222287e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60350, training loss= 6.7900124e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60400, training loss= 1.0510451e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60450, training loss= 2.5670197e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60500, training loss= 1.8449871e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60550, training loss= 2.493267e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60600, training loss= 1.8627667e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60650, training loss= 1.8750483e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60700, training loss= 1.23602e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60750, training loss= 2.4269264e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60800, training loss= 4.430337e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60850, training loss= 3.9544288e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60900, training loss= 1.6647706e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 60950, training loss= 2.2962919e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61000, training loss= 8.133133e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61050, training loss= 1.01595215e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61100, training loss= 2.7136615e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61150, training loss= 1.2414676e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61200, training loss= 1.8287208e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61250, training loss= 1.2687127e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61300, training loss= 1.8112427e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61350, training loss= 1.1692348e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61400, training loss= 7.646819e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61450, training loss= 1.0080134e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61500, training loss= 1.4533242e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 61550, training loss= 0.00022245415, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 61600, training loss= 6.9198664e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 61650, training loss= 0.0007054735, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 61700, training loss= 0.0002719759, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 61750, training loss= 9.310471e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 61800, training loss= 8.8140005e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 61850, training loss= 2.4597737e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 61900, training loss= 0.000142182, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 61950, training loss= 0.00014360386, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62000, training loss= 8.492383e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 62050, training loss= 2.485783e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62100, training loss= 0.0002371998, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62150, training loss= 9.839126e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 62200, training loss= 9.399561e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 62250, training loss= 5.1298415e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 62300, training loss= 0.00013388332, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 62350, training loss= 4.3486096e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62400, training loss= 2.953886e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62450, training loss= 2.1781083e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62500, training loss= 7.188112e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62550, training loss= 7.4162264e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62600, training loss= 5.1907304e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62650, training loss= 4.173524e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62700, training loss= 2.5961379e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62750, training loss= 1.1269385e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62800, training loss= 0.000104918174, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62850, training loss= 7.831659e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62900, training loss= 3.082342e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 62950, training loss= 3.6916674e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63000, training loss= 7.708867e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 63050, training loss= 6.067909e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63100, training loss= 4.3358865e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63150, training loss= 1.8145993e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63200, training loss= 0.00020959767, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63250, training loss= 3.9048828e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63300, training loss= 4.483688e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 63350, training loss= 1.5038431e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 63400, training loss= 2.0088584e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63450, training loss= 7.505689e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63500, training loss= 2.4669758e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63550, training loss= 1.57475e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63600, training loss= 3.380623e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63650, training loss= 0.00015129412, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63700, training loss= 7.262395e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63750, training loss= 6.930684e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63800, training loss= 4.5687888e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63850, training loss= 2.1009733e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 63900, training loss= 1.5258252e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 63950, training loss= 2.8418543e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64000, training loss= 3.455777e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 64050, training loss= 1.2816701e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64100, training loss= 1.950969e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 64150, training loss= 5.6208714e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 64200, training loss= 5.5929428e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 64250, training loss= 1.4679105e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64300, training loss= 1.5844093e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64350, training loss= 1.2058251e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64400, training loss= 3.290869e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64450, training loss= 6.901244e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64500, training loss= 2.0927646e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 64550, training loss= 1.5656957e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64600, training loss= 1.8785466e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 64650, training loss= 2.2803273e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 64700, training loss= 2.3791437e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64750, training loss= 2.6777516e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 64800, training loss= 1.3062455e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 64850, training loss= 2.3573475e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 64900, training loss= 9.93394e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 64950, training loss= 1.5128842e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 65000, training loss= 2.0075766e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 65050, training loss= 5.6020046e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65100, training loss= 1.4195482e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65150, training loss= 2.5353209e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65200, training loss= 1.2061724e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65250, training loss= 2.1278169e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65300, training loss= 1.48272375e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 65350, training loss= 1.8341623e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65400, training loss= 1.7457029e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65450, training loss= 4.0120944e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65500, training loss= 2.262286e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65550, training loss= 1.1972176e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65600, training loss= 2.0527092e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65650, training loss= 1.0793922e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65700, training loss= 2.8473865e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65750, training loss= 2.5809972e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65800, training loss= 3.9072565e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 65850, training loss= 2.0966998e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 65900, training loss= 1.0813722e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 65950, training loss= 1.3336507e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 66000, training loss= 5.584182e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66050, training loss= 1.03380235e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66100, training loss= 5.2042556e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66150, training loss= 8.56075e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 66200, training loss= 1.6003165e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66250, training loss= 1.3783059e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66300, training loss= 1.647147e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 66350, training loss= 2.3588724e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66400, training loss= 1.0440553e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66450, training loss= 9.498557e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66500, training loss= 2.166845e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66550, training loss= 3.4636898e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66600, training loss= 1.4178629e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 66650, training loss= 4.821481e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 66700, training loss= 7.4475365e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66750, training loss= 7.2823773e-06, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 66800, training loss= 1.689697e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66850, training loss= 1.1212075e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 66900, training loss= 1.1537765e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 66950, training loss= 1.097558e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67000, training loss= 1.416133e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 67050, training loss= 1.10802e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 67100, training loss= 3.5572534e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 67150, training loss= 8.369124e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 67200, training loss= 8.514719e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 67250, training loss= 1.46020275e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 67300, training loss= 6.224711e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67350, training loss= 1.387035e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 67400, training loss= 0.00028378537, training acc= 100.0%\n",
            "Validation Accuracy 99.19947052001953 ...\n",
            "\n",
            "step 67450, training loss= 0.0010655102, training acc= 100.0%\n",
            "Validation Accuracy 99.13275146484375 ...\n",
            "\n",
            "step 67500, training loss= 9.3225775e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 67550, training loss= 4.0135703e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67600, training loss= 9.897427e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 67650, training loss= 0.00013349176, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67700, training loss= 0.00036260026, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67750, training loss= 5.0436753e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67800, training loss= 9.497693e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67850, training loss= 4.71045e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67900, training loss= 0.00011740217, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 67950, training loss= 8.803756e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 68000, training loss= 3.3056334e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 68050, training loss= 1.644086e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 68100, training loss= 5.1950494e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68150, training loss= 6.0610233e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68200, training loss= 7.580973e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68250, training loss= 2.6732603e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68300, training loss= 2.1526128e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68350, training loss= 1.0312548e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68400, training loss= 2.8569726e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68450, training loss= 4.8173373e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68500, training loss= 0.0001474086, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68550, training loss= 3.5115158e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68600, training loss= 5.3591528e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68650, training loss= 3.6331152e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68700, training loss= 2.913224e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68750, training loss= 1.4969869e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68800, training loss= 5.0097977e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68850, training loss= 7.702807e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68900, training loss= 5.212405e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 68950, training loss= 2.6685331e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69000, training loss= 9.187392e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69050, training loss= 2.6745292e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69100, training loss= 2.3789182e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69150, training loss= 3.3969085e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69200, training loss= 1.0965234e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69250, training loss= 3.0065303e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69300, training loss= 2.0677722e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69350, training loss= 3.1928037e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69400, training loss= 4.6682875e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69450, training loss= 1.9587016e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69500, training loss= 1.802901e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69550, training loss= 6.244744e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69600, training loss= 3.127336e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69650, training loss= 1.1469352e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69700, training loss= 2.3231809e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69750, training loss= 8.808766e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69800, training loss= 1.1821409e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69850, training loss= 1.1838888e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69900, training loss= 1.7253906e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 69950, training loss= 2.2827704e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70000, training loss= 5.219406e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70050, training loss= 9.8374345e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70100, training loss= 7.208265e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70150, training loss= 2.2202974e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70200, training loss= 2.8341725e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70250, training loss= 9.666039e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70300, training loss= 3.685243e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70350, training loss= 2.2264598e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70400, training loss= 3.5714602e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70450, training loss= 1.2928343e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70500, training loss= 8.562156e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70550, training loss= 9.965401e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 70600, training loss= 6.6299854e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70650, training loss= 1.1756751e-05, training acc= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70700, training loss= 3.20987e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70750, training loss= 1.4522185e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70800, training loss= 3.5363137e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70850, training loss= 1.095757e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70900, training loss= 4.695547e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 70950, training loss= 1.1859764e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71000, training loss= 3.5710727e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71050, training loss= 1.3941617e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71100, training loss= 9.191594e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71150, training loss= 8.560621e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71200, training loss= 1.9720446e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71250, training loss= 9.677946e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71300, training loss= 2.9714192e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71350, training loss= 9.265855e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71400, training loss= 1.029444e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71450, training loss= 2.7459182e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71500, training loss= 4.6221758e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71550, training loss= 2.5946887e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71600, training loss= 2.807444e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71650, training loss= 8.5411275e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71700, training loss= 1.0921973e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71750, training loss= 2.6145595e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71800, training loss= 6.202807e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71850, training loss= 9.801672e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71900, training loss= 7.884134e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 71950, training loss= 2.0023776e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72000, training loss= 1.6756207e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72050, training loss= 1.1350354e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72100, training loss= 1.2399327e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72150, training loss= 6.284641e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72200, training loss= 9.209911e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72250, training loss= 5.2152377e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72300, training loss= 5.354116e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72350, training loss= 1.4180827e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72400, training loss= 3.084388e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 72450, training loss= 8.479171e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 72500, training loss= 0.00033485645, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 72550, training loss= 9.286581e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 72600, training loss= 5.7585068e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 72650, training loss= 0.00013457797, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 72700, training loss= 3.717171e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 72750, training loss= 4.958396e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72800, training loss= 5.6158447e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72850, training loss= 7.817247e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72900, training loss= 6.836683e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 72950, training loss= 4.538375e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73000, training loss= 5.3392854e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73050, training loss= 2.2327067e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73100, training loss= 5.1265073e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73150, training loss= 2.063141e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73200, training loss= 2.3288232e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73250, training loss= 0.0001266975, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73300, training loss= 2.7807733e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73350, training loss= 1.232362e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73400, training loss= 1.2120267e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73450, training loss= 2.2434144e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73500, training loss= 1.291452e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73550, training loss= 2.7119166e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73600, training loss= 2.6941088e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73650, training loss= 3.9772362e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73700, training loss= 8.434974e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73750, training loss= 4.2222484e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73800, training loss= 6.123652e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73850, training loss= 8.6405635e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 73900, training loss= 5.8832907e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 73950, training loss= 5.577019e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 74000, training loss= 4.255468e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74050, training loss= 4.225681e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74100, training loss= 2.1938147e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74150, training loss= 1.0436573e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74200, training loss= 1.4898285e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74250, training loss= 2.7649363e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74300, training loss= 2.2937924e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74350, training loss= 1.6540955e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74400, training loss= 1.4728724e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74450, training loss= 6.3511e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74500, training loss= 2.3341667e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74550, training loss= 1.9247713e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 74600, training loss= 1.0788188e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74650, training loss= 2.5494563e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 74700, training loss= 3.4467615e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74750, training loss= 1.661714e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74800, training loss= 1.6588934e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74850, training loss= 1.3074127e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74900, training loss= 1.2981947e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 74950, training loss= 4.1587296e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 75000, training loss= 1.3937446e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 75050, training loss= 8.22777e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 75100, training loss= 2.297333e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 75150, training loss= 1.237036e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75200, training loss= 1.9942972e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75250, training loss= 7.740155e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75300, training loss= 1.3040398e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75350, training loss= 1.4915483e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75400, training loss= 3.1523584e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75450, training loss= 2.7690909e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75500, training loss= 1.9249988e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75550, training loss= 2.8652077e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75600, training loss= 1.0491275e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75650, training loss= 8.189109e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75700, training loss= 5.359722e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75750, training loss= 1.2773551e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75800, training loss= 4.998809e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75850, training loss= 1.046831e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75900, training loss= 7.3396204e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 75950, training loss= 1.9020332e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76000, training loss= 2.8478717e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76050, training loss= 1.9365092e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76100, training loss= 4.8844563e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76150, training loss= 7.5433154e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76200, training loss= 3.9376705e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76250, training loss= 1.0577386e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76300, training loss= 6.6947473e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76350, training loss= 1.9048747e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76400, training loss= 7.1347026e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76450, training loss= 2.5594954e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76500, training loss= 1.03067605e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76550, training loss= 1.6537742e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76600, training loss= 1.3700571e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76650, training loss= 5.98687e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76700, training loss= 3.2970747e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76750, training loss= 2.9916914e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76800, training loss= 3.4574646e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76850, training loss= 4.210609e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76900, training loss= 9.000943e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 76950, training loss= 1.0833402e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77000, training loss= 7.788432e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77050, training loss= 7.823298e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77100, training loss= 1.9408151e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77150, training loss= 1.1272313e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77200, training loss= 4.5545944e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77250, training loss= 7.5622143e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77300, training loss= 5.3654508e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77350, training loss= 7.817305e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77400, training loss= 4.166304e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77450, training loss= 3.5179141e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77500, training loss= 9.215704e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77550, training loss= 3.156091e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77600, training loss= 3.4567104e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77650, training loss= 9.8153305e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77700, training loss= 3.419909e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77750, training loss= 4.7771355e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77800, training loss= 3.1512727e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77850, training loss= 4.227104e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77900, training loss= 3.574189e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 77950, training loss= 8.001719e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 78000, training loss= 0.0006349783, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 78050, training loss= 0.0052851588, training acc= 100.0%\n",
            "Validation Accuracy 98.79920196533203 ...\n",
            "\n",
            "step 78100, training loss= 0.00046816326, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 78150, training loss= 0.0036489288, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 78200, training loss= 8.6620945e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78250, training loss= 6.795363e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78300, training loss= 7.2140654e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78350, training loss= 4.9553797e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 78400, training loss= 0.0010752534, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78450, training loss= 6.58337e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78500, training loss= 7.188156e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 78550, training loss= 4.7078254e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78600, training loss= 0.00010571879, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78650, training loss= 0.00019085052, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78700, training loss= 3.1185657e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78750, training loss= 0.00014102904, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78800, training loss= 3.5869798e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78850, training loss= 9.851737e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78900, training loss= 7.002343e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 78950, training loss= 7.00325e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79000, training loss= 1.6654265e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79050, training loss= 3.5495803e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79100, training loss= 5.1705156e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79150, training loss= 9.938478e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79200, training loss= 0.0006174686, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79250, training loss= 1.3294817e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79300, training loss= 9.826761e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79350, training loss= 4.577169e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79400, training loss= 3.7403584e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79450, training loss= 5.2542768e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 79500, training loss= 2.2236725e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79550, training loss= 8.764853e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79600, training loss= 2.2906825e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79650, training loss= 2.0109663e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79700, training loss= 6.32729e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79750, training loss= 3.39455e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79800, training loss= 4.834102e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79850, training loss= 2.3751045e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79900, training loss= 2.4571185e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 79950, training loss= 2.3768147e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 80000, training loss= 1.0582008e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 80050, training loss= 1.1681856e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 80100, training loss= 2.9565934e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 80150, training loss= 1.8599749e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80200, training loss= 2.8554883e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80250, training loss= 1.1064109e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80300, training loss= 1.2641501e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80350, training loss= 3.0731015e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80400, training loss= 8.797203e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80450, training loss= 2.907999e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80500, training loss= 4.734057e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80550, training loss= 1.1849349e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80600, training loss= 7.476788e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80650, training loss= 9.512336e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80700, training loss= 7.240263e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80750, training loss= 8.910508e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80800, training loss= 3.0438932e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80850, training loss= 2.6736801e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80900, training loss= 5.5850937e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 80950, training loss= 8.943664e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81000, training loss= 4.541587e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81050, training loss= 3.165622e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81100, training loss= 1.1067446e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81150, training loss= 3.0239818e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81200, training loss= 1.0645689e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81250, training loss= 1.320461e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81300, training loss= 6.79458e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81350, training loss= 6.22309e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81400, training loss= 6.4866185e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81450, training loss= 1.8841189e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81500, training loss= 4.977511e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81550, training loss= 1.6785936e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81600, training loss= 2.4872437e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81650, training loss= 5.306311e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81700, training loss= 4.9986527e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81750, training loss= 7.905319e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81800, training loss= 2.1176182e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81850, training loss= 6.630164e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81900, training loss= 6.9725447e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 81950, training loss= 8.13243e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82000, training loss= 2.5645743e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82050, training loss= 8.558273e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82100, training loss= 6.965545e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82150, training loss= 1.941008e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82200, training loss= 5.3429694e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82250, training loss= 2.7243877e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82300, training loss= 6.5541353e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82350, training loss= 5.114861e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82400, training loss= 1.0090805e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82450, training loss= 8.47497e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 82500, training loss= 2.1532192e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82550, training loss= 6.3065945e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82600, training loss= 1.1721385e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82650, training loss= 3.5781816e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82700, training loss= 1.8107385e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82750, training loss= 1.6301248e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82800, training loss= 5.0687418e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82850, training loss= 8.131596e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82900, training loss= 4.398148e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 82950, training loss= 8.747734e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83000, training loss= 2.4618203e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83050, training loss= 2.8577464e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83100, training loss= 2.481697e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83150, training loss= 4.2355125e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83200, training loss= 7.281889e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83250, training loss= 6.0578222e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83300, training loss= 9.235821e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83350, training loss= 4.4722133e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 83400, training loss= 9.4783594e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 83450, training loss= 3.854467e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83500, training loss= 1.0171015e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83550, training loss= 1.9353133e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83600, training loss= 6.856554e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83650, training loss= 4.1294516e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83700, training loss= 5.5501923e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83750, training loss= 9.551668e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83800, training loss= 3.2455541e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83850, training loss= 6.5661507e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83900, training loss= 2.1330332e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 83950, training loss= 3.845366e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84000, training loss= 4.999636e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 84050, training loss= 1.6063887e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 84100, training loss= 2.606473e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84150, training loss= 2.872694e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84200, training loss= 3.0050058e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84250, training loss= 4.680516e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84300, training loss= 3.897427e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84350, training loss= 8.258558e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84400, training loss= 2.1235172e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84450, training loss= 3.7977316e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84500, training loss= 1.8218346e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84550, training loss= 2.9366247e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84600, training loss= 6.6723255e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84650, training loss= 1.899517e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84700, training loss= 4.889295e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84750, training loss= 6.217921e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 84800, training loss= 0.0032709623, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 84850, training loss= 0.0016158471, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 84900, training loss= 0.0018432721, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 84950, training loss= 0.00045339853, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 85000, training loss= 7.676738e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85050, training loss= 2.8072682e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85100, training loss= 7.8288474e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 85150, training loss= 0.00012470261, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85200, training loss= 1.3107117e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 85250, training loss= 6.912864e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85300, training loss= 2.8448098e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85350, training loss= 6.945704e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85400, training loss= 2.4206609e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85450, training loss= 2.5938558e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85500, training loss= 4.494718e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85550, training loss= 2.7412992e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85600, training loss= 2.8539585e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85650, training loss= 1.5436375e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85700, training loss= 5.8195004e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85750, training loss= 3.441086e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85800, training loss= 2.4807403e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85850, training loss= 1.9527284e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85900, training loss= 1.3432784e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 85950, training loss= 6.7792935e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86000, training loss= 8.362021e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86050, training loss= 1.5488478e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86100, training loss= 4.936672e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86150, training loss= 2.3504934e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86200, training loss= 2.8555547e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86250, training loss= 9.761331e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86300, training loss= 1.3342425e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86350, training loss= 3.3827244e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 86400, training loss= 1.4726265e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86450, training loss= 3.8365168e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86500, training loss= 0.00012279392, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86550, training loss= 9.797768e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86600, training loss= 9.83348e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86650, training loss= 1.2758528e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 86700, training loss= 5.892818e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 86750, training loss= 9.752139e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 86800, training loss= 7.180284e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 86850, training loss= 7.7322775e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 86900, training loss= 7.1315e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 86950, training loss= 1.9377288e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87000, training loss= 6.778908e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87050, training loss= 1.1794854e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87100, training loss= 1.4024307e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87150, training loss= 2.2733666e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87200, training loss= 1.229699e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87250, training loss= 1.8223845e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87300, training loss= 2.0131109e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87350, training loss= 6.753233e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87400, training loss= 6.393919e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87450, training loss= 3.153505e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87500, training loss= 1.8613555e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87550, training loss= 6.6424345e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87600, training loss= 2.3116183e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87650, training loss= 8.422132e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87700, training loss= 7.488398e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87750, training loss= 1.0711688e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87800, training loss= 6.458399e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87850, training loss= 2.3258275e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87900, training loss= 5.607857e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 87950, training loss= 2.0322494e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88000, training loss= 1.6420294e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88050, training loss= 9.161242e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88100, training loss= 6.515366e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88150, training loss= 7.927405e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88200, training loss= 5.191791e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88250, training loss= 4.4997328e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88300, training loss= 4.6817837e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88350, training loss= 3.371135e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88400, training loss= 1.196015e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88450, training loss= 3.7577668e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88500, training loss= 1.7815733e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88550, training loss= 1.170105e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88600, training loss= 7.551461e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88650, training loss= 4.5021147e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88700, training loss= 7.501813e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88750, training loss= 7.2257244e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88800, training loss= 1.37907855e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88850, training loss= 8.891178e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88900, training loss= 6.1729625e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 88950, training loss= 1.1763816e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 89000, training loss= 4.561651e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 89050, training loss= 6.638124e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 89100, training loss= 7.686403e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 89150, training loss= 6.037418e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 89200, training loss= 7.4331447e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89250, training loss= 4.767166e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 89300, training loss= 3.903754e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89350, training loss= 5.8017067e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 89400, training loss= 5.5286578e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89450, training loss= 2.3808348e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89500, training loss= 6.543052e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89550, training loss= 2.074183e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89600, training loss= 3.0476383e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89650, training loss= 3.487546e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89700, training loss= 7.057708e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89750, training loss= 3.5166127e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89800, training loss= 5.4586385e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89850, training loss= 3.2113358e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89900, training loss= 2.3063146e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 89950, training loss= 7.006648e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90000, training loss= 4.02229e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90050, training loss= 6.1011942e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90100, training loss= 2.5645404e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90150, training loss= 6.693373e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90200, training loss= 1.3213406e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90250, training loss= 5.1292873e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90300, training loss= 2.8995926e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 90350, training loss= 1.9566837e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90400, training loss= 2.738634e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90450, training loss= 4.398576e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90500, training loss= 5.1447737e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90550, training loss= 4.0492014e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90600, training loss= 5.208869e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 90650, training loss= 0.00014163382, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 90700, training loss= 0.00021343397, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 90750, training loss= 5.9526747e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 90800, training loss= 9.493276e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 90850, training loss= 6.9200396e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 90900, training loss= 4.59972e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 90950, training loss= 4.046059e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 91000, training loss= 3.5099274e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 91050, training loss= 1.5210444e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 91100, training loss= 1.4736923e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 91150, training loss= 2.475767e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 91200, training loss= 2.501717e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91250, training loss= 1.7217702e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91300, training loss= 4.9482598e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91350, training loss= 3.982967e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91400, training loss= 6.791086e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91450, training loss= 9.473207e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91500, training loss= 1.5938547e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91550, training loss= 3.4254997e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91600, training loss= 6.0750885e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91650, training loss= 2.3637569e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91700, training loss= 1.443206e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91750, training loss= 4.339381e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91800, training loss= 4.581713e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91850, training loss= 2.4935103e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91900, training loss= 1.8985911e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 91950, training loss= 5.213032e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92000, training loss= 2.9809653e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92050, training loss= 2.8755783e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92100, training loss= 6.7554254e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92150, training loss= 4.2163112e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92200, training loss= 6.573701e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92250, training loss= 1.7741459e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92300, training loss= 4.1457893e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92350, training loss= 3.9395796e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92400, training loss= 2.249328e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92450, training loss= 8.5075435e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92500, training loss= 4.3157226e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92550, training loss= 9.094627e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92600, training loss= 2.606244e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92650, training loss= 4.876159e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92700, training loss= 3.9041793e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92750, training loss= 5.771988e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92800, training loss= 2.982297e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92850, training loss= 9.445212e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92900, training loss= 1.1376196e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 92950, training loss= 4.95583e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93000, training loss= 3.3832048e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93050, training loss= 1.540471e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93100, training loss= 1.5909873e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93150, training loss= 7.904916e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93200, training loss= 1.956483e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93250, training loss= 6.273443e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93300, training loss= 6.1066135e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93350, training loss= 1.7982797e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93400, training loss= 1.7324426e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93450, training loss= 2.2966757e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93500, training loss= 9.339066e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93550, training loss= 3.7844252e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93600, training loss= 5.4658362e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93650, training loss= 5.796379e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93700, training loss= 3.9968627e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93750, training loss= 1.2921217e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93800, training loss= 2.3337261e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93850, training loss= 3.405794e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93900, training loss= 6.1461624e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 93950, training loss= 1.6532043e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94000, training loss= 5.306907e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94050, training loss= 3.8166168e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94100, training loss= 7.269071e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94150, training loss= 1.5111383e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94200, training loss= 5.578608e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94250, training loss= 3.2476219e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 94300, training loss= 7.726373e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94350, training loss= 4.3244377e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94400, training loss= 5.952838e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94450, training loss= 4.4257376e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94500, training loss= 3.7346474e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94550, training loss= 4.665068e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94600, training loss= 3.6620359e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94650, training loss= 3.988987e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94700, training loss= 9.329384e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94750, training loss= 8.579253e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94800, training loss= 1.5678104e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94850, training loss= 4.4967073e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94900, training loss= 2.8646318e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 94950, training loss= 6.6784796e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 95000, training loss= 6.5400322e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 95050, training loss= 9.682557e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 95100, training loss= 1.1976568e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 95150, training loss= 3.0279878e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 95200, training loss= 4.4098906e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95250, training loss= 2.6570317e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95300, training loss= 1.2672646e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95350, training loss= 1.3611984e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95400, training loss= 1.882099e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95450, training loss= 2.7619994e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95500, training loss= 2.6524287e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95550, training loss= 2.400761e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95600, training loss= 3.8302223e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95650, training loss= 2.1891556e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95700, training loss= 5.3363165e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95750, training loss= 2.5904076e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95800, training loss= 2.4210046e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95850, training loss= 7.756246e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95900, training loss= 3.4551554e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 95950, training loss= 1.1262916e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96000, training loss= 4.778608e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96050, training loss= 3.1241404e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96100, training loss= 1.765938e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96150, training loss= 3.0182764e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96200, training loss= 1.696399e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96250, training loss= 2.6822754e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96300, training loss= 1.0168948e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96350, training loss= 1.1797035e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96400, training loss= 2.5652062e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96450, training loss= 2.114967e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96500, training loss= 2.6749128e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96550, training loss= 1.1567605e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96600, training loss= 8.813247e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96650, training loss= 6.83208e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96700, training loss= 5.11346e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96750, training loss= 1.4512445e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96800, training loss= 2.1178785e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96850, training loss= 2.3006812e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96900, training loss= 5.8441997e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 96950, training loss= 1.0944778e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97000, training loss= 1.5548294e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97050, training loss= 2.317999e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97100, training loss= 5.07307e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97150, training loss= 3.7088548e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97200, training loss= 2.501407e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97250, training loss= 2.3097637e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97300, training loss= 1.3911416e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97350, training loss= 1.6428679e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97400, training loss= 9.71915e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97450, training loss= 2.5732638e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97500, training loss= 1.1984782e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97550, training loss= 6.393204e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97600, training loss= 1.3616916e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97650, training loss= 7.7938654e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97700, training loss= 1.7330179e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97750, training loss= 2.029961e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97800, training loss= 7.067489e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97850, training loss= 2.371894e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97900, training loss= 9.955278e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 97950, training loss= 2.831309e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 98000, training loss= 5.529032e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 98050, training loss= 4.213746e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n",
            "step 98100, training loss= 9.5872e-07, training acc= 100.0%\n",
            "Validation Accuracy 99.733154296875 ...\n",
            "\n",
            "step 98150, training loss= 8.311357e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.66644287109375 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 98200, training loss= 0.0010265906, training acc= 100.0%\n",
            "Validation Accuracy 99.3328857421875 ...\n",
            "\n",
            "step 98250, training loss= 8.424942e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 98300, training loss= 9.1181166e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.39960479736328 ...\n",
            "\n",
            "step 98350, training loss= 0.00037295456, training acc= 100.0%\n",
            "Validation Accuracy 99.26617431640625 ...\n",
            "\n",
            "step 98400, training loss= 0.0002534828, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98450, training loss= 0.00023180536, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98500, training loss= 3.2789947e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98550, training loss= 1.3103706e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98600, training loss= 0.00018722317, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98650, training loss= 3.2737546e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98700, training loss= 0.000105306, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98750, training loss= 1.743038e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98800, training loss= 1.958734e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98850, training loss= 1.9411651e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98900, training loss= 2.3015917e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.46630859375 ...\n",
            "\n",
            "step 98950, training loss= 5.167103e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 99000, training loss= 7.9036035e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 99050, training loss= 2.0089525e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 99100, training loss= 2.3178407e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99150, training loss= 2.6859672e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99200, training loss= 2.95421e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99250, training loss= 2.0606465e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99300, training loss= 3.209108e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99350, training loss= 1.023911e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99400, training loss= 2.5508141e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99450, training loss= 7.0839747e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 99500, training loss= 1.1340597e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.53302001953125 ...\n",
            "\n",
            "step 99550, training loss= 2.7872084e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99600, training loss= 2.1263582e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99650, training loss= 3.4991153e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99700, training loss= 2.1541404e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99750, training loss= 1.291741e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99800, training loss= 7.8086705e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99850, training loss= 5.103378e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99900, training loss= 9.980943e-06, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "step 99950, training loss= 1.3705901e-05, training acc= 100.0%\n",
            "Validation Accuracy 99.5997314453125 ...\n",
            "\n",
            "Valid acc= 99.733154 %\n",
            "==================================================\n",
            "W1\n",
            "6\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "87YDnYnPypZm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import spline\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyq1OHxDypZp",
        "colab_type": "code",
        "outputId": "955d657a-e3cc-453c-e83a-0b52ca483bdc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, num_steps, plot_every)]\n",
        "# plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "# plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "plt.plot(steps_plot, savgol_filter(100*np.asarray(train_accuracy),51,1))  \n",
        "plt.plot(steps_plot, savgol_filter(np.asarray(val_accuracy),51,1))\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HINBozCsypZu",
        "colab_type": "code",
        "outputId": "e16db99f-6035-42ce-fece-fea71c9ac52a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_accuracy_filtered = savgol_filter(np.asarray(val_accuracy),51,1)\n",
        "print(max(valid_accuracy_filtered))\n",
        "valid_accuracy_filtered_np = np.asarray(valid_accuracy_filtered)\n",
        "print(np.argmax(valid_accuracy_filtered))\n",
        "print(steps_plot[np.argmax(valid_accuracy_filtered)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.733154\n",
            "1528\n",
            "76400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cqN7Sk73ypZx",
        "colab_type": "code",
        "outputId": "63b6617c-d939-46d9-881e-095a617e84eb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train_label_one_hot.shape)\n",
        "print(train_valid_combined.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5995, 10)\n",
            "(7494, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lQa7GLugypZ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 100\n",
        "aside_valid_test = train_valid_combined[-aside_examples:]\n",
        "aside_valid_test_label = validation_test_label_one_hot[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = validation_test_label_one_hot[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BdFAuDYkypZ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now retrain on this appended test data till 76400 steps"
      ]
    },
    {
      "metadata": {
        "id": "NFe5QWxVypZ7",
        "colab_type": "code",
        "outputId": "25fa0321-a5cc-4f8d-efaa-2fa488edf19a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "hid_neuron = [104]\n",
        "num_steps = 76400\n",
        "# num_steps = 20000\n",
        "\n",
        "batch_size = 200\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 6\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % plot_every == 0:\n",
        "            train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "            \n",
        "            train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "            train_accuracy.append(train_acc_total)\n",
        "            train_losses.append(train_loss_total)\n",
        "            print(\"step \" + str(step) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "    \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params =  G_W1np, G_b1np, G_W2np, G_b2np, G_W3np, G_b3np\n",
        "    W_track.append(this_params)\n",
        "    saver.save(sess, './PendigitAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, training loss Total= 107.80377, training acc total= 99.89180564880371%\n",
            "step 50, training loss Total= 2.910257, training acc total= 96.78117632865906%\n",
            "step 100, training loss Total= 1.1558126, training acc total= 97.132807970047%\n",
            "step 150, training loss Total= 0.555584, training acc total= 97.91722893714905%\n",
            "step 200, training loss Total= 0.30808684, training acc total= 98.63402843475342%\n",
            "step 250, training loss Total= 0.2063407, training acc total= 98.7422227859497%\n",
            "step 300, training loss Total= 0.1639376, training acc total= 98.98566603660583%\n",
            "step 350, training loss Total= 0.13866413, training acc total= 99.09386038780212%\n",
            "step 400, training loss Total= 0.11609118, training acc total= 99.13443326950073%\n",
            "step 450, training loss Total= 0.09607366, training acc total= 99.25615191459656%\n",
            "step 500, training loss Total= 0.08615331, training acc total= 99.3102490901947%\n",
            "step 550, training loss Total= 0.07135151, training acc total= 99.3914008140564%\n",
            "step 600, training loss Total= 0.06368588, training acc total= 99.40492510795593%\n",
            "step 650, training loss Total= 0.055022128, training acc total= 99.45902228355408%\n",
            "step 700, training loss Total= 0.05376835, training acc total= 99.5401680469513%\n",
            "step 750, training loss Total= 0.043866746, training acc total= 99.5401680469513%\n",
            "step 800, training loss Total= 0.04110361, training acc total= 99.60778951644897%\n",
            "step 850, training loss Total= 0.03789134, training acc total= 99.62131381034851%\n",
            "step 900, training loss Total= 0.034881245, training acc total= 99.66188669204712%\n",
            "step 950, training loss Total= 0.03231971, training acc total= 99.63483810424805%\n",
            "step 1000, training loss Total= 0.02896591, training acc total= 99.67541098594666%\n",
            "step 1050, training loss Total= 0.027416173, training acc total= 99.74303245544434%\n",
            "step 1100, training loss Total= 0.026245486, training acc total= 99.7295081615448%\n",
            "step 1150, training loss Total= 0.027479086, training acc total= 99.74303245544434%\n",
            "step 1200, training loss Total= 0.024219194, training acc total= 99.7295081615448%\n",
            "step 1250, training loss Total= 0.022088489, training acc total= 99.75655674934387%\n",
            "step 1300, training loss Total= 0.021095335, training acc total= 99.75655674934387%\n",
            "step 1350, training loss Total= 0.021773119, training acc total= 99.78361129760742%\n",
            "step 1400, training loss Total= 0.020578817, training acc total= 99.77008104324341%\n",
            "step 1450, training loss Total= 0.018677738, training acc total= 99.79713559150696%\n",
            "step 1500, training loss Total= 0.018623864, training acc total= 99.77008104324341%\n",
            "step 1550, training loss Total= 0.016973706, training acc total= 99.82418417930603%\n",
            "step 1600, training loss Total= 0.017353224, training acc total= 99.82418417930603%\n",
            "step 1650, training loss Total= 0.014502704, training acc total= 99.79713559150696%\n",
            "step 1700, training loss Total= 0.01402763, training acc total= 99.82418417930603%\n",
            "step 1750, training loss Total= 0.014438047, training acc total= 99.82418417930603%\n",
            "step 1800, training loss Total= 0.01334626, training acc total= 99.8512327671051%\n",
            "step 1850, training loss Total= 0.01296464, training acc total= 99.82418417930603%\n",
            "step 1900, training loss Total= 0.013570862, training acc total= 99.82418417930603%\n",
            "step 1950, training loss Total= 0.012760884, training acc total= 99.83770847320557%\n",
            "step 2000, training loss Total= 0.014259344, training acc total= 99.79713559150696%\n",
            "step 2050, training loss Total= 0.0135714505, training acc total= 99.82418417930603%\n",
            "step 2100, training loss Total= 0.01270663, training acc total= 99.87828135490417%\n",
            "step 2150, training loss Total= 0.010897068, training acc total= 99.83770847320557%\n",
            "step 2200, training loss Total= 0.010866306, training acc total= 99.8512327671051%\n",
            "step 2250, training loss Total= 0.01228426, training acc total= 99.87828135490417%\n",
            "step 2300, training loss Total= 0.0097982045, training acc total= 99.90532994270325%\n",
            "step 2350, training loss Total= 0.009918202, training acc total= 99.90532994270325%\n",
            "step 2400, training loss Total= 0.008976272, training acc total= 99.89180564880371%\n",
            "step 2450, training loss Total= 0.009260137, training acc total= 99.90532994270325%\n",
            "step 2500, training loss Total= 0.011479609, training acc total= 99.87828135490417%\n",
            "step 2550, training loss Total= 0.010282501, training acc total= 99.87828135490417%\n",
            "step 2600, training loss Total= 0.010559518, training acc total= 99.90532994270325%\n",
            "step 2650, training loss Total= 0.009073339, training acc total= 99.90532994270325%\n",
            "step 2700, training loss Total= 0.014146369, training acc total= 99.90532994270325%\n",
            "step 2750, training loss Total= 0.008366437, training acc total= 99.89180564880371%\n",
            "step 2800, training loss Total= 0.008382364, training acc total= 99.90532994270325%\n",
            "step 2850, training loss Total= 0.04102221, training acc total= 99.86475706100464%\n",
            "step 2900, training loss Total= 0.02082929, training acc total= 99.86475706100464%\n",
            "step 2950, training loss Total= 0.007908676, training acc total= 99.90532994270325%\n",
            "step 3000, training loss Total= 0.009736076, training acc total= 99.90532994270325%\n",
            "step 3050, training loss Total= 0.007506529, training acc total= 99.91885423660278%\n",
            "step 3100, training loss Total= 0.0072489614, training acc total= 99.93237853050232%\n",
            "step 3150, training loss Total= 0.008833012, training acc total= 99.89180564880371%\n",
            "step 3200, training loss Total= 0.0072017526, training acc total= 99.93237853050232%\n",
            "step 3250, training loss Total= 0.006689465, training acc total= 99.91885423660278%\n",
            "step 3300, training loss Total= 0.0077876165, training acc total= 99.91885423660278%\n",
            "step 3350, training loss Total= 0.009630045, training acc total= 99.93237853050232%\n",
            "step 3400, training loss Total= 0.0075914026, training acc total= 99.90532994270325%\n",
            "step 3450, training loss Total= 0.007887391, training acc total= 99.90532994270325%\n",
            "step 3500, training loss Total= 0.0068421084, training acc total= 99.91885423660278%\n",
            "step 3550, training loss Total= 0.0080583, training acc total= 99.90532994270325%\n",
            "step 3600, training loss Total= 0.008805657, training acc total= 99.90532994270325%\n",
            "step 3650, training loss Total= 0.007297773, training acc total= 99.91885423660278%\n",
            "step 3700, training loss Total= 0.01665089, training acc total= 99.91885423660278%\n",
            "step 3750, training loss Total= 0.009706285, training acc total= 99.93237853050232%\n",
            "step 3800, training loss Total= 0.008221633, training acc total= 99.90532994270325%\n",
            "step 3850, training loss Total= 0.0076459693, training acc total= 99.93237853050232%\n",
            "step 3900, training loss Total= 0.0072192927, training acc total= 99.93237853050232%\n",
            "step 3950, training loss Total= 0.0068130437, training acc total= 99.93237853050232%\n",
            "step 4000, training loss Total= 0.0067903562, training acc total= 99.93237853050232%\n",
            "step 4050, training loss Total= 0.0075173397, training acc total= 99.91885423660278%\n",
            "step 4100, training loss Total= 0.0087321, training acc total= 99.93237853050232%\n",
            "step 4150, training loss Total= 0.008740533, training acc total= 99.93237853050232%\n",
            "step 4200, training loss Total= 0.012686094, training acc total= 99.91885423660278%\n",
            "step 4250, training loss Total= 0.00787168, training acc total= 99.93237853050232%\n",
            "step 4300, training loss Total= 0.0062700943, training acc total= 99.94590282440186%\n",
            "step 4350, training loss Total= 0.0068326746, training acc total= 99.91885423660278%\n",
            "step 4400, training loss Total= 0.0065617855, training acc total= 99.93237853050232%\n",
            "step 4450, training loss Total= 0.0048795254, training acc total= 99.95942711830139%\n",
            "step 4500, training loss Total= 0.008576472, training acc total= 99.94590282440186%\n",
            "step 4550, training loss Total= 0.008094987, training acc total= 99.95942711830139%\n",
            "step 4600, training loss Total= 0.006086152, training acc total= 99.94590282440186%\n",
            "step 4650, training loss Total= 0.008165593, training acc total= 99.93237853050232%\n",
            "step 4700, training loss Total= 0.011768263, training acc total= 99.93237853050232%\n",
            "step 4750, training loss Total= 0.01339663, training acc total= 99.90532994270325%\n",
            "step 4800, training loss Total= 0.014255973, training acc total= 99.94590282440186%\n",
            "step 4850, training loss Total= 0.015324441, training acc total= 99.93237853050232%\n",
            "step 4900, training loss Total= 0.006686056, training acc total= 99.93237853050232%\n",
            "step 4950, training loss Total= 0.0062422254, training acc total= 99.93237853050232%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5000, training loss Total= 0.010798812, training acc total= 99.93237853050232%\n",
            "step 5050, training loss Total= 0.008557538, training acc total= 99.91885423660278%\n",
            "step 5100, training loss Total= 0.004709977, training acc total= 99.94590282440186%\n",
            "step 5150, training loss Total= 0.0059341067, training acc total= 99.94590282440186%\n",
            "step 5200, training loss Total= 0.0064507676, training acc total= 99.94590282440186%\n",
            "step 5250, training loss Total= 0.0079489015, training acc total= 99.94590282440186%\n",
            "step 5300, training loss Total= 0.0053567104, training acc total= 99.94590282440186%\n",
            "step 5350, training loss Total= 0.012472905, training acc total= 99.94590282440186%\n",
            "step 5400, training loss Total= 0.0067291465, training acc total= 99.95942711830139%\n",
            "step 5450, training loss Total= 0.0063821133, training acc total= 99.95942711830139%\n",
            "step 5500, training loss Total= 0.004915611, training acc total= 99.94590282440186%\n",
            "step 5550, training loss Total= 0.005586457, training acc total= 99.94590282440186%\n",
            "step 5600, training loss Total= 0.00512522, training acc total= 99.95942711830139%\n",
            "step 5650, training loss Total= 0.0043273913, training acc total= 99.95942711830139%\n",
            "step 5700, training loss Total= 0.004071583, training acc total= 99.97295141220093%\n",
            "step 5750, training loss Total= 0.008275594, training acc total= 99.97295141220093%\n",
            "step 5800, training loss Total= 0.005335127, training acc total= 99.97295141220093%\n",
            "step 5850, training loss Total= 0.004505645, training acc total= 99.97295141220093%\n",
            "step 5900, training loss Total= 0.005649018, training acc total= 99.97295141220093%\n",
            "step 5950, training loss Total= 0.00913956, training acc total= 99.94590282440186%\n",
            "step 6000, training loss Total= 0.009357061, training acc total= 99.94590282440186%\n",
            "step 6050, training loss Total= 0.015813796, training acc total= 99.94590282440186%\n",
            "step 6100, training loss Total= 0.015416482, training acc total= 99.95942711830139%\n",
            "step 6150, training loss Total= 0.06029041, training acc total= 99.94590282440186%\n",
            "step 6200, training loss Total= 0.008255387, training acc total= 99.95942711830139%\n",
            "step 6250, training loss Total= 0.015964273, training acc total= 99.93237853050232%\n",
            "step 6300, training loss Total= 0.010194431, training acc total= 99.93237853050232%\n",
            "step 6350, training loss Total= 0.0052489033, training acc total= 99.95942711830139%\n",
            "step 6400, training loss Total= 0.0046447213, training acc total= 99.97295141220093%\n",
            "step 6450, training loss Total= 0.008459885, training acc total= 99.97295141220093%\n",
            "step 6500, training loss Total= 0.004735218, training acc total= 99.97295141220093%\n",
            "step 6550, training loss Total= 0.0047713886, training acc total= 99.95942711830139%\n",
            "step 6600, training loss Total= 0.0058304816, training acc total= 99.97295141220093%\n",
            "step 6650, training loss Total= 0.037367504, training acc total= 99.94590282440186%\n",
            "step 6700, training loss Total= 0.024843808, training acc total= 99.90532994270325%\n",
            "step 6750, training loss Total= 0.009643259, training acc total= 99.94590282440186%\n",
            "step 6800, training loss Total= 0.009776289, training acc total= 99.95942711830139%\n",
            "step 6850, training loss Total= 0.0050596027, training acc total= 99.97295141220093%\n",
            "step 6900, training loss Total= 0.0045713913, training acc total= 99.95942711830139%\n",
            "step 6950, training loss Total= 0.004754696, training acc total= 99.95942711830139%\n",
            "step 7000, training loss Total= 0.0058337934, training acc total= 99.95942711830139%\n",
            "step 7050, training loss Total= 0.007868564, training acc total= 99.97295141220093%\n",
            "step 7100, training loss Total= 0.005657959, training acc total= 99.95942711830139%\n",
            "step 7150, training loss Total= 0.0086298585, training acc total= 99.95942711830139%\n",
            "step 7200, training loss Total= 0.016431801, training acc total= 99.94590282440186%\n",
            "step 7250, training loss Total= 0.00818594, training acc total= 99.95942711830139%\n",
            "step 7300, training loss Total= 0.0057529397, training acc total= 99.95942711830139%\n",
            "step 7350, training loss Total= 0.025109733, training acc total= 99.94590282440186%\n",
            "step 7400, training loss Total= 0.008038563, training acc total= 99.94590282440186%\n",
            "step 7450, training loss Total= 0.022860557, training acc total= 99.90532994270325%\n",
            "step 7500, training loss Total= 0.008083807, training acc total= 99.93237853050232%\n",
            "step 7550, training loss Total= 0.015994202, training acc total= 99.90532994270325%\n",
            "step 7600, training loss Total= 0.008989809, training acc total= 99.94590282440186%\n",
            "step 7650, training loss Total= 0.022600023, training acc total= 99.93237853050232%\n",
            "step 7700, training loss Total= 0.005565185, training acc total= 99.97295141220093%\n",
            "step 7750, training loss Total= 0.005915841, training acc total= 99.97295141220093%\n",
            "step 7800, training loss Total= 0.005852733, training acc total= 99.95942711830139%\n",
            "step 7850, training loss Total= 0.0042294627, training acc total= 99.97295141220093%\n",
            "step 7900, training loss Total= 0.0050344034, training acc total= 99.97295141220093%\n",
            "step 7950, training loss Total= 0.0043583536, training acc total= 99.97295141220093%\n",
            "step 8000, training loss Total= 0.0044443947, training acc total= 99.97295141220093%\n",
            "step 8050, training loss Total= 0.0032516387, training acc total= 99.97295141220093%\n",
            "step 8100, training loss Total= 0.0039340598, training acc total= 99.97295141220093%\n",
            "step 8150, training loss Total= 0.011071904, training acc total= 99.97295141220093%\n",
            "step 8200, training loss Total= 0.0145737, training acc total= 99.93237853050232%\n",
            "step 8250, training loss Total= 0.0048424914, training acc total= 99.97295141220093%\n",
            "step 8300, training loss Total= 0.0039253016, training acc total= 99.97295141220093%\n",
            "step 8350, training loss Total= 0.0059976103, training acc total= 99.97295141220093%\n",
            "step 8400, training loss Total= 0.004707688, training acc total= 99.97295141220093%\n",
            "step 8450, training loss Total= 0.003017308, training acc total= 99.97295141220093%\n",
            "step 8500, training loss Total= 0.0028671075, training acc total= 99.97295141220093%\n",
            "step 8550, training loss Total= 0.0031461623, training acc total= 99.97295141220093%\n",
            "step 8600, training loss Total= 0.003767312, training acc total= 99.97295141220093%\n",
            "step 8650, training loss Total= 0.0033016896, training acc total= 99.97295141220093%\n",
            "step 8700, training loss Total= 0.0032791367, training acc total= 99.97295141220093%\n",
            "step 8750, training loss Total= 0.0033270419, training acc total= 99.97295141220093%\n",
            "step 8800, training loss Total= 0.0024553079, training acc total= 99.97295141220093%\n",
            "step 8850, training loss Total= 0.0029052137, training acc total= 99.97295141220093%\n",
            "step 8900, training loss Total= 0.003048371, training acc total= 99.97295141220093%\n",
            "step 8950, training loss Total= 0.0066876197, training acc total= 99.97295141220093%\n",
            "step 9000, training loss Total= 0.0047076046, training acc total= 99.97295141220093%\n",
            "step 9050, training loss Total= 0.0066571655, training acc total= 99.95942711830139%\n",
            "step 9100, training loss Total= 0.00515054, training acc total= 99.97295141220093%\n",
            "step 9150, training loss Total= 0.005440488, training acc total= 99.95942711830139%\n",
            "step 9200, training loss Total= 0.0055452585, training acc total= 99.94590282440186%\n",
            "step 9250, training loss Total= 0.0035650772, training acc total= 99.97295141220093%\n",
            "step 9300, training loss Total= 0.002474177, training acc total= 99.97295141220093%\n",
            "step 9350, training loss Total= 0.003155441, training acc total= 99.97295141220093%\n",
            "step 9400, training loss Total= 0.0024126451, training acc total= 99.97295141220093%\n",
            "step 9450, training loss Total= 0.0025137742, training acc total= 99.97295141220093%\n",
            "step 9500, training loss Total= 0.0045941966, training acc total= 99.97295141220093%\n",
            "step 9550, training loss Total= 0.0036597867, training acc total= 99.97295141220093%\n",
            "step 9600, training loss Total= 0.003829165, training acc total= 99.97295141220093%\n",
            "step 9650, training loss Total= 0.0030664871, training acc total= 99.97295141220093%\n",
            "step 9700, training loss Total= 0.004400581, training acc total= 99.98647570610046%\n",
            "step 9750, training loss Total= 0.004222287, training acc total= 99.97295141220093%\n",
            "step 9800, training loss Total= 0.0027580522, training acc total= 99.97295141220093%\n",
            "step 9850, training loss Total= 0.0022702406, training acc total= 99.97295141220093%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9900, training loss Total= 0.002392349, training acc total= 99.97295141220093%\n",
            "step 9950, training loss Total= 0.002543044, training acc total= 99.97295141220093%\n",
            "step 10000, training loss Total= 0.0027021526, training acc total= 99.97295141220093%\n",
            "step 10050, training loss Total= 0.002133778, training acc total= 99.97295141220093%\n",
            "step 10100, training loss Total= 0.004848987, training acc total= 99.93237853050232%\n",
            "step 10150, training loss Total= 0.048872016, training acc total= 99.93237853050232%\n",
            "step 10200, training loss Total= 0.100582495, training acc total= 99.83770847320557%\n",
            "step 10250, training loss Total= 0.018141244, training acc total= 99.91885423660278%\n",
            "step 10300, training loss Total= 0.013173669, training acc total= 99.94590282440186%\n",
            "step 10350, training loss Total= 0.005432794, training acc total= 99.97295141220093%\n",
            "step 10400, training loss Total= 0.0030482505, training acc total= 99.97295141220093%\n",
            "step 10450, training loss Total= 0.0031061373, training acc total= 99.97295141220093%\n",
            "step 10500, training loss Total= 0.0037636561, training acc total= 99.97295141220093%\n",
            "step 10550, training loss Total= 0.003469212, training acc total= 99.97295141220093%\n",
            "step 10600, training loss Total= 0.0033960985, training acc total= 99.97295141220093%\n",
            "step 10650, training loss Total= 0.0028283184, training acc total= 99.97295141220093%\n",
            "step 10700, training loss Total= 0.0024110791, training acc total= 99.97295141220093%\n",
            "step 10750, training loss Total= 0.0024260445, training acc total= 99.97295141220093%\n",
            "step 10800, training loss Total= 0.0022999854, training acc total= 99.97295141220093%\n",
            "step 10850, training loss Total= 0.00243484, training acc total= 99.97295141220093%\n",
            "step 10900, training loss Total= 0.0024524732, training acc total= 99.97295141220093%\n",
            "step 10950, training loss Total= 0.0024853116, training acc total= 99.97295141220093%\n",
            "step 11000, training loss Total= 0.0028962342, training acc total= 99.97295141220093%\n",
            "step 11050, training loss Total= 0.002130936, training acc total= 99.97295141220093%\n",
            "step 11100, training loss Total= 0.0026529636, training acc total= 99.97295141220093%\n",
            "step 11150, training loss Total= 0.0020924367, training acc total= 99.97295141220093%\n",
            "step 11200, training loss Total= 0.002027565, training acc total= 99.97295141220093%\n",
            "step 11250, training loss Total= 0.0020199623, training acc total= 99.97295141220093%\n",
            "step 11300, training loss Total= 0.0022667523, training acc total= 99.97295141220093%\n",
            "step 11350, training loss Total= 0.0019751485, training acc total= 99.97295141220093%\n",
            "step 11400, training loss Total= 0.0019267021, training acc total= 99.97295141220093%\n",
            "step 11450, training loss Total= 0.0024633526, training acc total= 99.97295141220093%\n",
            "step 11500, training loss Total= 0.0029083428, training acc total= 99.97295141220093%\n",
            "step 11550, training loss Total= 0.0020808657, training acc total= 99.97295141220093%\n",
            "step 11600, training loss Total= 0.0023417606, training acc total= 99.97295141220093%\n",
            "step 11650, training loss Total= 0.0025832779, training acc total= 99.97295141220093%\n",
            "step 11700, training loss Total= 0.002029579, training acc total= 99.97295141220093%\n",
            "step 11750, training loss Total= 0.0032403276, training acc total= 99.97295141220093%\n",
            "step 11800, training loss Total= 0.0052599004, training acc total= 99.95942711830139%\n",
            "step 11850, training loss Total= 0.0031312823, training acc total= 99.97295141220093%\n",
            "step 11900, training loss Total= 0.009680023, training acc total= 99.95942711830139%\n",
            "step 11950, training loss Total= 0.031313315, training acc total= 99.87828135490417%\n",
            "step 12000, training loss Total= 0.011087173, training acc total= 99.93237853050232%\n",
            "step 12050, training loss Total= 0.008078265, training acc total= 99.95942711830139%\n",
            "step 12100, training loss Total= 0.0076428847, training acc total= 99.94590282440186%\n",
            "step 12150, training loss Total= 0.0031417718, training acc total= 99.97295141220093%\n",
            "step 12200, training loss Total= 0.0033679442, training acc total= 99.97295141220093%\n",
            "step 12250, training loss Total= 0.005727087, training acc total= 99.97295141220093%\n",
            "step 12300, training loss Total= 0.003308522, training acc total= 99.97295141220093%\n",
            "step 12350, training loss Total= 0.0051476685, training acc total= 99.95942711830139%\n",
            "step 12400, training loss Total= 0.003153443, training acc total= 99.97295141220093%\n",
            "step 12450, training loss Total= 0.003074853, training acc total= 99.97295141220093%\n",
            "step 12500, training loss Total= 0.0021941185, training acc total= 99.97295141220093%\n",
            "step 12550, training loss Total= 0.002160789, training acc total= 99.97295141220093%\n",
            "step 12600, training loss Total= 0.0019361933, training acc total= 99.97295141220093%\n",
            "step 12650, training loss Total= 0.0028906323, training acc total= 99.97295141220093%\n",
            "step 12700, training loss Total= 0.0018883848, training acc total= 99.97295141220093%\n",
            "step 12750, training loss Total= 0.0025067478, training acc total= 99.97295141220093%\n",
            "step 12800, training loss Total= 0.0020606406, training acc total= 99.97295141220093%\n",
            "step 12850, training loss Total= 0.0022797284, training acc total= 99.97295141220093%\n",
            "step 12900, training loss Total= 0.0022933073, training acc total= 99.97295141220093%\n",
            "step 12950, training loss Total= 0.0017535085, training acc total= 99.97295141220093%\n",
            "step 13000, training loss Total= 0.0016707543, training acc total= 99.97295141220093%\n",
            "step 13050, training loss Total= 0.0016395242, training acc total= 99.97295141220093%\n",
            "step 13100, training loss Total= 0.0016122195, training acc total= 99.97295141220093%\n",
            "step 13150, training loss Total= 0.0016039802, training acc total= 99.97295141220093%\n",
            "step 13200, training loss Total= 0.0016064546, training acc total= 99.97295141220093%\n",
            "step 13250, training loss Total= 0.0015696855, training acc total= 99.97295141220093%\n",
            "step 13300, training loss Total= 0.0015597614, training acc total= 99.97295141220093%\n",
            "step 13350, training loss Total= 0.0015545237, training acc total= 99.97295141220093%\n",
            "step 13400, training loss Total= 0.001584172, training acc total= 99.97295141220093%\n",
            "step 13450, training loss Total= 0.0015050911, training acc total= 99.97295141220093%\n",
            "step 13500, training loss Total= 0.0015170596, training acc total= 99.97295141220093%\n",
            "step 13550, training loss Total= 0.0015466568, training acc total= 99.97295141220093%\n",
            "step 13600, training loss Total= 0.0015210911, training acc total= 99.97295141220093%\n",
            "step 13650, training loss Total= 0.0024297936, training acc total= 99.97295141220093%\n",
            "step 13700, training loss Total= 0.0023247213, training acc total= 99.97295141220093%\n",
            "step 13750, training loss Total= 0.0025429805, training acc total= 99.97295141220093%\n",
            "step 13800, training loss Total= 0.0020029163, training acc total= 99.97295141220093%\n",
            "step 13850, training loss Total= 0.0016371367, training acc total= 99.97295141220093%\n",
            "step 13900, training loss Total= 0.0020207188, training acc total= 99.97295141220093%\n",
            "step 13950, training loss Total= 0.0015951168, training acc total= 99.97295141220093%\n",
            "step 14000, training loss Total= 0.0024820957, training acc total= 99.97295141220093%\n",
            "step 14050, training loss Total= 0.0020436586, training acc total= 99.97295141220093%\n",
            "step 14100, training loss Total= 0.0027436058, training acc total= 99.98647570610046%\n",
            "step 14150, training loss Total= 0.008981025, training acc total= 99.95942711830139%\n",
            "step 14200, training loss Total= 0.004724836, training acc total= 99.97295141220093%\n",
            "step 14250, training loss Total= 0.027153779, training acc total= 99.93237853050232%\n",
            "step 14300, training loss Total= 0.038176972, training acc total= 99.91885423660278%\n",
            "step 14350, training loss Total= 0.01689094, training acc total= 99.93237853050232%\n",
            "step 14400, training loss Total= 0.006467728, training acc total= 99.94590282440186%\n",
            "step 14450, training loss Total= 0.0124242585, training acc total= 99.90532994270325%\n",
            "step 14500, training loss Total= 0.010223226, training acc total= 99.94590282440186%\n",
            "step 14550, training loss Total= 0.0030968322, training acc total= 99.97295141220093%\n",
            "step 14600, training loss Total= 0.0024709452, training acc total= 99.97295141220093%\n",
            "step 14650, training loss Total= 0.0030906743, training acc total= 99.97295141220093%\n",
            "step 14700, training loss Total= 0.0025917795, training acc total= 99.97295141220093%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14750, training loss Total= 0.0023606194, training acc total= 99.97295141220093%\n",
            "step 14800, training loss Total= 0.0018212249, training acc total= 99.97295141220093%\n",
            "step 14850, training loss Total= 0.0017793439, training acc total= 99.97295141220093%\n",
            "step 14900, training loss Total= 0.0017433839, training acc total= 99.97295141220093%\n",
            "step 14950, training loss Total= 0.0017050392, training acc total= 99.97295141220093%\n",
            "step 15000, training loss Total= 0.0016876386, training acc total= 99.97295141220093%\n",
            "step 15050, training loss Total= 0.0017083763, training acc total= 99.97295141220093%\n",
            "step 15100, training loss Total= 0.0016536368, training acc total= 99.97295141220093%\n",
            "step 15150, training loss Total= 0.0016647776, training acc total= 99.97295141220093%\n",
            "step 15200, training loss Total= 0.0017717731, training acc total= 99.97295141220093%\n",
            "step 15250, training loss Total= 0.0021667883, training acc total= 99.97295141220093%\n",
            "step 15300, training loss Total= 0.0018837727, training acc total= 99.97295141220093%\n",
            "step 15350, training loss Total= 0.0020867276, training acc total= 99.97295141220093%\n",
            "step 15400, training loss Total= 0.0019606594, training acc total= 99.97295141220093%\n",
            "step 15450, training loss Total= 0.0017371626, training acc total= 99.97295141220093%\n",
            "step 15500, training loss Total= 0.0014273789, training acc total= 99.97295141220093%\n",
            "step 15550, training loss Total= 0.0020627854, training acc total= 99.97295141220093%\n",
            "step 15600, training loss Total= 0.0015535813, training acc total= 99.97295141220093%\n",
            "step 15650, training loss Total= 0.002004499, training acc total= 99.97295141220093%\n",
            "step 15700, training loss Total= 0.001628481, training acc total= 99.97295141220093%\n",
            "step 15750, training loss Total= 0.0014441975, training acc total= 99.97295141220093%\n",
            "step 15800, training loss Total= 0.0017901451, training acc total= 99.97295141220093%\n",
            "step 15850, training loss Total= 0.001446844, training acc total= 99.97295141220093%\n",
            "step 15900, training loss Total= 0.00141589, training acc total= 99.97295141220093%\n",
            "step 15950, training loss Total= 0.0013555754, training acc total= 99.97295141220093%\n",
            "step 16000, training loss Total= 0.0020415215, training acc total= 99.97295141220093%\n",
            "step 16050, training loss Total= 0.0014608643, training acc total= 99.97295141220093%\n",
            "step 16100, training loss Total= 0.0044555147, training acc total= 99.95942711830139%\n",
            "step 16150, training loss Total= 0.0043982007, training acc total= 99.94590282440186%\n",
            "step 16200, training loss Total= 0.001427398, training acc total= 99.97295141220093%\n",
            "step 16250, training loss Total= 0.0015835937, training acc total= 99.97295141220093%\n",
            "step 16300, training loss Total= 0.0013239854, training acc total= 99.97295141220093%\n",
            "step 16350, training loss Total= 0.0012365079, training acc total= 99.97295141220093%\n",
            "step 16400, training loss Total= 0.0013070283, training acc total= 99.97295141220093%\n",
            "step 16450, training loss Total= 0.0012638585, training acc total= 99.97295141220093%\n",
            "step 16500, training loss Total= 0.00117444, training acc total= 99.97295141220093%\n",
            "step 16550, training loss Total= 0.0011537593, training acc total= 99.97295141220093%\n",
            "step 16600, training loss Total= 0.0011441078, training acc total= 99.97295141220093%\n",
            "step 16650, training loss Total= 0.0025352207, training acc total= 99.97295141220093%\n",
            "step 16700, training loss Total= 0.0013405473, training acc total= 99.97295141220093%\n",
            "step 16750, training loss Total= 0.0020073783, training acc total= 99.97295141220093%\n",
            "step 16800, training loss Total= 0.0017650912, training acc total= 99.97295141220093%\n",
            "step 16850, training loss Total= 0.0025898376, training acc total= 99.97295141220093%\n",
            "step 16900, training loss Total= 0.015626814, training acc total= 99.95942711830139%\n",
            "step 16950, training loss Total= 0.0108557055, training acc total= 99.93237853050232%\n",
            "step 17000, training loss Total= 0.008382184, training acc total= 99.95942711830139%\n",
            "step 17050, training loss Total= 0.007269322, training acc total= 99.94590282440186%\n",
            "step 17100, training loss Total= 0.0048007155, training acc total= 99.94590282440186%\n",
            "step 17150, training loss Total= 0.0061872075, training acc total= 99.98647570610046%\n",
            "step 17200, training loss Total= 0.0038324357, training acc total= 99.97295141220093%\n",
            "step 17250, training loss Total= 0.0020700668, training acc total= 99.97295141220093%\n",
            "step 17300, training loss Total= 0.001788574, training acc total= 99.97295141220093%\n",
            "step 17350, training loss Total= 0.0015978711, training acc total= 99.97295141220093%\n",
            "step 17400, training loss Total= 0.0015144808, training acc total= 99.97295141220093%\n",
            "step 17450, training loss Total= 0.0018000964, training acc total= 99.97295141220093%\n",
            "step 17500, training loss Total= 0.0014345932, training acc total= 99.97295141220093%\n",
            "step 17550, training loss Total= 0.0014786589, training acc total= 99.97295141220093%\n",
            "step 17600, training loss Total= 0.0014358496, training acc total= 99.97295141220093%\n",
            "step 17650, training loss Total= 0.0013399598, training acc total= 99.97295141220093%\n",
            "step 17700, training loss Total= 0.0015486527, training acc total= 99.98647570610046%\n",
            "step 17750, training loss Total= 0.0013293325, training acc total= 99.97295141220093%\n",
            "step 17800, training loss Total= 0.0013005588, training acc total= 99.97295141220093%\n",
            "step 17850, training loss Total= 0.0012412451, training acc total= 99.97295141220093%\n",
            "step 17900, training loss Total= 0.0012976556, training acc total= 99.97295141220093%\n",
            "step 17950, training loss Total= 0.001265664, training acc total= 99.97295141220093%\n",
            "step 18000, training loss Total= 0.0012840258, training acc total= 99.97295141220093%\n",
            "step 18050, training loss Total= 0.0012353724, training acc total= 99.97295141220093%\n",
            "step 18100, training loss Total= 0.0012806086, training acc total= 99.97295141220093%\n",
            "step 18150, training loss Total= 0.0011221553, training acc total= 99.97295141220093%\n",
            "step 18200, training loss Total= 0.0011331703, training acc total= 99.97295141220093%\n",
            "step 18250, training loss Total= 0.001133389, training acc total= 99.97295141220093%\n",
            "step 18300, training loss Total= 0.0010693122, training acc total= 99.97295141220093%\n",
            "step 18350, training loss Total= 0.001043298, training acc total= 99.97295141220093%\n",
            "step 18400, training loss Total= 0.0010545297, training acc total= 99.97295141220093%\n",
            "step 18450, training loss Total= 0.0010362839, training acc total= 99.97295141220093%\n",
            "step 18500, training loss Total= 0.0010795422, training acc total= 99.97295141220093%\n",
            "step 18550, training loss Total= 0.0023387605, training acc total= 99.97295141220093%\n",
            "step 18600, training loss Total= 0.0012151068, training acc total= 99.97295141220093%\n",
            "step 18650, training loss Total= 0.0010745031, training acc total= 99.97295141220093%\n",
            "step 18700, training loss Total= 0.0020199863, training acc total= 99.97295141220093%\n",
            "step 18750, training loss Total= 0.0019219981, training acc total= 99.97295141220093%\n",
            "step 18800, training loss Total= 0.001160691, training acc total= 99.97295141220093%\n",
            "step 18850, training loss Total= 0.0015372555, training acc total= 99.98647570610046%\n",
            "step 18900, training loss Total= 0.0019089194, training acc total= 99.98647570610046%\n",
            "step 18950, training loss Total= 0.0010469618, training acc total= 99.97295141220093%\n",
            "step 19000, training loss Total= 0.0011545685, training acc total= 99.97295141220093%\n",
            "step 19050, training loss Total= 0.0013725506, training acc total= 99.98647570610046%\n",
            "step 19100, training loss Total= 0.0020855945, training acc total= 99.97295141220093%\n",
            "step 19150, training loss Total= 0.0019853094, training acc total= 99.97295141220093%\n",
            "step 19200, training loss Total= 0.019449897, training acc total= 99.79713559150696%\n",
            "step 19250, training loss Total= 0.009593347, training acc total= 99.97295141220093%\n",
            "step 19300, training loss Total= 0.0041220267, training acc total= 99.94590282440186%\n",
            "step 19350, training loss Total= 0.0024190492, training acc total= 99.97295141220093%\n",
            "step 19400, training loss Total= 0.0016229367, training acc total= 99.97295141220093%\n",
            "step 19450, training loss Total= 0.0014391755, training acc total= 99.97295141220093%\n",
            "step 19500, training loss Total= 0.0013676839, training acc total= 99.97295141220093%\n",
            "step 19550, training loss Total= 0.0013120673, training acc total= 99.97295141220093%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19600, training loss Total= 0.0013714691, training acc total= 99.97295141220093%\n",
            "step 19650, training loss Total= 0.0012160199, training acc total= 99.97295141220093%\n",
            "step 19700, training loss Total= 0.0011839863, training acc total= 99.97295141220093%\n",
            "step 19750, training loss Total= 0.0011023436, training acc total= 99.97295141220093%\n",
            "step 19800, training loss Total= 0.0013094689, training acc total= 99.97295141220093%\n",
            "step 19850, training loss Total= 0.0012858493, training acc total= 99.98647570610046%\n",
            "step 19900, training loss Total= 0.0010423483, training acc total= 99.97295141220093%\n",
            "step 19950, training loss Total= 0.0010284631, training acc total= 99.97295141220093%\n",
            "step 20000, training loss Total= 0.0009940936, training acc total= 99.97295141220093%\n",
            "step 20050, training loss Total= 0.0009777604, training acc total= 99.97295141220093%\n",
            "step 20100, training loss Total= 0.0010370784, training acc total= 99.98647570610046%\n",
            "step 20150, training loss Total= 0.0009623518, training acc total= 99.97295141220093%\n",
            "step 20200, training loss Total= 0.0010861031, training acc total= 99.97295141220093%\n",
            "step 20250, training loss Total= 0.0010088733, training acc total= 99.97295141220093%\n",
            "step 20300, training loss Total= 0.0012656577, training acc total= 99.97295141220093%\n",
            "step 20350, training loss Total= 0.0013357257, training acc total= 99.97295141220093%\n",
            "step 20400, training loss Total= 0.0013944667, training acc total= 99.97295141220093%\n",
            "step 20450, training loss Total= 0.0010560632, training acc total= 99.98647570610046%\n",
            "step 20500, training loss Total= 0.0012877935, training acc total= 99.97295141220093%\n",
            "step 20550, training loss Total= 0.0012659836, training acc total= 99.98647570610046%\n",
            "step 20600, training loss Total= 0.0012783085, training acc total= 99.97295141220093%\n",
            "step 20650, training loss Total= 0.0008621167, training acc total= 99.97295141220093%\n",
            "step 20700, training loss Total= 0.0012202275, training acc total= 99.97295141220093%\n",
            "step 20750, training loss Total= 0.0063121347, training acc total= 99.94590282440186%\n",
            "step 20800, training loss Total= 0.0023135173, training acc total= 99.95942711830139%\n",
            "step 20850, training loss Total= 0.04553076, training acc total= 99.90532994270325%\n",
            "step 20900, training loss Total= 0.02012777, training acc total= 99.97295141220093%\n",
            "step 20950, training loss Total= 0.008102541, training acc total= 99.94590282440186%\n",
            "step 21000, training loss Total= 0.0019440718, training acc total= 99.97295141220093%\n",
            "step 21050, training loss Total= 0.001687519, training acc total= 99.97295141220093%\n",
            "step 21100, training loss Total= 0.0016815274, training acc total= 99.97295141220093%\n",
            "step 21150, training loss Total= 0.0011742122, training acc total= 99.97295141220093%\n",
            "step 21200, training loss Total= 0.0010788579, training acc total= 99.97295141220093%\n",
            "step 21250, training loss Total= 0.0010213684, training acc total= 99.97295141220093%\n",
            "step 21300, training loss Total= 0.0010309807, training acc total= 99.98647570610046%\n",
            "step 21350, training loss Total= 0.00095724344, training acc total= 99.97295141220093%\n",
            "step 21400, training loss Total= 0.0009302021, training acc total= 99.97295141220093%\n",
            "step 21450, training loss Total= 0.00094856985, training acc total= 99.97295141220093%\n",
            "step 21500, training loss Total= 0.0010647791, training acc total= 99.98647570610046%\n",
            "step 21550, training loss Total= 0.0016552052, training acc total= 99.98647570610046%\n",
            "step 21600, training loss Total= 0.0009279885, training acc total= 99.97295141220093%\n",
            "step 21650, training loss Total= 0.00087953167, training acc total= 99.97295141220093%\n",
            "step 21700, training loss Total= 0.00088097824, training acc total= 99.98647570610046%\n",
            "step 21750, training loss Total= 0.00080750685, training acc total= 99.98647570610046%\n",
            "step 21800, training loss Total= 0.00081657554, training acc total= 99.98647570610046%\n",
            "step 21850, training loss Total= 0.00077335694, training acc total= 99.97295141220093%\n",
            "step 21900, training loss Total= 0.0008389362, training acc total= 99.98647570610046%\n",
            "step 21950, training loss Total= 0.00077500584, training acc total= 99.97295141220093%\n",
            "step 22000, training loss Total= 0.0007719368, training acc total= 99.97295141220093%\n",
            "step 22050, training loss Total= 0.00076041045, training acc total= 99.98647570610046%\n",
            "step 22100, training loss Total= 0.00074412976, training acc total= 99.98647570610046%\n",
            "step 22150, training loss Total= 0.000708179, training acc total= 99.97295141220093%\n",
            "step 22200, training loss Total= 0.000736182, training acc total= 99.98647570610046%\n",
            "step 22250, training loss Total= 0.00080572255, training acc total= 99.98647570610046%\n",
            "step 22300, training loss Total= 0.0007372175, training acc total= 99.97295141220093%\n",
            "step 22350, training loss Total= 0.00075064134, training acc total= 99.98647570610046%\n",
            "step 22400, training loss Total= 0.0006885846, training acc total= 99.98647570610046%\n",
            "step 22450, training loss Total= 0.00082814397, training acc total= 100.0%\n",
            "step 22500, training loss Total= 0.00067745976, training acc total= 99.98647570610046%\n",
            "step 22550, training loss Total= 0.0007106074, training acc total= 99.98647570610046%\n",
            "step 22600, training loss Total= 0.00063535187, training acc total= 99.97295141220093%\n",
            "step 22650, training loss Total= 0.0007153774, training acc total= 100.0%\n",
            "step 22700, training loss Total= 0.000616586, training acc total= 99.97295141220093%\n",
            "step 22750, training loss Total= 0.0006386252, training acc total= 100.0%\n",
            "step 22800, training loss Total= 0.0006471975, training acc total= 100.0%\n",
            "step 22850, training loss Total= 0.0006140083, training acc total= 99.98647570610046%\n",
            "step 22900, training loss Total= 0.0006203867, training acc total= 99.98647570610046%\n",
            "step 22950, training loss Total= 0.00061460474, training acc total= 100.0%\n",
            "step 23000, training loss Total= 0.00063850684, training acc total= 99.98647570610046%\n",
            "step 23050, training loss Total= 0.0007969944, training acc total= 100.0%\n",
            "step 23100, training loss Total= 0.00066727406, training acc total= 99.98647570610046%\n",
            "step 23150, training loss Total= 0.0017963278, training acc total= 99.97295141220093%\n",
            "step 23200, training loss Total= 0.0521498, training acc total= 99.79713559150696%\n",
            "step 23250, training loss Total= 0.016510528, training acc total= 99.94590282440186%\n",
            "step 23300, training loss Total= 0.01054833, training acc total= 99.91885423660278%\n",
            "step 23350, training loss Total= 0.032627787, training acc total= 99.91885423660278%\n",
            "step 23400, training loss Total= 0.0135689005, training acc total= 99.91885423660278%\n",
            "step 23450, training loss Total= 0.0021567987, training acc total= 99.95942711830139%\n",
            "step 23500, training loss Total= 0.0018001508, training acc total= 99.97295141220093%\n",
            "step 23550, training loss Total= 0.0012713624, training acc total= 99.98647570610046%\n",
            "step 23600, training loss Total= 0.0015282045, training acc total= 99.97295141220093%\n",
            "step 23650, training loss Total= 0.0015460628, training acc total= 99.97295141220093%\n",
            "step 23700, training loss Total= 0.0015861741, training acc total= 99.97295141220093%\n",
            "step 23750, training loss Total= 0.0011694243, training acc total= 99.98647570610046%\n",
            "step 23800, training loss Total= 0.0011471469, training acc total= 99.98647570610046%\n",
            "step 23850, training loss Total= 0.00091547373, training acc total= 99.98647570610046%\n",
            "step 23900, training loss Total= 0.0009278786, training acc total= 99.97295141220093%\n",
            "step 23950, training loss Total= 0.0010144059, training acc total= 99.98647570610046%\n",
            "step 24000, training loss Total= 0.0010387623, training acc total= 99.98647570610046%\n",
            "step 24050, training loss Total= 0.00082825747, training acc total= 99.98647570610046%\n",
            "step 24100, training loss Total= 0.0007869562, training acc total= 100.0%\n",
            "step 24150, training loss Total= 0.0007581249, training acc total= 99.97295141220093%\n",
            "step 24200, training loss Total= 0.00079275714, training acc total= 99.98647570610046%\n",
            "step 24250, training loss Total= 0.0007769924, training acc total= 99.98647570610046%\n",
            "step 24300, training loss Total= 0.0008014181, training acc total= 99.97295141220093%\n",
            "step 24350, training loss Total= 0.0007232981, training acc total= 99.97295141220093%\n",
            "step 24400, training loss Total= 0.0007027841, training acc total= 99.98647570610046%\n",
            "step 24450, training loss Total= 0.0007451812, training acc total= 99.98647570610046%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24500, training loss Total= 0.0006851655, training acc total= 100.0%\n",
            "step 24550, training loss Total= 0.00068303995, training acc total= 100.0%\n",
            "step 24600, training loss Total= 0.00072441663, training acc total= 99.98647570610046%\n",
            "step 24650, training loss Total= 0.00076361286, training acc total= 99.97295141220093%\n",
            "step 24700, training loss Total= 0.0009521131, training acc total= 100.0%\n",
            "step 24750, training loss Total= 0.0006948452, training acc total= 99.98647570610046%\n",
            "step 24800, training loss Total= 0.0006406626, training acc total= 99.98647570610046%\n",
            "step 24850, training loss Total= 0.00072394183, training acc total= 99.98647570610046%\n",
            "step 24900, training loss Total= 0.0006800402, training acc total= 99.98647570610046%\n",
            "step 24950, training loss Total= 0.00076173124, training acc total= 99.98647570610046%\n",
            "step 25000, training loss Total= 0.00084828644, training acc total= 99.98647570610046%\n",
            "step 25050, training loss Total= 0.0013045105, training acc total= 99.98647570610046%\n",
            "step 25100, training loss Total= 0.0021039208, training acc total= 99.98647570610046%\n",
            "step 25150, training loss Total= 0.0009508979, training acc total= 99.98647570610046%\n",
            "step 25200, training loss Total= 0.00066390936, training acc total= 100.0%\n",
            "step 25250, training loss Total= 0.0006144508, training acc total= 99.98647570610046%\n",
            "step 25300, training loss Total= 0.0006291059, training acc total= 99.98647570610046%\n",
            "step 25350, training loss Total= 0.0005791475, training acc total= 99.97295141220093%\n",
            "step 25400, training loss Total= 0.0005948323, training acc total= 99.98647570610046%\n",
            "step 25450, training loss Total= 0.00058795686, training acc total= 99.98647570610046%\n",
            "step 25500, training loss Total= 0.0006226122, training acc total= 99.97295141220093%\n",
            "step 25550, training loss Total= 0.0006348436, training acc total= 100.0%\n",
            "step 25600, training loss Total= 0.0011391082, training acc total= 99.98647570610046%\n",
            "step 25650, training loss Total= 0.0010859473, training acc total= 99.98647570610046%\n",
            "step 25700, training loss Total= 0.00078888197, training acc total= 99.97295141220093%\n",
            "step 25750, training loss Total= 0.00077270216, training acc total= 99.98647570610046%\n",
            "step 25800, training loss Total= 0.00058588496, training acc total= 100.0%\n",
            "step 25850, training loss Total= 0.00078699214, training acc total= 99.97295141220093%\n",
            "step 25900, training loss Total= 0.0005940624, training acc total= 99.97295141220093%\n",
            "step 25950, training loss Total= 0.00075794547, training acc total= 99.98647570610046%\n",
            "step 26000, training loss Total= 0.0007570894, training acc total= 99.98647570610046%\n",
            "step 26050, training loss Total= 0.0007867062, training acc total= 100.0%\n",
            "step 26100, training loss Total= 0.0009603481, training acc total= 99.98647570610046%\n",
            "step 26150, training loss Total= 0.018710516, training acc total= 99.93237853050232%\n",
            "step 26200, training loss Total= 0.004412002, training acc total= 99.95942711830139%\n",
            "step 26250, training loss Total= 0.011003824, training acc total= 99.97295141220093%\n",
            "step 26300, training loss Total= 0.0031751422, training acc total= 99.97295141220093%\n",
            "step 26350, training loss Total= 0.0014533121, training acc total= 99.95942711830139%\n",
            "step 26400, training loss Total= 0.0009878372, training acc total= 99.98647570610046%\n",
            "step 26450, training loss Total= 0.0010881952, training acc total= 100.0%\n",
            "step 26500, training loss Total= 0.0008747601, training acc total= 99.98647570610046%\n",
            "step 26550, training loss Total= 0.0008853047, training acc total= 99.98647570610046%\n",
            "step 26600, training loss Total= 0.0006989678, training acc total= 99.98647570610046%\n",
            "step 26650, training loss Total= 0.0006979544, training acc total= 99.98647570610046%\n",
            "step 26700, training loss Total= 0.00071993115, training acc total= 99.98647570610046%\n",
            "step 26750, training loss Total= 0.0006387695, training acc total= 99.98647570610046%\n",
            "step 26800, training loss Total= 0.00074282155, training acc total= 99.97295141220093%\n",
            "step 26850, training loss Total= 0.00076507207, training acc total= 100.0%\n",
            "step 26900, training loss Total= 0.0013364044, training acc total= 99.98647570610046%\n",
            "step 26950, training loss Total= 0.00065610267, training acc total= 99.97295141220093%\n",
            "step 27000, training loss Total= 0.00061894197, training acc total= 99.98647570610046%\n",
            "step 27050, training loss Total= 0.00071085244, training acc total= 100.0%\n",
            "step 27100, training loss Total= 0.0006733143, training acc total= 99.98647570610046%\n",
            "step 27150, training loss Total= 0.00053869764, training acc total= 99.98647570610046%\n",
            "step 27200, training loss Total= 0.0005438372, training acc total= 99.97295141220093%\n",
            "step 27250, training loss Total= 0.0005668364, training acc total= 100.0%\n",
            "step 27300, training loss Total= 0.00054405414, training acc total= 99.97295141220093%\n",
            "step 27350, training loss Total= 0.0005470287, training acc total= 100.0%\n",
            "step 27400, training loss Total= 0.00049605785, training acc total= 100.0%\n",
            "step 27450, training loss Total= 0.00051945477, training acc total= 99.98647570610046%\n",
            "step 27500, training loss Total= 0.00048087724, training acc total= 99.98647570610046%\n",
            "step 27550, training loss Total= 0.0004825602, training acc total= 100.0%\n",
            "step 27600, training loss Total= 0.0004934152, training acc total= 99.97295141220093%\n",
            "step 27650, training loss Total= 0.00052520836, training acc total= 99.98647570610046%\n",
            "step 27700, training loss Total= 0.0014090649, training acc total= 100.0%\n",
            "step 27750, training loss Total= 0.0006915987, training acc total= 99.98647570610046%\n",
            "step 27800, training loss Total= 0.00046367024, training acc total= 100.0%\n",
            "step 27850, training loss Total= 0.0004975821, training acc total= 99.98647570610046%\n",
            "step 27900, training loss Total= 0.0005042598, training acc total= 99.98647570610046%\n",
            "step 27950, training loss Total= 0.00082881516, training acc total= 100.0%\n",
            "step 28000, training loss Total= 0.00063022174, training acc total= 99.97295141220093%\n",
            "step 28050, training loss Total= 0.0007087582, training acc total= 99.98647570610046%\n",
            "step 28100, training loss Total= 0.0016319766, training acc total= 99.98647570610046%\n",
            "step 28150, training loss Total= 0.0009579187, training acc total= 99.98647570610046%\n",
            "step 28200, training loss Total= 0.0005080842, training acc total= 99.97295141220093%\n",
            "step 28250, training loss Total= 0.0004372335, training acc total= 100.0%\n",
            "step 28300, training loss Total= 0.0019840251, training acc total= 99.98647570610046%\n",
            "step 28350, training loss Total= 0.0017486499, training acc total= 99.98647570610046%\n",
            "step 28400, training loss Total= 0.0007642848, training acc total= 99.98647570610046%\n",
            "step 28450, training loss Total= 0.0009034114, training acc total= 99.97295141220093%\n",
            "step 28500, training loss Total= 0.00052423315, training acc total= 99.98647570610046%\n",
            "step 28550, training loss Total= 0.00097502244, training acc total= 99.97295141220093%\n",
            "step 28600, training loss Total= 0.0014572748, training acc total= 99.97295141220093%\n",
            "step 28650, training loss Total= 0.0007458252, training acc total= 99.98647570610046%\n",
            "step 28700, training loss Total= 0.0010983816, training acc total= 99.98647570610046%\n",
            "step 28750, training loss Total= 0.0014965786, training acc total= 99.97295141220093%\n",
            "step 28800, training loss Total= 0.00067481835, training acc total= 100.0%\n",
            "step 28850, training loss Total= 0.0005081175, training acc total= 99.98647570610046%\n",
            "step 28900, training loss Total= 0.0015804102, training acc total= 99.98647570610046%\n",
            "step 28950, training loss Total= 0.0007661615, training acc total= 100.0%\n",
            "step 29000, training loss Total= 0.00065621495, training acc total= 99.98647570610046%\n",
            "step 29050, training loss Total= 0.0038232172, training acc total= 99.97295141220093%\n",
            "step 29100, training loss Total= 0.018202724, training acc total= 99.90532994270325%\n",
            "step 29150, training loss Total= 0.02384271, training acc total= 99.94590282440186%\n",
            "step 29200, training loss Total= 0.007293147, training acc total= 99.95942711830139%\n",
            "step 29250, training loss Total= 0.0020294036, training acc total= 100.0%\n",
            "step 29300, training loss Total= 0.0032706198, training acc total= 99.97295141220093%\n",
            "step 29350, training loss Total= 0.001448216, training acc total= 99.95942711830139%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29400, training loss Total= 0.001038721, training acc total= 99.97295141220093%\n",
            "step 29450, training loss Total= 0.0008732299, training acc total= 100.0%\n",
            "step 29500, training loss Total= 0.0007785296, training acc total= 100.0%\n",
            "step 29550, training loss Total= 0.0007388578, training acc total= 99.98647570610046%\n",
            "step 29600, training loss Total= 0.00071566773, training acc total= 99.98647570610046%\n",
            "step 29650, training loss Total= 0.0006710088, training acc total= 99.98647570610046%\n",
            "step 29700, training loss Total= 0.00065883616, training acc total= 100.0%\n",
            "step 29750, training loss Total= 0.0006667866, training acc total= 100.0%\n",
            "step 29800, training loss Total= 0.0006712597, training acc total= 100.0%\n",
            "step 29850, training loss Total= 0.00059048715, training acc total= 100.0%\n",
            "step 29900, training loss Total= 0.00060406345, training acc total= 100.0%\n",
            "step 29950, training loss Total= 0.00055701175, training acc total= 100.0%\n",
            "step 30000, training loss Total= 0.00054156623, training acc total= 100.0%\n",
            "step 30050, training loss Total= 0.00053122395, training acc total= 100.0%\n",
            "step 30100, training loss Total= 0.0005219534, training acc total= 100.0%\n",
            "step 30150, training loss Total= 0.00050205784, training acc total= 100.0%\n",
            "step 30200, training loss Total= 0.0004935569, training acc total= 100.0%\n",
            "step 30250, training loss Total= 0.0005796998, training acc total= 100.0%\n",
            "step 30300, training loss Total= 0.0004735373, training acc total= 100.0%\n",
            "step 30350, training loss Total= 0.00047748175, training acc total= 99.98647570610046%\n",
            "step 30400, training loss Total= 0.0004995238, training acc total= 99.98647570610046%\n",
            "step 30450, training loss Total= 0.00044450167, training acc total= 100.0%\n",
            "step 30500, training loss Total= 0.00043395377, training acc total= 100.0%\n",
            "step 30550, training loss Total= 0.0004990336, training acc total= 99.98647570610046%\n",
            "step 30600, training loss Total= 0.00042571462, training acc total= 100.0%\n",
            "step 30650, training loss Total= 0.00041408688, training acc total= 100.0%\n",
            "step 30700, training loss Total= 0.00041503, training acc total= 100.0%\n",
            "step 30750, training loss Total= 0.00041860636, training acc total= 100.0%\n",
            "step 30800, training loss Total= 0.0004396979, training acc total= 99.98647570610046%\n",
            "step 30850, training loss Total= 0.0004767031, training acc total= 99.98647570610046%\n",
            "step 30900, training loss Total= 0.0005040065, training acc total= 99.98647570610046%\n",
            "step 30950, training loss Total= 0.00038614383, training acc total= 100.0%\n",
            "step 31000, training loss Total= 0.00037918892, training acc total= 100.0%\n",
            "step 31050, training loss Total= 0.00038463587, training acc total= 100.0%\n",
            "step 31100, training loss Total= 0.00036319823, training acc total= 100.0%\n",
            "step 31150, training loss Total= 0.00043982075, training acc total= 99.97295141220093%\n",
            "step 31200, training loss Total= 0.00039832448, training acc total= 100.0%\n",
            "step 31250, training loss Total= 0.0004075714, training acc total= 100.0%\n",
            "step 31300, training loss Total= 0.00035959954, training acc total= 99.98647570610046%\n",
            "step 31350, training loss Total= 0.0004663103, training acc total= 100.0%\n",
            "step 31400, training loss Total= 0.0004056397, training acc total= 100.0%\n",
            "step 31450, training loss Total= 0.0003399322, training acc total= 100.0%\n",
            "step 31500, training loss Total= 0.00034448432, training acc total= 100.0%\n",
            "step 31550, training loss Total= 0.00031790847, training acc total= 100.0%\n",
            "step 31600, training loss Total= 0.00033559828, training acc total= 99.98647570610046%\n",
            "step 31650, training loss Total= 0.0003293548, training acc total= 100.0%\n",
            "step 31700, training loss Total= 0.00034707907, training acc total= 100.0%\n",
            "step 31750, training loss Total= 0.00047364453, training acc total= 99.98647570610046%\n",
            "step 31800, training loss Total= 0.0018964238, training acc total= 99.97295141220093%\n",
            "step 31850, training loss Total= 0.02181801, training acc total= 99.86475706100464%\n",
            "step 31900, training loss Total= 0.027499795, training acc total= 99.90532994270325%\n",
            "step 31950, training loss Total= 0.0048623197, training acc total= 99.97295141220093%\n",
            "step 32000, training loss Total= 0.0019365372, training acc total= 99.98647570610046%\n",
            "step 32050, training loss Total= 0.0014318984, training acc total= 100.0%\n",
            "step 32100, training loss Total= 0.00092893926, training acc total= 100.0%\n",
            "step 32150, training loss Total= 0.0006681722, training acc total= 100.0%\n",
            "step 32200, training loss Total= 0.00056759454, training acc total= 100.0%\n",
            "step 32250, training loss Total= 0.00054803176, training acc total= 100.0%\n",
            "step 32300, training loss Total= 0.00054636656, training acc total= 100.0%\n",
            "step 32350, training loss Total= 0.0005023477, training acc total= 100.0%\n",
            "step 32400, training loss Total= 0.0005073089, training acc total= 99.98647570610046%\n",
            "step 32450, training loss Total= 0.0005573907, training acc total= 99.98647570610046%\n",
            "step 32500, training loss Total= 0.00055650115, training acc total= 99.98647570610046%\n",
            "step 32550, training loss Total= 0.00055992574, training acc total= 99.98647570610046%\n",
            "step 32600, training loss Total= 0.0005201988, training acc total= 100.0%\n",
            "step 32650, training loss Total= 0.00044394645, training acc total= 99.98647570610046%\n",
            "step 32700, training loss Total= 0.00045614954, training acc total= 100.0%\n",
            "step 32750, training loss Total= 0.00045773902, training acc total= 99.98647570610046%\n",
            "step 32800, training loss Total= 0.00046314503, training acc total= 100.0%\n",
            "step 32850, training loss Total= 0.00055706897, training acc total= 99.98647570610046%\n",
            "step 32900, training loss Total= 0.0005191029, training acc total= 100.0%\n",
            "step 32950, training loss Total= 0.0006030159, training acc total= 99.98647570610046%\n",
            "step 33000, training loss Total= 0.00059042184, training acc total= 99.98647570610046%\n",
            "step 33050, training loss Total= 0.0005672925, training acc total= 99.98647570610046%\n",
            "step 33100, training loss Total= 0.0005709742, training acc total= 99.98647570610046%\n",
            "step 33150, training loss Total= 0.0005828962, training acc total= 99.98647570610046%\n",
            "step 33200, training loss Total= 0.00062063156, training acc total= 99.98647570610046%\n",
            "step 33250, training loss Total= 0.0003411327, training acc total= 100.0%\n",
            "step 33300, training loss Total= 0.0003289788, training acc total= 100.0%\n",
            "step 33350, training loss Total= 0.00034532513, training acc total= 99.98647570610046%\n",
            "step 33400, training loss Total= 0.00037111694, training acc total= 99.98647570610046%\n",
            "step 33450, training loss Total= 0.00037685997, training acc total= 99.98647570610046%\n",
            "step 33500, training loss Total= 0.00031588174, training acc total= 100.0%\n",
            "step 33550, training loss Total= 0.0007163868, training acc total= 99.98647570610046%\n",
            "step 33600, training loss Total= 0.00056735973, training acc total= 99.97295141220093%\n",
            "step 33650, training loss Total= 0.0013232661, training acc total= 99.98647570610046%\n",
            "step 33700, training loss Total= 0.0033935178, training acc total= 99.97295141220093%\n",
            "step 33750, training loss Total= 0.00090200827, training acc total= 99.97295141220093%\n",
            "step 33800, training loss Total= 0.00039866634, training acc total= 100.0%\n",
            "step 33850, training loss Total= 0.00043005144, training acc total= 99.98647570610046%\n",
            "step 33900, training loss Total= 0.00031638527, training acc total= 100.0%\n",
            "step 33950, training loss Total= 0.00030832642, training acc total= 100.0%\n",
            "step 34000, training loss Total= 0.00044293373, training acc total= 99.98647570610046%\n",
            "step 34050, training loss Total= 0.0003791807, training acc total= 100.0%\n",
            "step 34100, training loss Total= 0.00055324537, training acc total= 99.98647570610046%\n",
            "step 34150, training loss Total= 0.0009803674, training acc total= 99.98647570610046%\n",
            "step 34200, training loss Total= 0.005456962, training acc total= 99.97295141220093%\n",
            "step 34250, training loss Total= 0.004471545, training acc total= 99.94590282440186%\n",
            "step 34300, training loss Total= 0.0016474817, training acc total= 99.98647570610046%\n",
            "step 34350, training loss Total= 0.009141969, training acc total= 99.98647570610046%\n",
            "step 34400, training loss Total= 0.0017255128, training acc total= 100.0%\n",
            "step 34450, training loss Total= 0.00063549564, training acc total= 99.98647570610046%\n",
            "step 34500, training loss Total= 0.0004986871, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34550, training loss Total= 0.00044587321, training acc total= 100.0%\n",
            "step 34600, training loss Total= 0.0004609167, training acc total= 99.97295141220093%\n",
            "step 34650, training loss Total= 0.00048981584, training acc total= 99.97295141220093%\n",
            "step 34700, training loss Total= 0.00043408686, training acc total= 99.98647570610046%\n",
            "step 34750, training loss Total= 0.00048327862, training acc total= 99.97295141220093%\n",
            "step 34800, training loss Total= 0.0003621421, training acc total= 100.0%\n",
            "step 34850, training loss Total= 0.00036967345, training acc total= 100.0%\n",
            "step 34900, training loss Total= 0.00033684485, training acc total= 100.0%\n",
            "step 34950, training loss Total= 0.00031488892, training acc total= 100.0%\n",
            "step 35000, training loss Total= 0.00035603577, training acc total= 99.98647570610046%\n",
            "step 35050, training loss Total= 0.0003075614, training acc total= 100.0%\n",
            "step 35100, training loss Total= 0.0002967632, training acc total= 100.0%\n",
            "step 35150, training loss Total= 0.00028900837, training acc total= 100.0%\n",
            "step 35200, training loss Total= 0.00032569896, training acc total= 99.98647570610046%\n",
            "step 35250, training loss Total= 0.00028459713, training acc total= 100.0%\n",
            "step 35300, training loss Total= 0.0002899258, training acc total= 100.0%\n",
            "step 35350, training loss Total= 0.00031900455, training acc total= 99.98647570610046%\n",
            "step 35400, training loss Total= 0.000262177, training acc total= 100.0%\n",
            "step 35450, training loss Total= 0.00030898358, training acc total= 99.98647570610046%\n",
            "step 35500, training loss Total= 0.00029822823, training acc total= 100.0%\n",
            "step 35550, training loss Total= 0.00027697324, training acc total= 99.98647570610046%\n",
            "step 35600, training loss Total= 0.00028489725, training acc total= 100.0%\n",
            "step 35650, training loss Total= 0.00024532407, training acc total= 100.0%\n",
            "step 35700, training loss Total= 0.00026209457, training acc total= 100.0%\n",
            "step 35750, training loss Total= 0.00035280973, training acc total= 100.0%\n",
            "step 35800, training loss Total= 0.0007802666, training acc total= 99.98647570610046%\n",
            "step 35850, training loss Total= 0.0247875, training acc total= 99.94590282440186%\n",
            "step 35900, training loss Total= 0.016104873, training acc total= 99.91885423660278%\n",
            "step 35950, training loss Total= 0.0044220774, training acc total= 99.98647570610046%\n",
            "step 36000, training loss Total= 0.0009755064, training acc total= 99.97295141220093%\n",
            "step 36050, training loss Total= 0.0029157642, training acc total= 99.97295141220093%\n",
            "step 36100, training loss Total= 0.0010065499, training acc total= 99.97295141220093%\n",
            "step 36150, training loss Total= 0.0006380737, training acc total= 100.0%\n",
            "step 36200, training loss Total= 0.0005489908, training acc total= 100.0%\n",
            "step 36250, training loss Total= 0.000474124, training acc total= 100.0%\n",
            "step 36300, training loss Total= 0.00045339964, training acc total= 100.0%\n",
            "step 36350, training loss Total= 0.00043064766, training acc total= 100.0%\n",
            "step 36400, training loss Total= 0.0003884292, training acc total= 100.0%\n",
            "step 36450, training loss Total= 0.0003862326, training acc total= 99.98647570610046%\n",
            "step 36500, training loss Total= 0.00036195188, training acc total= 100.0%\n",
            "step 36550, training loss Total= 0.0003793852, training acc total= 99.98647570610046%\n",
            "step 36600, training loss Total= 0.00040378215, training acc total= 99.98647570610046%\n",
            "step 36650, training loss Total= 0.00037107698, training acc total= 99.98647570610046%\n",
            "step 36700, training loss Total= 0.00044253859, training acc total= 100.0%\n",
            "step 36750, training loss Total= 0.00045699245, training acc total= 99.98647570610046%\n",
            "step 36800, training loss Total= 0.00051963807, training acc total= 99.98647570610046%\n",
            "step 36850, training loss Total= 0.000310578, training acc total= 100.0%\n",
            "step 36900, training loss Total= 0.00030273962, training acc total= 100.0%\n",
            "step 36950, training loss Total= 0.00031169498, training acc total= 100.0%\n",
            "step 37000, training loss Total= 0.00027684777, training acc total= 100.0%\n",
            "step 37050, training loss Total= 0.00028814215, training acc total= 100.0%\n",
            "step 37100, training loss Total= 0.0002971605, training acc total= 100.0%\n",
            "step 37150, training loss Total= 0.00026626678, training acc total= 100.0%\n",
            "step 37200, training loss Total= 0.0002595258, training acc total= 100.0%\n",
            "step 37250, training loss Total= 0.00029133933, training acc total= 99.98647570610046%\n",
            "step 37300, training loss Total= 0.00032834767, training acc total= 100.0%\n",
            "step 37350, training loss Total= 0.0002664767, training acc total= 100.0%\n",
            "step 37400, training loss Total= 0.00030853436, training acc total= 100.0%\n",
            "step 37450, training loss Total= 0.0002710384, training acc total= 100.0%\n",
            "step 37500, training loss Total= 0.0003741284, training acc total= 100.0%\n",
            "step 37550, training loss Total= 0.00022980668, training acc total= 100.0%\n",
            "step 37600, training loss Total= 0.0002436933, training acc total= 100.0%\n",
            "step 37650, training loss Total= 0.000223764, training acc total= 100.0%\n",
            "step 37700, training loss Total= 0.00024082794, training acc total= 100.0%\n",
            "step 37750, training loss Total= 0.00021357342, training acc total= 100.0%\n",
            "step 37800, training loss Total= 0.00021258737, training acc total= 100.0%\n",
            "step 37850, training loss Total= 0.00020679973, training acc total= 100.0%\n",
            "step 37900, training loss Total= 0.00022254695, training acc total= 100.0%\n",
            "step 37950, training loss Total= 0.00021333975, training acc total= 100.0%\n",
            "step 38000, training loss Total= 0.00020056027, training acc total= 100.0%\n",
            "step 38050, training loss Total= 0.00020696284, training acc total= 100.0%\n",
            "step 38100, training loss Total= 0.00019024889, training acc total= 100.0%\n",
            "step 38150, training loss Total= 0.00019902423, training acc total= 100.0%\n",
            "step 38200, training loss Total= 0.00019955056, training acc total= 100.0%\n",
            "step 38250, training loss Total= 0.00024446903, training acc total= 99.98647570610046%\n",
            "step 38300, training loss Total= 0.0002427636, training acc total= 99.98647570610046%\n",
            "step 38350, training loss Total= 0.00025376256, training acc total= 99.98647570610046%\n",
            "step 38400, training loss Total= 0.0002838613, training acc total= 100.0%\n",
            "step 38450, training loss Total= 0.00019825055, training acc total= 100.0%\n",
            "step 38500, training loss Total= 0.00023837597, training acc total= 99.98647570610046%\n",
            "step 38550, training loss Total= 0.00025077278, training acc total= 99.98647570610046%\n",
            "step 38600, training loss Total= 0.00037252821, training acc total= 99.98647570610046%\n",
            "step 38650, training loss Total= 0.00034800495, training acc total= 99.98647570610046%\n",
            "step 38700, training loss Total= 0.00035420648, training acc total= 99.98647570610046%\n",
            "step 38750, training loss Total= 0.00037827904, training acc total= 99.98647570610046%\n",
            "step 38800, training loss Total= 0.001009909, training acc total= 99.98647570610046%\n",
            "step 38850, training loss Total= 0.0005084538, training acc total= 100.0%\n",
            "step 38900, training loss Total= 0.0012766051, training acc total= 99.98647570610046%\n",
            "step 38950, training loss Total= 0.00065032835, training acc total= 100.0%\n",
            "step 39000, training loss Total= 0.0017829142, training acc total= 99.95942711830139%\n",
            "step 39050, training loss Total= 0.0030663481, training acc total= 99.97295141220093%\n",
            "step 39100, training loss Total= 0.0023223716, training acc total= 100.0%\n",
            "step 39150, training loss Total= 0.0056974785, training acc total= 99.95942711830139%\n",
            "step 39200, training loss Total= 0.0027747229, training acc total= 99.98647570610046%\n",
            "step 39250, training loss Total= 0.0014662215, training acc total= 100.0%\n",
            "step 39300, training loss Total= 0.00052882277, training acc total= 99.98647570610046%\n",
            "step 39350, training loss Total= 0.0003563262, training acc total= 100.0%\n",
            "step 39400, training loss Total= 0.000329605, training acc total= 100.0%\n",
            "step 39450, training loss Total= 0.0002726301, training acc total= 100.0%\n",
            "step 39500, training loss Total= 0.00026313082, training acc total= 100.0%\n",
            "step 39550, training loss Total= 0.0002560457, training acc total= 100.0%\n",
            "step 39600, training loss Total= 0.00024064134, training acc total= 100.0%\n",
            "step 39650, training loss Total= 0.0002322135, training acc total= 100.0%\n",
            "step 39700, training loss Total= 0.00022160664, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39750, training loss Total= 0.00020892119, training acc total= 100.0%\n",
            "step 39800, training loss Total= 0.00020740021, training acc total= 100.0%\n",
            "step 39850, training loss Total= 0.00020044796, training acc total= 100.0%\n",
            "step 39900, training loss Total= 0.00020117236, training acc total= 100.0%\n",
            "step 39950, training loss Total= 0.00020662982, training acc total= 100.0%\n",
            "step 40000, training loss Total= 0.00022174868, training acc total= 100.0%\n",
            "step 40050, training loss Total= 0.00022389105, training acc total= 100.0%\n",
            "step 40100, training loss Total= 0.00018409666, training acc total= 100.0%\n",
            "step 40150, training loss Total= 0.00018192152, training acc total= 100.0%\n",
            "step 40200, training loss Total= 0.00017530947, training acc total= 100.0%\n",
            "step 40250, training loss Total= 0.00017529089, training acc total= 100.0%\n",
            "step 40300, training loss Total= 0.00017908044, training acc total= 100.0%\n",
            "step 40350, training loss Total= 0.00016592113, training acc total= 100.0%\n",
            "step 40400, training loss Total= 0.00018335404, training acc total= 100.0%\n",
            "step 40450, training loss Total= 0.000189469, training acc total= 100.0%\n",
            "step 40500, training loss Total= 0.00019265624, training acc total= 100.0%\n",
            "step 40550, training loss Total= 0.00022876733, training acc total= 100.0%\n",
            "step 40600, training loss Total= 0.00050270825, training acc total= 99.98647570610046%\n",
            "step 40650, training loss Total= 0.00049385923, training acc total= 99.98647570610046%\n",
            "step 40700, training loss Total= 0.00020466125, training acc total= 100.0%\n",
            "step 40750, training loss Total= 0.00016259399, training acc total= 100.0%\n",
            "step 40800, training loss Total= 0.0009148816, training acc total= 99.97295141220093%\n",
            "step 40850, training loss Total= 0.00043752627, training acc total= 99.98647570610046%\n",
            "step 40900, training loss Total= 0.0017504855, training acc total= 99.97295141220093%\n",
            "step 40950, training loss Total= 0.00023362972, training acc total= 100.0%\n",
            "step 41000, training loss Total= 0.00015559468, training acc total= 100.0%\n",
            "step 41050, training loss Total= 0.00014997533, training acc total= 100.0%\n",
            "step 41100, training loss Total= 0.00013728184, training acc total= 100.0%\n",
            "step 41150, training loss Total= 0.00013759627, training acc total= 100.0%\n",
            "step 41200, training loss Total= 0.00014305663, training acc total= 100.0%\n",
            "step 41250, training loss Total= 0.00017785933, training acc total= 100.0%\n",
            "step 41300, training loss Total= 0.00014869828, training acc total= 100.0%\n",
            "step 41350, training loss Total= 0.00013260187, training acc total= 100.0%\n",
            "step 41400, training loss Total= 0.00013481634, training acc total= 100.0%\n",
            "step 41450, training loss Total= 0.00012856023, training acc total= 100.0%\n",
            "step 41500, training loss Total= 0.00012389844, training acc total= 100.0%\n",
            "step 41550, training loss Total= 0.00013409414, training acc total= 100.0%\n",
            "step 41600, training loss Total= 0.00013124406, training acc total= 100.0%\n",
            "step 41650, training loss Total= 0.00014191265, training acc total= 100.0%\n",
            "step 41700, training loss Total= 0.0001598285, training acc total= 100.0%\n",
            "step 41750, training loss Total= 0.000115703435, training acc total= 100.0%\n",
            "step 41800, training loss Total= 0.00011505526, training acc total= 100.0%\n",
            "step 41850, training loss Total= 0.00010813764, training acc total= 100.0%\n",
            "step 41900, training loss Total= 0.00011129694, training acc total= 100.0%\n",
            "step 41950, training loss Total= 0.00013700398, training acc total= 100.0%\n",
            "step 42000, training loss Total= 0.00017065815, training acc total= 100.0%\n",
            "step 42050, training loss Total= 0.00012182608, training acc total= 100.0%\n",
            "step 42100, training loss Total= 0.00012347367, training acc total= 100.0%\n",
            "step 42150, training loss Total= 0.00012434949, training acc total= 100.0%\n",
            "step 42200, training loss Total= 0.00016654344, training acc total= 100.0%\n",
            "step 42250, training loss Total= 0.000115714385, training acc total= 100.0%\n",
            "step 42300, training loss Total= 0.0001077353, training acc total= 100.0%\n",
            "step 42350, training loss Total= 0.0021603913, training acc total= 99.98647570610046%\n",
            "step 42400, training loss Total= 0.03647416, training acc total= 99.87828135490417%\n",
            "step 42450, training loss Total= 0.010580531, training acc total= 99.89180564880371%\n",
            "step 42500, training loss Total= 0.0022006403, training acc total= 100.0%\n",
            "step 42550, training loss Total= 0.0014902283, training acc total= 100.0%\n",
            "step 42600, training loss Total= 0.0009907787, training acc total= 100.0%\n",
            "step 42650, training loss Total= 0.0004580659, training acc total= 100.0%\n",
            "step 42700, training loss Total= 0.00037079686, training acc total= 100.0%\n",
            "step 42750, training loss Total= 0.00032392, training acc total= 100.0%\n",
            "step 42800, training loss Total= 0.00030222296, training acc total= 100.0%\n",
            "step 42850, training loss Total= 0.00031000335, training acc total= 100.0%\n",
            "step 42900, training loss Total= 0.00026869963, training acc total= 100.0%\n",
            "step 42950, training loss Total= 0.0002512396, training acc total= 100.0%\n",
            "step 43000, training loss Total= 0.00024038798, training acc total= 100.0%\n",
            "step 43050, training loss Total= 0.00023460705, training acc total= 100.0%\n",
            "step 43100, training loss Total= 0.00022040686, training acc total= 100.0%\n",
            "step 43150, training loss Total= 0.00021556408, training acc total= 100.0%\n",
            "step 43200, training loss Total= 0.00021049357, training acc total= 100.0%\n",
            "step 43250, training loss Total= 0.00020300804, training acc total= 100.0%\n",
            "step 43300, training loss Total= 0.00020431507, training acc total= 100.0%\n",
            "step 43350, training loss Total= 0.00021308062, training acc total= 100.0%\n",
            "step 43400, training loss Total= 0.00021579492, training acc total= 100.0%\n",
            "step 43450, training loss Total= 0.00019137267, training acc total= 100.0%\n",
            "step 43500, training loss Total= 0.00018314649, training acc total= 100.0%\n",
            "step 43550, training loss Total= 0.00017108531, training acc total= 100.0%\n",
            "step 43600, training loss Total= 0.00018459112, training acc total= 100.0%\n",
            "step 43650, training loss Total= 0.0001780295, training acc total= 100.0%\n",
            "step 43700, training loss Total= 0.00016351386, training acc total= 100.0%\n",
            "step 43750, training loss Total= 0.00015776862, training acc total= 100.0%\n",
            "step 43800, training loss Total= 0.00015448888, training acc total= 100.0%\n",
            "step 43850, training loss Total= 0.00015820222, training acc total= 100.0%\n",
            "step 43900, training loss Total= 0.00015039278, training acc total= 100.0%\n",
            "step 43950, training loss Total= 0.00014601336, training acc total= 100.0%\n",
            "step 44000, training loss Total= 0.00015562282, training acc total= 100.0%\n",
            "step 44050, training loss Total= 0.00014505404, training acc total= 100.0%\n",
            "step 44100, training loss Total= 0.00015879478, training acc total= 100.0%\n",
            "step 44150, training loss Total= 0.00016944177, training acc total= 100.0%\n",
            "step 44200, training loss Total= 0.00013757631, training acc total= 100.0%\n",
            "step 44250, training loss Total= 0.00013765482, training acc total= 100.0%\n",
            "step 44300, training loss Total= 0.00013172782, training acc total= 100.0%\n",
            "step 44350, training loss Total= 0.00012579454, training acc total= 100.0%\n",
            "step 44400, training loss Total= 0.00012516964, training acc total= 100.0%\n",
            "step 44450, training loss Total= 0.00012656709, training acc total= 100.0%\n",
            "step 44500, training loss Total= 0.00012468226, training acc total= 100.0%\n",
            "step 44550, training loss Total= 0.00011910911, training acc total= 100.0%\n",
            "step 44600, training loss Total= 0.00012546133, training acc total= 100.0%\n",
            "step 44650, training loss Total= 0.0001295748, training acc total= 100.0%\n",
            "step 44700, training loss Total= 0.0001319391, training acc total= 100.0%\n",
            "step 44750, training loss Total= 0.00012192828, training acc total= 100.0%\n",
            "step 44800, training loss Total= 0.00012148067, training acc total= 100.0%\n",
            "step 44850, training loss Total= 0.00010895289, training acc total= 100.0%\n",
            "step 44900, training loss Total= 0.00011883522, training acc total= 100.0%\n",
            "step 44950, training loss Total= 0.00011911066, training acc total= 100.0%\n",
            "step 45000, training loss Total= 0.00012002764, training acc total= 100.0%\n",
            "step 45050, training loss Total= 0.00029848225, training acc total= 100.0%\n",
            "step 45100, training loss Total= 0.00011920635, training acc total= 100.0%\n",
            "step 45150, training loss Total= 9.414908e-05, training acc total= 100.0%\n",
            "step 45200, training loss Total= 9.348801e-05, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 45250, training loss Total= 9.690328e-05, training acc total= 100.0%\n",
            "step 45300, training loss Total= 9.2336144e-05, training acc total= 100.0%\n",
            "step 45350, training loss Total= 9.602592e-05, training acc total= 100.0%\n",
            "step 45400, training loss Total= 0.00010487824, training acc total= 100.0%\n",
            "step 45450, training loss Total= 0.00010220647, training acc total= 100.0%\n",
            "step 45500, training loss Total= 9.5626034e-05, training acc total= 100.0%\n",
            "step 45550, training loss Total= 0.00011331605, training acc total= 100.0%\n",
            "step 45600, training loss Total= 8.575009e-05, training acc total= 100.0%\n",
            "step 45650, training loss Total= 8.509231e-05, training acc total= 100.0%\n",
            "step 45700, training loss Total= 8.212646e-05, training acc total= 100.0%\n",
            "step 45750, training loss Total= 9.5733034e-05, training acc total= 100.0%\n",
            "step 45800, training loss Total= 0.00011283034, training acc total= 100.0%\n",
            "step 45850, training loss Total= 0.00018208145, training acc total= 100.0%\n",
            "step 45900, training loss Total= 0.0002812412, training acc total= 100.0%\n",
            "step 45950, training loss Total= 0.0036988566, training acc total= 99.94590282440186%\n",
            "step 46000, training loss Total= 0.007813261, training acc total= 99.91885423660278%\n",
            "step 46050, training loss Total= 0.01148841, training acc total= 99.93237853050232%\n",
            "step 46100, training loss Total= 0.0023403869, training acc total= 99.95942711830139%\n",
            "step 46150, training loss Total= 0.001207451, training acc total= 100.0%\n",
            "step 46200, training loss Total= 0.00057363644, training acc total= 100.0%\n",
            "step 46250, training loss Total= 0.00042149075, training acc total= 100.0%\n",
            "step 46300, training loss Total= 0.00037701076, training acc total= 100.0%\n",
            "step 46350, training loss Total= 0.0003215185, training acc total= 100.0%\n",
            "step 46400, training loss Total= 0.0003160758, training acc total= 100.0%\n",
            "step 46450, training loss Total= 0.0002870032, training acc total= 100.0%\n",
            "step 46500, training loss Total= 0.00027932625, training acc total= 100.0%\n",
            "step 46550, training loss Total= 0.00024724737, training acc total= 100.0%\n",
            "step 46600, training loss Total= 0.00024101266, training acc total= 100.0%\n",
            "step 46650, training loss Total= 0.00022724073, training acc total= 100.0%\n",
            "step 46700, training loss Total= 0.00022406582, training acc total= 100.0%\n",
            "step 46750, training loss Total= 0.0002160888, training acc total= 100.0%\n",
            "step 46800, training loss Total= 0.00020930199, training acc total= 100.0%\n",
            "step 46850, training loss Total= 0.00020077627, training acc total= 100.0%\n",
            "step 46900, training loss Total= 0.00022549613, training acc total= 100.0%\n",
            "step 46950, training loss Total= 0.0002475831, training acc total= 100.0%\n",
            "step 47000, training loss Total= 0.0002042424, training acc total= 100.0%\n",
            "step 47050, training loss Total= 0.00041599647, training acc total= 99.98647570610046%\n",
            "step 47100, training loss Total= 0.00018689243, training acc total= 100.0%\n",
            "step 47150, training loss Total= 0.0001776957, training acc total= 100.0%\n",
            "step 47200, training loss Total= 0.00016406868, training acc total= 100.0%\n",
            "step 47250, training loss Total= 0.00016808185, training acc total= 100.0%\n",
            "step 47300, training loss Total= 0.00015403566, training acc total= 100.0%\n",
            "step 47350, training loss Total= 0.00015624262, training acc total= 100.0%\n",
            "step 47400, training loss Total= 0.00015483862, training acc total= 100.0%\n",
            "step 47450, training loss Total= 0.0001483276, training acc total= 100.0%\n",
            "step 47500, training loss Total= 0.00014129528, training acc total= 100.0%\n",
            "step 47550, training loss Total= 0.00014521078, training acc total= 100.0%\n",
            "step 47600, training loss Total= 0.0001345731, training acc total= 100.0%\n",
            "step 47650, training loss Total= 0.00014142056, training acc total= 100.0%\n",
            "step 47700, training loss Total= 0.00013068032, training acc total= 100.0%\n",
            "step 47750, training loss Total= 0.00012580528, training acc total= 100.0%\n",
            "step 47800, training loss Total= 0.00012072277, training acc total= 100.0%\n",
            "step 47850, training loss Total= 0.00011852859, training acc total= 100.0%\n",
            "step 47900, training loss Total= 0.00011571217, training acc total= 100.0%\n",
            "step 47950, training loss Total= 0.00012564362, training acc total= 100.0%\n",
            "step 48000, training loss Total= 0.00012834901, training acc total= 100.0%\n",
            "step 48050, training loss Total= 0.000108853914, training acc total= 100.0%\n",
            "step 48100, training loss Total= 0.0001282811, training acc total= 100.0%\n",
            "step 48150, training loss Total= 0.00010352458, training acc total= 100.0%\n",
            "step 48200, training loss Total= 0.00010274119, training acc total= 100.0%\n",
            "step 48250, training loss Total= 9.785493e-05, training acc total= 100.0%\n",
            "step 48300, training loss Total= 0.00010459933, training acc total= 100.0%\n",
            "step 48350, training loss Total= 0.000114760245, training acc total= 100.0%\n",
            "step 48400, training loss Total= 0.0001242854, training acc total= 100.0%\n",
            "step 48450, training loss Total= 0.000100222096, training acc total= 100.0%\n",
            "step 48500, training loss Total= 9.007291e-05, training acc total= 100.0%\n",
            "step 48550, training loss Total= 9.193524e-05, training acc total= 100.0%\n",
            "step 48600, training loss Total= 8.791846e-05, training acc total= 100.0%\n",
            "step 48650, training loss Total= 8.525374e-05, training acc total= 100.0%\n",
            "step 48700, training loss Total= 8.918766e-05, training acc total= 100.0%\n",
            "step 48750, training loss Total= 0.000104153674, training acc total= 100.0%\n",
            "step 48800, training loss Total= 9.2997136e-05, training acc total= 100.0%\n",
            "step 48850, training loss Total= 8.732549e-05, training acc total= 100.0%\n",
            "step 48900, training loss Total= 8.074162e-05, training acc total= 100.0%\n",
            "step 48950, training loss Total= 8.122298e-05, training acc total= 100.0%\n",
            "step 49000, training loss Total= 7.822134e-05, training acc total= 100.0%\n",
            "step 49050, training loss Total= 8.599759e-05, training acc total= 100.0%\n",
            "step 49100, training loss Total= 7.770197e-05, training acc total= 100.0%\n",
            "step 49150, training loss Total= 8.730908e-05, training acc total= 100.0%\n",
            "step 49200, training loss Total= 7.377904e-05, training acc total= 100.0%\n",
            "step 49250, training loss Total= 7.610295e-05, training acc total= 100.0%\n",
            "step 49300, training loss Total= 7.0528644e-05, training acc total= 100.0%\n",
            "step 49350, training loss Total= 7.078338e-05, training acc total= 100.0%\n",
            "step 49400, training loss Total= 7.221648e-05, training acc total= 100.0%\n",
            "step 49450, training loss Total= 6.882954e-05, training acc total= 100.0%\n",
            "step 49500, training loss Total= 7.106791e-05, training acc total= 100.0%\n",
            "step 49550, training loss Total= 6.529177e-05, training acc total= 100.0%\n",
            "step 49600, training loss Total= 6.484533e-05, training acc total= 100.0%\n",
            "step 49650, training loss Total= 7.016889e-05, training acc total= 100.0%\n",
            "step 49700, training loss Total= 7.970124e-05, training acc total= 100.0%\n",
            "step 49750, training loss Total= 6.830431e-05, training acc total= 100.0%\n",
            "step 49800, training loss Total= 0.00230635, training acc total= 99.98647570610046%\n",
            "step 49850, training loss Total= 0.0011841907, training acc total= 99.98647570610046%\n",
            "step 49900, training loss Total= 0.0006780471, training acc total= 99.97295141220093%\n",
            "step 49950, training loss Total= 0.0070528034, training acc total= 99.91885423660278%\n",
            "step 50000, training loss Total= 0.006089446, training acc total= 99.95942711830139%\n",
            "step 50050, training loss Total= 0.0025516893, training acc total= 99.98647570610046%\n",
            "step 50100, training loss Total= 0.001408776, training acc total= 100.0%\n",
            "step 50150, training loss Total= 0.0022459354, training acc total= 99.98647570610046%\n",
            "step 50200, training loss Total= 0.0023847606, training acc total= 99.98647570610046%\n",
            "step 50250, training loss Total= 0.00034262394, training acc total= 99.98647570610046%\n",
            "step 50300, training loss Total= 0.00032875463, training acc total= 100.0%\n",
            "step 50350, training loss Total= 0.00024951942, training acc total= 100.0%\n",
            "step 50400, training loss Total= 0.00024448123, training acc total= 99.98647570610046%\n",
            "step 50450, training loss Total= 0.00020348317, training acc total= 100.0%\n",
            "step 50500, training loss Total= 0.0001852421, training acc total= 100.0%\n",
            "step 50550, training loss Total= 0.00016789249, training acc total= 100.0%\n",
            "step 50600, training loss Total= 0.00015900386, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 50650, training loss Total= 0.00015835249, training acc total= 100.0%\n",
            "step 50700, training loss Total= 0.00014916174, training acc total= 100.0%\n",
            "step 50750, training loss Total= 0.00014019664, training acc total= 100.0%\n",
            "step 50800, training loss Total= 0.00014101976, training acc total= 100.0%\n",
            "step 50850, training loss Total= 0.00013153105, training acc total= 100.0%\n",
            "step 50900, training loss Total= 0.00012707274, training acc total= 100.0%\n",
            "step 50950, training loss Total= 0.0001230074, training acc total= 100.0%\n",
            "step 51000, training loss Total= 0.00011899862, training acc total= 100.0%\n",
            "step 51050, training loss Total= 0.000118262804, training acc total= 100.0%\n",
            "step 51100, training loss Total= 0.000114020346, training acc total= 100.0%\n",
            "step 51150, training loss Total= 0.000109570035, training acc total= 100.0%\n",
            "step 51200, training loss Total= 0.000105423576, training acc total= 100.0%\n",
            "step 51250, training loss Total= 0.0001028803, training acc total= 100.0%\n",
            "step 51300, training loss Total= 9.98717e-05, training acc total= 100.0%\n",
            "step 51350, training loss Total= 0.0001004027, training acc total= 100.0%\n",
            "step 51400, training loss Total= 9.4415e-05, training acc total= 100.0%\n",
            "step 51450, training loss Total= 9.270738e-05, training acc total= 100.0%\n",
            "step 51500, training loss Total= 9.666297e-05, training acc total= 100.0%\n",
            "step 51550, training loss Total= 0.000101209545, training acc total= 100.0%\n",
            "step 51600, training loss Total= 8.818168e-05, training acc total= 100.0%\n",
            "step 51650, training loss Total= 8.603428e-05, training acc total= 100.0%\n",
            "step 51700, training loss Total= 8.53777e-05, training acc total= 100.0%\n",
            "step 51750, training loss Total= 8.451021e-05, training acc total= 100.0%\n",
            "step 51800, training loss Total= 8.4267274e-05, training acc total= 100.0%\n",
            "step 51850, training loss Total= 8.232937e-05, training acc total= 100.0%\n",
            "step 51900, training loss Total= 7.755038e-05, training acc total= 100.0%\n",
            "step 51950, training loss Total= 7.9723235e-05, training acc total= 100.0%\n",
            "step 52000, training loss Total= 7.764985e-05, training acc total= 100.0%\n",
            "step 52050, training loss Total= 7.814324e-05, training acc total= 100.0%\n",
            "step 52100, training loss Total= 7.574812e-05, training acc total= 100.0%\n",
            "step 52150, training loss Total= 7.285648e-05, training acc total= 100.0%\n",
            "step 52200, training loss Total= 6.937617e-05, training acc total= 100.0%\n",
            "step 52250, training loss Total= 6.9203255e-05, training acc total= 100.0%\n",
            "step 52300, training loss Total= 6.8261856e-05, training acc total= 100.0%\n",
            "step 52350, training loss Total= 6.636123e-05, training acc total= 100.0%\n",
            "step 52400, training loss Total= 7.061858e-05, training acc total= 100.0%\n",
            "step 52450, training loss Total= 6.656179e-05, training acc total= 100.0%\n",
            "step 52500, training loss Total= 6.3661304e-05, training acc total= 100.0%\n",
            "step 52550, training loss Total= 6.2518884e-05, training acc total= 100.0%\n",
            "step 52600, training loss Total= 6.201075e-05, training acc total= 100.0%\n",
            "step 52650, training loss Total= 6.058088e-05, training acc total= 100.0%\n",
            "step 52700, training loss Total= 7.5906595e-05, training acc total= 100.0%\n",
            "step 52750, training loss Total= 6.3290456e-05, training acc total= 100.0%\n",
            "step 52800, training loss Total= 5.8407666e-05, training acc total= 100.0%\n",
            "step 52850, training loss Total= 5.689275e-05, training acc total= 100.0%\n",
            "step 52900, training loss Total= 6.182934e-05, training acc total= 100.0%\n",
            "step 52950, training loss Total= 5.9651327e-05, training acc total= 100.0%\n",
            "step 53000, training loss Total= 6.2796265e-05, training acc total= 100.0%\n",
            "step 53050, training loss Total= 6.808052e-05, training acc total= 100.0%\n",
            "step 53100, training loss Total= 5.6071538e-05, training acc total= 100.0%\n",
            "step 53150, training loss Total= 5.797314e-05, training acc total= 100.0%\n",
            "step 53200, training loss Total= 5.4500793e-05, training acc total= 100.0%\n",
            "step 53250, training loss Total= 5.369348e-05, training acc total= 100.0%\n",
            "step 53300, training loss Total= 5.688613e-05, training acc total= 100.0%\n",
            "step 53350, training loss Total= 6.2128594e-05, training acc total= 100.0%\n",
            "step 53400, training loss Total= 7.291116e-05, training acc total= 100.0%\n",
            "step 53450, training loss Total= 6.998354e-05, training acc total= 100.0%\n",
            "step 53500, training loss Total= 5.2805724e-05, training acc total= 100.0%\n",
            "step 53550, training loss Total= 5.1855775e-05, training acc total= 100.0%\n",
            "step 53600, training loss Total= 4.9265673e-05, training acc total= 100.0%\n",
            "step 53650, training loss Total= 6.5913606e-05, training acc total= 100.0%\n",
            "step 53700, training loss Total= 0.0004995227, training acc total= 99.98647570610046%\n",
            "step 53750, training loss Total= 0.0006642514, training acc total= 99.98647570610046%\n",
            "step 53800, training loss Total= 0.01136847, training acc total= 99.97295141220093%\n",
            "step 53850, training loss Total= 0.00886508, training acc total= 99.94590282440186%\n",
            "step 53900, training loss Total= 0.03289027, training acc total= 99.98647570610046%\n",
            "step 53950, training loss Total= 0.013158293, training acc total= 99.95942711830139%\n",
            "step 54000, training loss Total= 0.0010453172, training acc total= 100.0%\n",
            "step 54050, training loss Total= 0.00068223535, training acc total= 100.0%\n",
            "step 54100, training loss Total= 0.00040926496, training acc total= 100.0%\n",
            "step 54150, training loss Total= 0.0003357556, training acc total= 100.0%\n",
            "step 54200, training loss Total= 0.00027445442, training acc total= 100.0%\n",
            "step 54250, training loss Total= 0.00025223778, training acc total= 100.0%\n",
            "step 54300, training loss Total= 0.00023936066, training acc total= 100.0%\n",
            "step 54350, training loss Total= 0.00022230767, training acc total= 100.0%\n",
            "step 54400, training loss Total= 0.00021378863, training acc total= 100.0%\n",
            "step 54450, training loss Total= 0.000202473, training acc total= 100.0%\n",
            "step 54500, training loss Total= 0.0001933186, training acc total= 100.0%\n",
            "step 54550, training loss Total= 0.00018525546, training acc total= 100.0%\n",
            "step 54600, training loss Total= 0.00017581061, training acc total= 100.0%\n",
            "step 54650, training loss Total= 0.00016990742, training acc total= 100.0%\n",
            "step 54700, training loss Total= 0.0001620823, training acc total= 100.0%\n",
            "step 54750, training loss Total= 0.00015986271, training acc total= 100.0%\n",
            "step 54800, training loss Total= 0.00015237005, training acc total= 100.0%\n",
            "step 54850, training loss Total= 0.00014571418, training acc total= 100.0%\n",
            "step 54900, training loss Total= 0.00014014605, training acc total= 100.0%\n",
            "step 54950, training loss Total= 0.00013525918, training acc total= 100.0%\n",
            "step 55000, training loss Total= 0.00013088893, training acc total= 100.0%\n",
            "step 55050, training loss Total= 0.00012763454, training acc total= 100.0%\n",
            "step 55100, training loss Total= 0.00012401456, training acc total= 100.0%\n",
            "step 55150, training loss Total= 0.00012335143, training acc total= 100.0%\n",
            "step 55200, training loss Total= 0.00012523771, training acc total= 100.0%\n",
            "step 55250, training loss Total= 0.00012874157, training acc total= 100.0%\n",
            "step 55300, training loss Total= 0.00012962313, training acc total= 100.0%\n",
            "step 55350, training loss Total= 0.00010993469, training acc total= 100.0%\n",
            "step 55400, training loss Total= 0.00010983204, training acc total= 100.0%\n",
            "step 55450, training loss Total= 0.00010570611, training acc total= 100.0%\n",
            "step 55500, training loss Total= 0.00011160964, training acc total= 100.0%\n",
            "step 55550, training loss Total= 9.8379656e-05, training acc total= 100.0%\n",
            "step 55600, training loss Total= 9.620383e-05, training acc total= 100.0%\n",
            "step 55650, training loss Total= 9.271609e-05, training acc total= 100.0%\n",
            "step 55700, training loss Total= 9.369997e-05, training acc total= 100.0%\n",
            "step 55750, training loss Total= 9.522123e-05, training acc total= 100.0%\n",
            "step 55800, training loss Total= 8.585111e-05, training acc total= 100.0%\n",
            "step 55850, training loss Total= 8.545379e-05, training acc total= 100.0%\n",
            "step 55900, training loss Total= 8.167621e-05, training acc total= 100.0%\n",
            "step 55950, training loss Total= 7.944936e-05, training acc total= 100.0%\n",
            "step 56000, training loss Total= 7.796783e-05, training acc total= 100.0%\n",
            "step 56050, training loss Total= 7.744895e-05, training acc total= 100.0%\n",
            "step 56100, training loss Total= 7.565335e-05, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 56150, training loss Total= 7.4100346e-05, training acc total= 100.0%\n",
            "step 56200, training loss Total= 7.180451e-05, training acc total= 100.0%\n",
            "step 56250, training loss Total= 7.224029e-05, training acc total= 100.0%\n",
            "step 56300, training loss Total= 6.93041e-05, training acc total= 100.0%\n",
            "step 56350, training loss Total= 6.7512745e-05, training acc total= 100.0%\n",
            "step 56400, training loss Total= 6.749332e-05, training acc total= 100.0%\n",
            "step 56450, training loss Total= 6.474875e-05, training acc total= 100.0%\n",
            "step 56500, training loss Total= 6.861071e-05, training acc total= 100.0%\n",
            "step 56550, training loss Total= 6.483749e-05, training acc total= 100.0%\n",
            "step 56600, training loss Total= 6.269029e-05, training acc total= 100.0%\n",
            "step 56650, training loss Total= 6.198897e-05, training acc total= 100.0%\n",
            "step 56700, training loss Total= 5.9720052e-05, training acc total= 100.0%\n",
            "step 56750, training loss Total= 6.165661e-05, training acc total= 100.0%\n",
            "step 56800, training loss Total= 6.586456e-05, training acc total= 100.0%\n",
            "step 56850, training loss Total= 5.8240035e-05, training acc total= 100.0%\n",
            "step 56900, training loss Total= 5.772998e-05, training acc total= 100.0%\n",
            "step 56950, training loss Total= 5.957065e-05, training acc total= 100.0%\n",
            "step 57000, training loss Total= 5.472446e-05, training acc total= 100.0%\n",
            "step 57050, training loss Total= 5.4762546e-05, training acc total= 100.0%\n",
            "step 57100, training loss Total= 5.2035273e-05, training acc total= 100.0%\n",
            "step 57150, training loss Total= 5.2693144e-05, training acc total= 100.0%\n",
            "step 57200, training loss Total= 5.5596054e-05, training acc total= 100.0%\n",
            "step 57250, training loss Total= 6.120116e-05, training acc total= 100.0%\n",
            "step 57300, training loss Total= 4.99186e-05, training acc total= 100.0%\n",
            "step 57350, training loss Total= 4.7832327e-05, training acc total= 100.0%\n",
            "step 57400, training loss Total= 4.7066856e-05, training acc total= 100.0%\n",
            "step 57450, training loss Total= 4.9488204e-05, training acc total= 100.0%\n",
            "step 57500, training loss Total= 4.858759e-05, training acc total= 100.0%\n",
            "step 57550, training loss Total= 5.1650895e-05, training acc total= 100.0%\n",
            "step 57600, training loss Total= 4.5580564e-05, training acc total= 100.0%\n",
            "step 57650, training loss Total= 5.1708415e-05, training acc total= 100.0%\n",
            "step 57700, training loss Total= 5.0986615e-05, training acc total= 100.0%\n",
            "step 57750, training loss Total= 4.2073996e-05, training acc total= 100.0%\n",
            "step 57800, training loss Total= 4.448674e-05, training acc total= 100.0%\n",
            "step 57850, training loss Total= 4.8159072e-05, training acc total= 100.0%\n",
            "step 57900, training loss Total= 4.5102206e-05, training acc total= 100.0%\n",
            "step 57950, training loss Total= 4.2732237e-05, training acc total= 100.0%\n",
            "step 58000, training loss Total= 3.8755465e-05, training acc total= 100.0%\n",
            "step 58050, training loss Total= 4.4418583e-05, training acc total= 100.0%\n",
            "step 58100, training loss Total= 3.7485053e-05, training acc total= 100.0%\n",
            "step 58150, training loss Total= 3.6586003e-05, training acc total= 100.0%\n",
            "step 58200, training loss Total= 3.994604e-05, training acc total= 100.0%\n",
            "step 58250, training loss Total= 3.955288e-05, training acc total= 100.0%\n",
            "step 58300, training loss Total= 4.7463658e-05, training acc total= 100.0%\n",
            "step 58350, training loss Total= 5.1002375e-05, training acc total= 100.0%\n",
            "step 58400, training loss Total= 5.463408e-05, training acc total= 100.0%\n",
            "step 58450, training loss Total= 0.00031994874, training acc total= 100.0%\n",
            "step 58500, training loss Total= 0.00049618236, training acc total= 99.98647570610046%\n",
            "step 58550, training loss Total= 0.00066449, training acc total= 99.98647570610046%\n",
            "step 58600, training loss Total= 0.000991463, training acc total= 99.98647570610046%\n",
            "step 58650, training loss Total= 0.000180803, training acc total= 100.0%\n",
            "step 58700, training loss Total= 0.009764124, training acc total= 99.98647570610046%\n",
            "step 58750, training loss Total= 0.0024121203, training acc total= 99.97295141220093%\n",
            "step 58800, training loss Total= 0.0018039369, training acc total= 100.0%\n",
            "step 58850, training loss Total= 0.00046739774, training acc total= 100.0%\n",
            "step 58900, training loss Total= 0.00034869555, training acc total= 100.0%\n",
            "step 58950, training loss Total= 0.00023791628, training acc total= 100.0%\n",
            "step 59000, training loss Total= 0.00018401067, training acc total= 100.0%\n",
            "step 59050, training loss Total= 0.00016157104, training acc total= 100.0%\n",
            "step 59100, training loss Total= 0.00015091614, training acc total= 100.0%\n",
            "step 59150, training loss Total= 0.00014461517, training acc total= 100.0%\n",
            "step 59200, training loss Total= 0.00013824472, training acc total= 100.0%\n",
            "step 59250, training loss Total= 0.00014418435, training acc total= 100.0%\n",
            "step 59300, training loss Total= 0.00012504324, training acc total= 100.0%\n",
            "step 59350, training loss Total= 0.000116505485, training acc total= 100.0%\n",
            "step 59400, training loss Total= 0.00011100089, training acc total= 100.0%\n",
            "step 59450, training loss Total= 0.00011538121, training acc total= 100.0%\n",
            "step 59500, training loss Total= 0.00010192835, training acc total= 100.0%\n",
            "step 59550, training loss Total= 0.00010187541, training acc total= 100.0%\n",
            "step 59600, training loss Total= 0.00010520505, training acc total= 100.0%\n",
            "step 59650, training loss Total= 0.000101984995, training acc total= 100.0%\n",
            "step 59700, training loss Total= 9.106602e-05, training acc total= 100.0%\n",
            "step 59750, training loss Total= 8.7462184e-05, training acc total= 100.0%\n",
            "step 59800, training loss Total= 8.8395485e-05, training acc total= 100.0%\n",
            "step 59850, training loss Total= 9.066856e-05, training acc total= 100.0%\n",
            "step 59900, training loss Total= 8.050678e-05, training acc total= 100.0%\n",
            "step 59950, training loss Total= 8.049132e-05, training acc total= 100.0%\n",
            "step 60000, training loss Total= 7.464575e-05, training acc total= 100.0%\n",
            "step 60050, training loss Total= 7.180491e-05, training acc total= 100.0%\n",
            "step 60100, training loss Total= 7.8304685e-05, training acc total= 100.0%\n",
            "step 60150, training loss Total= 7.620064e-05, training acc total= 100.0%\n",
            "step 60200, training loss Total= 6.9947164e-05, training acc total= 100.0%\n",
            "step 60250, training loss Total= 6.453328e-05, training acc total= 100.0%\n",
            "step 60300, training loss Total= 6.683312e-05, training acc total= 100.0%\n",
            "step 60350, training loss Total= 6.156747e-05, training acc total= 100.0%\n",
            "step 60400, training loss Total= 6.0470196e-05, training acc total= 100.0%\n",
            "step 60450, training loss Total= 5.9565347e-05, training acc total= 100.0%\n",
            "step 60500, training loss Total= 6.522375e-05, training acc total= 100.0%\n",
            "step 60550, training loss Total= 5.7900055e-05, training acc total= 100.0%\n",
            "step 60600, training loss Total= 7.7983066e-05, training acc total= 100.0%\n",
            "step 60650, training loss Total= 5.7501453e-05, training acc total= 100.0%\n",
            "step 60700, training loss Total= 5.4184828e-05, training acc total= 100.0%\n",
            "step 60750, training loss Total= 5.335334e-05, training acc total= 100.0%\n",
            "step 60800, training loss Total= 5.269485e-05, training acc total= 100.0%\n",
            "step 60850, training loss Total= 6.0742015e-05, training acc total= 100.0%\n",
            "step 60900, training loss Total= 6.235279e-05, training acc total= 100.0%\n",
            "step 60950, training loss Total= 6.6051696e-05, training acc total= 100.0%\n",
            "step 61000, training loss Total= 6.6597e-05, training acc total= 100.0%\n",
            "step 61050, training loss Total= 8.47033e-05, training acc total= 100.0%\n",
            "step 61100, training loss Total= 0.00031995564, training acc total= 99.98647570610046%\n",
            "step 61150, training loss Total= 0.0017005261, training acc total= 99.94590282440186%\n",
            "step 61200, training loss Total= 0.00012398147, training acc total= 100.0%\n",
            "step 61250, training loss Total= 7.8601064e-05, training acc total= 100.0%\n",
            "step 61300, training loss Total= 0.00013979833, training acc total= 100.0%\n",
            "step 61350, training loss Total= 5.466746e-05, training acc total= 100.0%\n",
            "step 61400, training loss Total= 5.0492647e-05, training acc total= 100.0%\n",
            "step 61450, training loss Total= 4.914119e-05, training acc total= 100.0%\n",
            "step 61500, training loss Total= 4.711597e-05, training acc total= 100.0%\n",
            "step 61550, training loss Total= 4.3775184e-05, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 61600, training loss Total= 4.388349e-05, training acc total= 100.0%\n",
            "step 61650, training loss Total= 3.934982e-05, training acc total= 100.0%\n",
            "step 61700, training loss Total= 3.9151335e-05, training acc total= 100.0%\n",
            "step 61750, training loss Total= 3.762736e-05, training acc total= 100.0%\n",
            "step 61800, training loss Total= 3.8864946e-05, training acc total= 100.0%\n",
            "step 61850, training loss Total= 3.6440688e-05, training acc total= 100.0%\n",
            "step 61900, training loss Total= 3.549115e-05, training acc total= 100.0%\n",
            "step 61950, training loss Total= 3.541477e-05, training acc total= 100.0%\n",
            "step 62000, training loss Total= 3.3963213e-05, training acc total= 100.0%\n",
            "step 62050, training loss Total= 3.3782417e-05, training acc total= 100.0%\n",
            "step 62100, training loss Total= 3.4798944e-05, training acc total= 100.0%\n",
            "step 62150, training loss Total= 3.2889755e-05, training acc total= 100.0%\n",
            "step 62200, training loss Total= 3.3676384e-05, training acc total= 100.0%\n",
            "step 62250, training loss Total= 3.3602548e-05, training acc total= 100.0%\n",
            "step 62300, training loss Total= 3.1756303e-05, training acc total= 100.0%\n",
            "step 62350, training loss Total= 3.29175e-05, training acc total= 100.0%\n",
            "step 62400, training loss Total= 3.0484314e-05, training acc total= 100.0%\n",
            "step 62450, training loss Total= 3.198367e-05, training acc total= 100.0%\n",
            "step 62500, training loss Total= 3.1918295e-05, training acc total= 100.0%\n",
            "step 62550, training loss Total= 3.685596e-05, training acc total= 100.0%\n",
            "step 62600, training loss Total= 3.2815035e-05, training acc total= 100.0%\n",
            "step 62650, training loss Total= 3.2196738e-05, training acc total= 100.0%\n",
            "step 62700, training loss Total= 2.9465573e-05, training acc total= 100.0%\n",
            "step 62750, training loss Total= 2.8360208e-05, training acc total= 100.0%\n",
            "step 62800, training loss Total= 3.153906e-05, training acc total= 100.0%\n",
            "step 62850, training loss Total= 3.413094e-05, training acc total= 100.0%\n",
            "step 62900, training loss Total= 2.660701e-05, training acc total= 100.0%\n",
            "step 62950, training loss Total= 3.4961595e-05, training acc total= 100.0%\n",
            "step 63000, training loss Total= 2.8363676e-05, training acc total= 100.0%\n",
            "step 63050, training loss Total= 2.5572595e-05, training acc total= 100.0%\n",
            "step 63100, training loss Total= 2.4774245e-05, training acc total= 100.0%\n",
            "step 63150, training loss Total= 2.4087436e-05, training acc total= 100.0%\n",
            "step 63200, training loss Total= 2.6223119e-05, training acc total= 100.0%\n",
            "step 63250, training loss Total= 2.2694532e-05, training acc total= 100.0%\n",
            "step 63300, training loss Total= 2.272698e-05, training acc total= 100.0%\n",
            "step 63350, training loss Total= 2.3523815e-05, training acc total= 100.0%\n",
            "step 63400, training loss Total= 2.659162e-05, training acc total= 100.0%\n",
            "step 63450, training loss Total= 2.2280074e-05, training acc total= 100.0%\n",
            "step 63500, training loss Total= 2.5583462e-05, training acc total= 100.0%\n",
            "step 63550, training loss Total= 2.8492834e-05, training acc total= 100.0%\n",
            "step 63600, training loss Total= 2.58029e-05, training acc total= 100.0%\n",
            "step 63650, training loss Total= 2.7651677e-05, training acc total= 100.0%\n",
            "step 63700, training loss Total= 2.0145128e-05, training acc total= 100.0%\n",
            "step 63750, training loss Total= 2.03486e-05, training acc total= 100.0%\n",
            "step 63800, training loss Total= 1.881625e-05, training acc total= 100.0%\n",
            "step 63850, training loss Total= 1.8782313e-05, training acc total= 100.0%\n",
            "step 63900, training loss Total= 2.002607e-05, training acc total= 100.0%\n",
            "step 63950, training loss Total= 1.866741e-05, training acc total= 100.0%\n",
            "step 64000, training loss Total= 2.0637483e-05, training acc total= 100.0%\n",
            "step 64050, training loss Total= 1.7924316e-05, training acc total= 100.0%\n",
            "step 64100, training loss Total= 1.896235e-05, training acc total= 100.0%\n",
            "step 64150, training loss Total= 1.6590177e-05, training acc total= 100.0%\n",
            "step 64200, training loss Total= 1.725133e-05, training acc total= 100.0%\n",
            "step 64250, training loss Total= 1.8354467e-05, training acc total= 100.0%\n",
            "step 64300, training loss Total= 1.727527e-05, training acc total= 100.0%\n",
            "step 64350, training loss Total= 1.7650984e-05, training acc total= 100.0%\n",
            "step 64400, training loss Total= 1.6607548e-05, training acc total= 100.0%\n",
            "step 64450, training loss Total= 1.548818e-05, training acc total= 100.0%\n",
            "step 64500, training loss Total= 1.6354705e-05, training acc total= 100.0%\n",
            "step 64550, training loss Total= 1.5110375e-05, training acc total= 100.0%\n",
            "step 64600, training loss Total= 1.6221422e-05, training acc total= 100.0%\n",
            "step 64650, training loss Total= 1.7681308e-05, training acc total= 100.0%\n",
            "step 64700, training loss Total= 1.51809045e-05, training acc total= 100.0%\n",
            "step 64750, training loss Total= 2.8349454e-05, training acc total= 100.0%\n",
            "step 64800, training loss Total= 0.007315973, training acc total= 99.97295141220093%\n",
            "step 64850, training loss Total= 0.0048693134, training acc total= 99.91885423660278%\n",
            "step 64900, training loss Total= 0.0022272984, training acc total= 100.0%\n",
            "step 64950, training loss Total= 0.0008787657, training acc total= 99.97295141220093%\n",
            "step 65000, training loss Total= 0.0006662351, training acc total= 100.0%\n",
            "step 65050, training loss Total= 0.00030656325, training acc total= 100.0%\n",
            "step 65100, training loss Total= 0.00025348875, training acc total= 99.98647570610046%\n",
            "step 65150, training loss Total= 0.00013611586, training acc total= 100.0%\n",
            "step 65200, training loss Total= 0.000115487164, training acc total= 100.0%\n",
            "step 65250, training loss Total= 0.000104584964, training acc total= 100.0%\n",
            "step 65300, training loss Total= 9.109688e-05, training acc total= 100.0%\n",
            "step 65350, training loss Total= 8.391783e-05, training acc total= 100.0%\n",
            "step 65400, training loss Total= 7.695225e-05, training acc total= 100.0%\n",
            "step 65450, training loss Total= 7.505839e-05, training acc total= 100.0%\n",
            "step 65500, training loss Total= 7.0805574e-05, training acc total= 100.0%\n",
            "step 65550, training loss Total= 6.5235974e-05, training acc total= 100.0%\n",
            "step 65600, training loss Total= 6.289474e-05, training acc total= 100.0%\n",
            "step 65650, training loss Total= 6.0101993e-05, training acc total= 100.0%\n",
            "step 65700, training loss Total= 5.8933758e-05, training acc total= 100.0%\n",
            "step 65750, training loss Total= 5.7676385e-05, training acc total= 100.0%\n",
            "step 65800, training loss Total= 5.652663e-05, training acc total= 100.0%\n",
            "step 65850, training loss Total= 5.3452302e-05, training acc total= 100.0%\n",
            "step 65900, training loss Total= 5.1869392e-05, training acc total= 100.0%\n",
            "step 65950, training loss Total= 5.025533e-05, training acc total= 100.0%\n",
            "step 66000, training loss Total= 4.8286798e-05, training acc total= 100.0%\n",
            "step 66050, training loss Total= 4.8687118e-05, training acc total= 100.0%\n",
            "step 66100, training loss Total= 4.7033223e-05, training acc total= 100.0%\n",
            "step 66150, training loss Total= 4.48284e-05, training acc total= 100.0%\n",
            "step 66200, training loss Total= 4.286251e-05, training acc total= 100.0%\n",
            "step 66250, training loss Total= 4.4356962e-05, training acc total= 100.0%\n",
            "step 66300, training loss Total= 4.200018e-05, training acc total= 100.0%\n",
            "step 66350, training loss Total= 4.062157e-05, training acc total= 100.0%\n",
            "step 66400, training loss Total= 3.8816353e-05, training acc total= 100.0%\n",
            "step 66450, training loss Total= 3.7918177e-05, training acc total= 100.0%\n",
            "step 66500, training loss Total= 3.8680737e-05, training acc total= 100.0%\n",
            "step 66550, training loss Total= 3.7041875e-05, training acc total= 100.0%\n",
            "step 66600, training loss Total= 3.691972e-05, training acc total= 100.0%\n",
            "step 66650, training loss Total= 3.755492e-05, training acc total= 100.0%\n",
            "step 66700, training loss Total= 3.7449707e-05, training acc total= 100.0%\n",
            "step 66750, training loss Total= 3.500939e-05, training acc total= 100.0%\n",
            "step 66800, training loss Total= 3.382405e-05, training acc total= 100.0%\n",
            "step 66850, training loss Total= 3.259505e-05, training acc total= 100.0%\n",
            "step 66900, training loss Total= 3.226625e-05, training acc total= 100.0%\n",
            "step 66950, training loss Total= 3.205171e-05, training acc total= 100.0%\n",
            "step 67000, training loss Total= 3.3990047e-05, training acc total= 100.0%\n",
            "step 67050, training loss Total= 3.084129e-05, training acc total= 100.0%\n",
            "step 67100, training loss Total= 3.19281e-05, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 67150, training loss Total= 3.0018924e-05, training acc total= 100.0%\n",
            "step 67200, training loss Total= 2.9021878e-05, training acc total= 100.0%\n",
            "step 67250, training loss Total= 2.846245e-05, training acc total= 100.0%\n",
            "step 67300, training loss Total= 2.7864291e-05, training acc total= 100.0%\n",
            "step 67350, training loss Total= 2.8762997e-05, training acc total= 100.0%\n",
            "step 67400, training loss Total= 2.6894899e-05, training acc total= 100.0%\n",
            "step 67450, training loss Total= 2.8075181e-05, training acc total= 100.0%\n",
            "step 67500, training loss Total= 2.7499987e-05, training acc total= 100.0%\n",
            "step 67550, training loss Total= 2.6125514e-05, training acc total= 100.0%\n",
            "step 67600, training loss Total= 2.681083e-05, training acc total= 100.0%\n",
            "step 67650, training loss Total= 2.4851539e-05, training acc total= 100.0%\n",
            "step 67700, training loss Total= 2.6955682e-05, training acc total= 100.0%\n",
            "step 67750, training loss Total= 2.4541901e-05, training acc total= 100.0%\n",
            "step 67800, training loss Total= 2.42585e-05, training acc total= 100.0%\n",
            "step 67850, training loss Total= 2.35275e-05, training acc total= 100.0%\n",
            "step 67900, training loss Total= 2.2674687e-05, training acc total= 100.0%\n",
            "step 67950, training loss Total= 2.227647e-05, training acc total= 100.0%\n",
            "step 68000, training loss Total= 2.2330716e-05, training acc total= 100.0%\n",
            "step 68050, training loss Total= 2.2600923e-05, training acc total= 100.0%\n",
            "step 68100, training loss Total= 2.1347241e-05, training acc total= 100.0%\n",
            "step 68150, training loss Total= 2.2458284e-05, training acc total= 100.0%\n",
            "step 68200, training loss Total= 2.1399732e-05, training acc total= 100.0%\n",
            "step 68250, training loss Total= 2.0300004e-05, training acc total= 100.0%\n",
            "step 68300, training loss Total= 2.209992e-05, training acc total= 100.0%\n",
            "step 68350, training loss Total= 2.0564177e-05, training acc total= 100.0%\n",
            "step 68400, training loss Total= 2.013252e-05, training acc total= 100.0%\n",
            "step 68450, training loss Total= 1.915016e-05, training acc total= 100.0%\n",
            "step 68500, training loss Total= 1.8923321e-05, training acc total= 100.0%\n",
            "step 68550, training loss Total= 1.8829267e-05, training acc total= 100.0%\n",
            "step 68600, training loss Total= 1.8431008e-05, training acc total= 100.0%\n",
            "step 68650, training loss Total= 1.7735136e-05, training acc total= 100.0%\n",
            "step 68700, training loss Total= 1.7430788e-05, training acc total= 100.0%\n",
            "step 68750, training loss Total= 1.7172264e-05, training acc total= 100.0%\n",
            "step 68800, training loss Total= 1.7493638e-05, training acc total= 100.0%\n",
            "step 68850, training loss Total= 1.674003e-05, training acc total= 100.0%\n",
            "step 68900, training loss Total= 1.8245588e-05, training acc total= 100.0%\n",
            "step 68950, training loss Total= 2.2392238e-05, training acc total= 100.0%\n",
            "step 69000, training loss Total= 2.2261973e-05, training acc total= 100.0%\n",
            "step 69050, training loss Total= 3.1827545e-05, training acc total= 100.0%\n",
            "step 69100, training loss Total= 9.792269e-05, training acc total= 100.0%\n",
            "step 69150, training loss Total= 0.0027967477, training acc total= 99.97295141220093%\n",
            "step 69200, training loss Total= 0.014982366, training acc total= 99.83770847320557%\n",
            "step 69250, training loss Total= 0.0007571548, training acc total= 99.98647570610046%\n",
            "step 69300, training loss Total= 0.00026783146, training acc total= 100.0%\n",
            "step 69350, training loss Total= 0.00019297397, training acc total= 100.0%\n",
            "step 69400, training loss Total= 0.00015222939, training acc total= 100.0%\n",
            "step 69450, training loss Total= 0.00012234927, training acc total= 100.0%\n",
            "step 69500, training loss Total= 0.00010641226, training acc total= 100.0%\n",
            "step 69550, training loss Total= 9.746905e-05, training acc total= 100.0%\n",
            "step 69600, training loss Total= 8.239947e-05, training acc total= 100.0%\n",
            "step 69650, training loss Total= 7.602927e-05, training acc total= 100.0%\n",
            "step 69700, training loss Total= 7.3179326e-05, training acc total= 100.0%\n",
            "step 69750, training loss Total= 6.789374e-05, training acc total= 100.0%\n",
            "step 69800, training loss Total= 6.5616e-05, training acc total= 100.0%\n",
            "step 69850, training loss Total= 6.269759e-05, training acc total= 100.0%\n",
            "step 69900, training loss Total= 5.7751713e-05, training acc total= 100.0%\n",
            "step 69950, training loss Total= 5.6137134e-05, training acc total= 100.0%\n",
            "step 70000, training loss Total= 5.3668413e-05, training acc total= 100.0%\n",
            "step 70050, training loss Total= 5.182862e-05, training acc total= 100.0%\n",
            "step 70100, training loss Total= 5.0199265e-05, training acc total= 100.0%\n",
            "step 70150, training loss Total= 4.843878e-05, training acc total= 100.0%\n",
            "step 70200, training loss Total= 4.700143e-05, training acc total= 100.0%\n",
            "step 70250, training loss Total= 4.65533e-05, training acc total= 100.0%\n",
            "step 70300, training loss Total= 4.6600166e-05, training acc total= 100.0%\n",
            "step 70350, training loss Total= 4.377176e-05, training acc total= 100.0%\n",
            "step 70400, training loss Total= 4.253388e-05, training acc total= 100.0%\n",
            "step 70450, training loss Total= 4.1262658e-05, training acc total= 100.0%\n",
            "step 70500, training loss Total= 4.126837e-05, training acc total= 100.0%\n",
            "step 70550, training loss Total= 4.0511797e-05, training acc total= 100.0%\n",
            "step 70600, training loss Total= 3.9947852e-05, training acc total= 100.0%\n",
            "step 70650, training loss Total= 3.8749415e-05, training acc total= 100.0%\n",
            "step 70700, training loss Total= 4.0255403e-05, training acc total= 100.0%\n",
            "step 70750, training loss Total= 3.651126e-05, training acc total= 100.0%\n",
            "step 70800, training loss Total= 3.6286252e-05, training acc total= 100.0%\n",
            "step 70850, training loss Total= 3.567832e-05, training acc total= 100.0%\n",
            "step 70900, training loss Total= 3.3927405e-05, training acc total= 100.0%\n",
            "step 70950, training loss Total= 3.4067652e-05, training acc total= 100.0%\n",
            "step 71000, training loss Total= 3.270031e-05, training acc total= 100.0%\n",
            "step 71050, training loss Total= 3.2771943e-05, training acc total= 100.0%\n",
            "step 71100, training loss Total= 3.149935e-05, training acc total= 100.0%\n",
            "step 71150, training loss Total= 3.136747e-05, training acc total= 100.0%\n",
            "step 71200, training loss Total= 3.0770625e-05, training acc total= 100.0%\n",
            "step 71250, training loss Total= 2.978459e-05, training acc total= 100.0%\n",
            "step 71300, training loss Total= 2.9923249e-05, training acc total= 100.0%\n",
            "step 71350, training loss Total= 2.9356592e-05, training acc total= 100.0%\n",
            "step 71400, training loss Total= 2.807731e-05, training acc total= 100.0%\n",
            "step 71450, training loss Total= 2.8242866e-05, training acc total= 100.0%\n",
            "step 71500, training loss Total= 2.6946036e-05, training acc total= 100.0%\n",
            "step 71550, training loss Total= 2.6516307e-05, training acc total= 100.0%\n",
            "step 71600, training loss Total= 2.845819e-05, training acc total= 100.0%\n",
            "step 71650, training loss Total= 3.061803e-05, training acc total= 100.0%\n",
            "step 71700, training loss Total= 3.1065374e-05, training acc total= 100.0%\n",
            "step 71750, training loss Total= 2.4627288e-05, training acc total= 100.0%\n",
            "step 71800, training loss Total= 2.5362824e-05, training acc total= 100.0%\n",
            "step 71850, training loss Total= 2.5915742e-05, training acc total= 100.0%\n",
            "step 71900, training loss Total= 2.3633656e-05, training acc total= 100.0%\n",
            "step 71950, training loss Total= 2.268683e-05, training acc total= 100.0%\n",
            "step 72000, training loss Total= 2.2565848e-05, training acc total= 100.0%\n",
            "step 72050, training loss Total= 2.2795995e-05, training acc total= 100.0%\n",
            "step 72100, training loss Total= 2.2342027e-05, training acc total= 100.0%\n",
            "step 72150, training loss Total= 2.1619246e-05, training acc total= 100.0%\n",
            "step 72200, training loss Total= 2.1692187e-05, training acc total= 100.0%\n",
            "step 72250, training loss Total= 2.2094879e-05, training acc total= 100.0%\n",
            "step 72300, training loss Total= 2.2543532e-05, training acc total= 100.0%\n",
            "step 72350, training loss Total= 2.361883e-05, training acc total= 100.0%\n",
            "step 72400, training loss Total= 2.4806965e-05, training acc total= 100.0%\n",
            "step 72450, training loss Total= 1.9962645e-05, training acc total= 100.0%\n",
            "step 72500, training loss Total= 1.971689e-05, training acc total= 100.0%\n",
            "step 72550, training loss Total= 2.1954616e-05, training acc total= 100.0%\n",
            "step 72600, training loss Total= 1.9886842e-05, training acc total= 100.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 72650, training loss Total= 1.8741659e-05, training acc total= 100.0%\n",
            "step 72700, training loss Total= 1.8067454e-05, training acc total= 100.0%\n",
            "step 72750, training loss Total= 1.9611332e-05, training acc total= 100.0%\n",
            "step 72800, training loss Total= 1.7597018e-05, training acc total= 100.0%\n",
            "step 72850, training loss Total= 1.6553427e-05, training acc total= 100.0%\n",
            "step 72900, training loss Total= 1.8374385e-05, training acc total= 100.0%\n",
            "step 72950, training loss Total= 2.0708258e-05, training acc total= 100.0%\n",
            "step 73000, training loss Total= 3.0586092e-05, training acc total= 100.0%\n",
            "step 73050, training loss Total= 1.5540822e-05, training acc total= 100.0%\n",
            "step 73100, training loss Total= 1.6361264e-05, training acc total= 100.0%\n",
            "step 73150, training loss Total= 2.6443455e-05, training acc total= 100.0%\n",
            "step 73200, training loss Total= 2.0350955e-05, training acc total= 100.0%\n",
            "step 73250, training loss Total= 8.135466e-05, training acc total= 100.0%\n",
            "step 73300, training loss Total= 0.0076634353, training acc total= 99.87828135490417%\n",
            "step 73350, training loss Total= 0.00071844866, training acc total= 100.0%\n",
            "step 73400, training loss Total= 0.045296825, training acc total= 99.97295141220093%\n",
            "step 73450, training loss Total= 0.002351509, training acc total= 99.98647570610046%\n",
            "step 73500, training loss Total= 0.00042511296, training acc total= 99.98647570610046%\n",
            "step 73550, training loss Total= 0.0005207588, training acc total= 99.98647570610046%\n",
            "step 73600, training loss Total= 0.0004994748, training acc total= 99.98647570610046%\n",
            "step 73650, training loss Total= 0.00048611208, training acc total= 99.98647570610046%\n",
            "step 73700, training loss Total= 9.562894e-05, training acc total= 100.0%\n",
            "step 73750, training loss Total= 8.760218e-05, training acc total= 100.0%\n",
            "step 73800, training loss Total= 6.947849e-05, training acc total= 100.0%\n",
            "step 73850, training loss Total= 6.439952e-05, training acc total= 100.0%\n",
            "step 73900, training loss Total= 5.740257e-05, training acc total= 100.0%\n",
            "step 73950, training loss Total= 5.3232696e-05, training acc total= 100.0%\n",
            "step 74000, training loss Total= 5.1686027e-05, training acc total= 100.0%\n",
            "step 74050, training loss Total= 4.8730464e-05, training acc total= 100.0%\n",
            "step 74100, training loss Total= 4.6645815e-05, training acc total= 100.0%\n",
            "step 74150, training loss Total= 4.4757133e-05, training acc total= 100.0%\n",
            "step 74200, training loss Total= 4.3122698e-05, training acc total= 100.0%\n",
            "step 74250, training loss Total= 4.106632e-05, training acc total= 100.0%\n",
            "step 74300, training loss Total= 4.0282917e-05, training acc total= 100.0%\n",
            "step 74350, training loss Total= 3.843363e-05, training acc total= 100.0%\n",
            "step 74400, training loss Total= 3.709684e-05, training acc total= 100.0%\n",
            "step 74450, training loss Total= 3.5542726e-05, training acc total= 100.0%\n",
            "step 74500, training loss Total= 3.453117e-05, training acc total= 100.0%\n",
            "step 74550, training loss Total= 3.3867367e-05, training acc total= 100.0%\n",
            "step 74600, training loss Total= 3.2900796e-05, training acc total= 100.0%\n",
            "step 74650, training loss Total= 3.178625e-05, training acc total= 100.0%\n",
            "step 74700, training loss Total= 3.1144173e-05, training acc total= 100.0%\n",
            "step 74750, training loss Total= 3.0280622e-05, training acc total= 100.0%\n",
            "step 74800, training loss Total= 2.9527653e-05, training acc total= 100.0%\n",
            "step 74850, training loss Total= 2.8931838e-05, training acc total= 100.0%\n",
            "step 74900, training loss Total= 2.8635419e-05, training acc total= 100.0%\n",
            "step 74950, training loss Total= 2.7757489e-05, training acc total= 100.0%\n",
            "step 75000, training loss Total= 2.7153383e-05, training acc total= 100.0%\n",
            "step 75050, training loss Total= 2.6604785e-05, training acc total= 100.0%\n",
            "step 75100, training loss Total= 2.6265761e-05, training acc total= 100.0%\n",
            "step 75150, training loss Total= 2.5463212e-05, training acc total= 100.0%\n",
            "step 75200, training loss Total= 2.5233736e-05, training acc total= 100.0%\n",
            "step 75250, training loss Total= 2.4301824e-05, training acc total= 100.0%\n",
            "step 75300, training loss Total= 2.369705e-05, training acc total= 100.0%\n",
            "step 75350, training loss Total= 2.3215145e-05, training acc total= 100.0%\n",
            "step 75400, training loss Total= 2.2827575e-05, training acc total= 100.0%\n",
            "step 75450, training loss Total= 2.2699254e-05, training acc total= 100.0%\n",
            "step 75500, training loss Total= 2.181697e-05, training acc total= 100.0%\n",
            "step 75550, training loss Total= 2.1329954e-05, training acc total= 100.0%\n",
            "step 75600, training loss Total= 2.0912566e-05, training acc total= 100.0%\n",
            "step 75650, training loss Total= 2.0785186e-05, training acc total= 100.0%\n",
            "step 75700, training loss Total= 2.0019748e-05, training acc total= 100.0%\n",
            "step 75750, training loss Total= 1.9501638e-05, training acc total= 100.0%\n",
            "step 75800, training loss Total= 1.9158486e-05, training acc total= 100.0%\n",
            "step 75850, training loss Total= 1.8701669e-05, training acc total= 100.0%\n",
            "step 75900, training loss Total= 1.8547387e-05, training acc total= 100.0%\n",
            "step 75950, training loss Total= 1.809788e-05, training acc total= 100.0%\n",
            "step 76000, training loss Total= 1.7914095e-05, training acc total= 100.0%\n",
            "step 76050, training loss Total= 1.7359383e-05, training acc total= 100.0%\n",
            "step 76100, training loss Total= 1.6938586e-05, training acc total= 100.0%\n",
            "step 76150, training loss Total= 1.747449e-05, training acc total= 100.0%\n",
            "step 76200, training loss Total= 1.6369731e-05, training acc total= 100.0%\n",
            "step 76250, training loss Total= 1.6070759e-05, training acc total= 100.0%\n",
            "step 76300, training loss Total= 1.5846257e-05, training acc total= 100.0%\n",
            "step 76350, training loss Total= 1.5778665e-05, training acc total= 100.0%\n",
            "ValidValid acc= 100.0 %\n",
            "ValidTest acc= 100.0 %\n",
            "==================================================\n",
            "W1\n",
            "6\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KnluQPynypaB",
        "colab_type": "code",
        "outputId": "259b3c53-ca19-4361-f38d-936a780488c9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_accuracy), \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from .\\PendigitAdam\n",
            "ValidValid acc= 100.0 %\n",
            "Test acc= 98.19897 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mtzNxx-CypaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXT67klPypaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7izc2t3qypaO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qx_Hd2w2ypaU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMAYWSQwypaZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0-VyixOypaf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Grout76rypah",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning by splitting valid into two sets"
      ]
    },
    {
      "metadata": {
        "id": "ijtoojnCypai",
        "colab_type": "code",
        "outputId": "78d25a18-3cca-4651-a566-81bda5981576",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "hid_neuron = [90]\n",
        "num_steps = 50000\n",
        "batch_size = 200\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "for h in hid_neuron:\n",
        "    num_hidden_neurons=h\n",
        "    learning_rate = 0.001\n",
        "\n",
        "\n",
        "    X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "    Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "    weights = {\n",
        "        'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "        'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "        'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "    }\n",
        "    saver = tf.train.Saver()\n",
        "    W_track = []\n",
        "    ValidAccuracy_Track = []\n",
        "    ValidAccuracy_Test_track = []\n",
        "    def neural_net(x,train = True):\n",
        "        layer_outputs = []\n",
        "        layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "        layer_outputs.append(out_layer)\n",
        "        for loop in range(0,2):        \n",
        "            layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "            layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "            layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "            layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "            \n",
        "            layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "            layer_1 = tf.nn.relu(layer_1)\n",
        "            layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "            layer_2 = tf.nn.relu(layer_2)\n",
        "            out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "      \n",
        "            layer_outputs.append(out_layer)\n",
        "        if train == True:\n",
        "            return layer_outputs\n",
        "        else:\n",
        "            return layer_outputs[0]\n",
        "    for wL1 in range(1,8):\n",
        "      for WL2 in range(1,wL1+1):\n",
        "        for WL3 in range(0,2):\n",
        "        \n",
        "            wLoss1 = wL1\n",
        "            wLoss2 = WL2\n",
        "            wLoss3 = WL3\n",
        "            loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "            loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "            loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "            loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "            train_op = optimizer.minimize(loss)\n",
        "            correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "            ### Initialization and running the model\n",
        "            with tf.Session() as sess:\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "                best_accuracy_valid = 0\n",
        "                for step in range(0, num_steps):\n",
        "                    batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "                    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "                    if step % 1000 == 0:\n",
        "                        train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                        print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                        train_losses.append(train_loss)\n",
        "                        validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                        if step%1000 == 0:\n",
        "                          print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                          print()\n",
        "                          if (validation_accuracy >= best_accuracy_valid):\n",
        "                            best_accuracy_valid = validation_accuracy\n",
        "                            saver.save(sess, './statlog_letter')\n",
        "                            G_W1np, G_b1np, G_W2np, G_b2np, G_W3np, G_b3np = sess.run([G_W1, G_b1, G_W2, G_b2, G_W3, G_b3])\n",
        "                print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "                ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "                this_params = G_W1np, G_b1np, G_W2np, G_b2np, G_W3np, G_b3np\n",
        "                W_track.append(this_params)\n",
        "                # code for checking accuracy of valid_test\n",
        "                validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "                ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "                print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "                print(\"=\"*50)\n",
        "                print(\"W1 = {} ...\".format(wLoss1))\n",
        "                print(\"W2 = {} ...\".format(wLoss2))\n",
        "                print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "                print(\"*\"*50)\n",
        "                print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, training loss= 20.2434, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0024754491, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0041238433, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.003001457, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0017010515, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.002645749, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.00051691965, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 7000, training loss= 0.00043812944, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.00096127915, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.00446604, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.001073645, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0004955716, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 12000, training loss= 0.0008590421, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.002139904, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 14000, training loss= 0.0053695627, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0006863215, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.00030180716, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.0021975613, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00012372267, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.00024088523, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.0003303426, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.00024360228, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.0001721807, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.0009692376, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.00041248425, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 25000, training loss= 0.00016491355, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 26000, training loss= 0.000101649835, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 6.346865e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.0012417015, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 29000, training loss= 0.00085761363, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00015178861, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0013123259, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 32000, training loss= 0.00021439737, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 0.00024247145, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 8.051241e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 35000, training loss= 5.1338306e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 36000, training loss= 0.00016457672, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 6.720987e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 0.00037941142, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 39000, training loss= 8.20682e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 0.00026908494, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 8.204992e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 0.0004845673, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 6.0359547e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 1.8372897e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 2.440435e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 2.893751e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 3.5085694e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 6.800243e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 49000, training loss= 0.00023571475, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 149.04855, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.010343863, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.004676189, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.69999694824219 ...\n",
            "\n",
            "step 3000, training loss= 0.0030050715, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 4000, training loss= 0.011764556, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 5000, training loss= 0.004039208, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0030377824, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0034647528, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.0027936688, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0020853714, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0015847035, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0009961061, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0013994912, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0025359443, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 14000, training loss= 0.0012940139, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.0025381295, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.0012142517, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.0012924918, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.0024953987, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0005641207, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0050481893, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.004901124, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0006931312, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0018173256, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.00042830966, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.0005343972, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.0014995545, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 27000, training loss= 0.000884368, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.00047185968, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 29000, training loss= 0.0006019486, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 0.00081384275, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0005154683, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.001039789, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 33000, training loss= 0.00056193216, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.00076476147, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.00026979952, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 36000, training loss= 0.00030527584, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.00039750495, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 0.0006162395, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 0.00024458754, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 0.0032151793, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 0.00029555647, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 0.00017288401, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 9.826843e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 44000, training loss= 0.00012311703, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.00039320148, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 46000, training loss= 4.0021292e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 47000, training loss= 0.00015833617, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 48000, training loss= 4.1435458e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 0.00037151063, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 10.499638, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 1000, training loss= 0.003650562, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 2000, training loss= 0.0033862756, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0018542027, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 4000, training loss= 0.0012087058, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0026453333, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0018134103, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 7000, training loss= 0.0010514362, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 8000, training loss= 0.0035950853, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.000761618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.00059859885, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 11000, training loss= 0.0005339412, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0010494883, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0004547591, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0009509835, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.00052148424, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00016494351, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.0002627442, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.00010071877, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.00019655125, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.00024941366, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.0012718195, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.00010792107, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 5.7041274e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 4.666681e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 25000, training loss= 8.5091204e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.00018486472, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 27000, training loss= 9.54494e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.00017427764, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00017799967, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 3.3395125e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 6.59385e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.00020004412, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00019477312, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 2.256171e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 4.545414e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 3.1165717e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 0.0001132074, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 2.7514612e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 1.5100901e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.00015942228, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 5.7825306e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 1.5031856e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 5.4262724e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 3.3265584e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 5.0370123e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 3.74134e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 1.5750975e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 2.3018594e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 1.1972097e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 58.89767, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.008770264, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.79999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.003195338, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 3000, training loss= 0.0036001408, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.004096055, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.019991865, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0020318618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0024778037, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0044707204, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 9000, training loss= 0.0031429233, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.039718322, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0072035603, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0015711142, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.017286303, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.00289299, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.0021254648, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 16000, training loss= 0.0017384941, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.0012937281, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.0075602666, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0004320836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.0009464421, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.001858448, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0042230007, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0025516509, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.00070148153, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 25000, training loss= 0.0034939637, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00035754562, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.0014647275, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.0005179871, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 29000, training loss= 0.00079497, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 0.00021081234, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0012831718, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 32000, training loss= 0.00046671546, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 33000, training loss= 0.00034776618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.00012549132, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 35000, training loss= 0.0004159263, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 0.00024954387, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 0.00017527559, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00022402035, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.0004595665, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 6.594966e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.00021534413, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00010942513, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 0.000108276916, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 0.00011858746, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 4.9633487e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 4.846964e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.00018734059, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 5.665112e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 6.0897768e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.5 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 21.42844, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0021991832, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0017308858, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0018788354, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.013065182, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 5000, training loss= 0.0018302519, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0020594473, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.0019248297, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.001392703, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.0008894208, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0006522382, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00060507585, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0020902166, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0005690752, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0017009655, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 15000, training loss= 0.0011199657, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.0003591814, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.0004910029, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00036606155, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.00051384664, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.0002781475, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.014751466, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 22000, training loss= 0.000202569, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00040339114, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.0022406154, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00062223314, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.0007042587, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00034908808, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.0001534164, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00015421386, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00014542723, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0002759746, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.00013182868, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 7.788504e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 4.913864e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 4.1509196e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.037552927, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 37000, training loss= 0.00042504867, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 3.628232e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 3.068117e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 40000, training loss= 0.00020210558, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.00088637066, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00012320241, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 9.496218e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 0.0004520663, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.00027996438, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 8.9809924e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 3.784028e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 0.0011054965, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 0.00014367442, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 71.671844, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.021420827, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "step 2000, training loss= 0.01134806, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.0032583214, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0029981318, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.011372452, training acc= 100.0%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0020544606, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0013872961, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.0027947184, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0011956226, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.0009409195, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.0009841571, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 12000, training loss= 0.0012744867, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0012738243, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 14000, training loss= 0.0012486099, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.001985009, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00093656883, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 17000, training loss= 0.00083958096, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.0012794716, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 19000, training loss= 0.0010956152, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 20000, training loss= 0.00044312023, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.0012099618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.00032121307, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0005826382, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.00042968066, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 25000, training loss= 0.0001994208, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 26000, training loss= 0.0001808427, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 27000, training loss= 0.0004325252, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 28000, training loss= 0.0006057391, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 29000, training loss= 0.0040382775, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 30000, training loss= 0.00041565756, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 0.0015784405, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 32000, training loss= 0.0002599576, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 33000, training loss= 0.00015185068, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.00013828326, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.0004849748, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.00035627288, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 37000, training loss= 0.00014219285, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 38000, training loss= 0.00027071775, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 39000, training loss= 9.198832e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 40000, training loss= 0.00036320864, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 41000, training loss= 0.00012277914, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 42000, training loss= 0.00077405304, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 43000, training loss= 0.0006130556, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 0.00038357844, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 7.7513454e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 3.9944054e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.00019814749, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 0.0045496947, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 7.950188e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.5 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 11.512584, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.006089318, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0019311265, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0016267829, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.001262069, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.00086978386, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0016496022, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 7000, training loss= 0.0012300356, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.00068427145, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.0007323464, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.0004523943, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.001240203, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 12000, training loss= 0.014366321, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.00037335453, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0003180264, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 15000, training loss= 0.0076671937, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.00016476192, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.00011055501, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00022965018, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 19000, training loss= 0.00018137255, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 20000, training loss= 0.00018026466, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 21000, training loss= 0.00015587451, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0013478143, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 9.2497765e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.0001154496, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00012710749, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00048123553, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 9.137354e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0068132887, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 29000, training loss= 3.3915665e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 3.5318335e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 7.860559e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 5.185591e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 3.440084e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 2.500603e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.0001641755, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 2.5962754e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 5.7387246e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 3.7091086e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 4.2529922e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 40000, training loss= 1.6622054e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 2.529887e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 1.8396437e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 43000, training loss= 4.1506377e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 3.3299715e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 45000, training loss= 1.2340477e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 46000, training loss= 1.5302012e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 47000, training loss= 6.9221414e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 48000, training loss= 4.0983214e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 0.00012723455, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 51.707924, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.030087689, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 2000, training loss= 0.0094769085, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0036935552, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.05491823, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0022676478, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 6000, training loss= 0.0050329803, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.010900139, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0049394453, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.009813216, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 10000, training loss= 0.03837064, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0012644659, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 12000, training loss= 0.009174963, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 13000, training loss= 0.0014590126, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.0017132895, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.0016144142, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00060665066, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00070781156, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.0006467187, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.0011572896, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 20000, training loss= 0.0020387236, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.002438098, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.000535432, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0003098706, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.00050956366, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00017348715, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.0003819711, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00027938155, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.0021735986, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.0016215375, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 30000, training loss= 0.00015425925, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.000223267, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.0001394973, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00040648348, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 34000, training loss= 0.00019598899, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 0.00013971067, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.0003182068, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.00024528123, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00022359121, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 0.10625382, training acc= 99.00000095367432%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 40000, training loss= 9.718656e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 8.0491576e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 5.6458568e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 0.00015573438, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 0.00017158534, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 5.547612e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 4.147118e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 0.0001921168, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 48000, training loss= 5.8197482e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 0.00011997543, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 9.618008, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0042881896, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 2000, training loss= 0.0018863336, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.001717976, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0023889889, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 5000, training loss= 0.001972996, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.004120578, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0013956454, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0006380301, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0011761382, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 10000, training loss= 0.0002666266, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.00063294324, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.07056131, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0044758124, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0070541534, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00039029442, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.0006499969, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.001017647, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.00016879318, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.002280275, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 9.329289e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00036684825, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00023859742, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.0043747677, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 24000, training loss= 3.859288e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 25000, training loss= 0.00014232796, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 26000, training loss= 0.00011406466, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 2.9154422e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00013535934, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 8.9189525e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 30000, training loss= 0.00010178378, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 31000, training loss= 0.002684497, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00017694636, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00014051862, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 9.6195814e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 7.697974e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 0.00013262453, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 6.4603366e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 5.4650573e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 3.5260226e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 3.265854e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 5.1591993e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 8.0022335e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 0.00013271079, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 3.631986e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 45000, training loss= 0.00010959827, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 46000, training loss= 6.708881e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 47000, training loss= 8.119642e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 48000, training loss= 2.1238193e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 0.00017496115, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 71.16664, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0060567986, training acc= 100.0%\n",
            "Validation Accuracy valid 98.69999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0037480264, training acc= 100.0%\n",
            "Validation Accuracy valid 98.79999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.003172922, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0043241023, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.14775147, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.0018021713, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 7000, training loss= 0.0053680046, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.002031526, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 9000, training loss= 0.014134571, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.001330655, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0014861774, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 12000, training loss= 0.0016403347, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.0019640564, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.0014196576, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.0010969319, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.00054816133, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.001961536, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.0007636896, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.00091907306, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.000695679, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.00037799624, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 22000, training loss= 0.00051490846, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 0.0019724267, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.00027573726, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.000323095, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00034291454, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 27000, training loss= 0.00034780143, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 28000, training loss= 0.00015771105, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.0003057202, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 0.00019422999, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 0.00012586685, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 32000, training loss= 0.0002520106, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 33000, training loss= 0.00036099204, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.00019712228, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.00031660646, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 36000, training loss= 0.00044007367, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 37000, training loss= 5.8446516e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 38000, training loss= 0.00017248068, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 0.00047369883, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 7.3414e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 41000, training loss= 0.00013546919, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00032686582, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 43000, training loss= 0.000103511, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 0.00039237086, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.00010310258, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 5.825802e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.00015974251, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 7.156616e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 9.2334085e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 20.049667, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0032452298, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.0052062757, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0020341275, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0026537627, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.01073655, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.023268532, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.0010602119, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.00093056215, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0009896862, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.00037785893, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.00038510136, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0007721449, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0005353313, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.064608805, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00074232736, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.00023155464, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.0009289998, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.000426079, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0004293166, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.0005178228, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00026455318, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00026500793, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00036256656, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.0004990782, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00012749512, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00097407645, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 7.451448e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.0006864522, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 29000, training loss= 9.470442e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00094051077, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 0.00030547543, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00011614765, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 8.924445e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 9.6669566e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.00078652916, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 9.899617e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 8.19902e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 0.00033834018, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.0010308952, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 7.118953e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 4.661461e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 0.00013065379, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 0.0006306764, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 9.55342e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 4.096387e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 3.5470857e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 1.6168447e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 0.00021010861, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 0.00016517885, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 56.63243, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.014976212, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.0046061925, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0038204836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.004549793, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0023152658, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 6000, training loss= 0.0037482753, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0034612163, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0020292185, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 9000, training loss= 0.0036579056, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 10000, training loss= 0.0023109277, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 11000, training loss= 0.0013420813, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 12000, training loss= 0.001982426, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0013716492, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.014786268, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.0040869964, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.0010974756, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 17000, training loss= 0.0029149351, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.0011687896, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.001166231, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0011657062, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.01185726, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.0010238374, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0008310213, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.00048691814, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 25000, training loss= 0.0004419356, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00063561957, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00047297365, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0009941761, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.00030567922, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 0.00081588817, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 31000, training loss= 0.00062212406, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.0001263163, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 0.00076408236, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.00027053244, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00013645263, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 8.071986e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 0.004328913, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00023406425, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.00038451265, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.00012926933, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.000724325, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 42000, training loss= 0.00034561384, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 0.00013165345, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 8.023335e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 5.559057e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 0.0014989359, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 47000, training loss= 0.00029812846, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 48000, training loss= 6.16578e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 0.00013927942, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 8.275024, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0059154504, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.001822777, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.010647512, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.0012529113, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0013636015, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.009052009, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 7000, training loss= 0.00051708124, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0010264253, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0007922223, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.00025801957, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 11000, training loss= 0.0008681682, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 12000, training loss= 0.00063121173, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0002515627, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.00015011834, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00038608303, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00026955502, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00019117356, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 18000, training loss= 0.00016135743, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 9.796445e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00010632863, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.0001774754, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.00015497318, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.00018886999, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 24000, training loss= 4.593594e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 25000, training loss= 4.0942115e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 26000, training loss= 5.0603467e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 27000, training loss= 8.3061524e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 4.997998e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 2.2656603e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 5.8227168e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 2.6786822e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 4.4265875e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 3.32171e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 1.8608169e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00010247643, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 1.890672e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 2.600565e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 0.00028125686, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 39000, training loss= 2.305105e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 0.0002096709, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 2.7305246e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 3.9439677e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 1.02599215e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 1.0255187e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 0.00015059253, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 0.00013411739, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 1.4014127e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 8.821473e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 9.1263855e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 29.128963, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0047366954, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.005074762, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.007270091, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0072693527, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0036906507, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.0024473253, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0022327506, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 8000, training loss= 0.0023864184, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.00213109, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0035149734, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.002363702, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0018370836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.001312213, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0029649418, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0012156611, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00056842866, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00049547676, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 18000, training loss= 0.0015385053, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 19000, training loss= 0.00053259794, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.014976821, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00032223723, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0005639815, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.00037670758, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.0005550191, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00023639853, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00023307544, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.0001442952, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.0003682774, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00060152425, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 0.00036842003, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.00025780447, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00012518745, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00022015805, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.0013058314, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00013704039, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.00025273178, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00016282474, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.000116928444, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 4.2199616e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.0006794017, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 41000, training loss= 0.00014186738, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 6.186664e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 8.932348e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 0.00018341042, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.0012287842, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 46000, training loss= 0.00012303935, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 4.401286e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 7.878024e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 0.00016859225, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 12.610281, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0021390743, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.003340865, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0061478983, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.008189675, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0022780164, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.016268495, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 7000, training loss= 0.0015197572, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.00062002253, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.00045125798, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.0013118068, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00034420728, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.00488943, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0004464043, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 14000, training loss= 0.00017209396, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0003812817, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00014451683, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00013259483, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.00021660418, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.00011054542, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00026730576, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 8.524977e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.00012568079, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 9.8762874e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.0010961499, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00014250202, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 26000, training loss= 0.00013178316, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00010107359, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 8.574992e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00014459367, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 9.388132e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 4.1059473e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 4.8080106e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00012450773, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.00017895809, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 3.0934207e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 3.441635e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00028517665, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 3.995671e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 2.627285e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 2.1592758e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 41000, training loss= 0.00011991904, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 42000, training loss= 0.000111786525, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 1.2768762e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 7.0516755e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 0.038912058, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 46000, training loss= 3.7420705e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 47000, training loss= 0.00012395941, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 5.618399e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 9.96024e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 32.595917, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0073636947, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.005986327, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.0022531801, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.004449566, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 5000, training loss= 0.015850859, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 6000, training loss= 0.0020611747, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.003077631, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0019397602, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0010709803, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 10000, training loss= 0.0023678313, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.0029704378, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 12000, training loss= 0.0023013474, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0014819761, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0027318946, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.0008330866, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00040074225, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.0007913425, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.0009702176, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.00051759934, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.0005577578, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.0009940874, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.00033234025, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00062644604, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.00046970477, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00045465215, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 26000, training loss= 0.0022938226, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00030606324, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0015131361, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.00037990566, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.0006489032, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 0.00025101306, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 0.0009423291, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00031090106, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 0.0002475758, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00033436884, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.00051909266, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 0.00037334242, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 38000, training loss= 0.0001724359, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 39000, training loss= 0.0001297603, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 40000, training loss= 0.0002854411, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 0.0001985328, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 6.35116e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 0.00012396561, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 0.0016887251, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 0.00011244668, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 46000, training loss= 6.264709e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 47000, training loss= 0.000214733, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 0.00025083113, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 0.00012304069, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 13.27437, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.010559689, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0046429792, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0028554106, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0025788005, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.004367916, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.0008182851, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.0010967818, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0012102198, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.0011348869, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0025059264, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 11000, training loss= 0.0007405797, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0004362402, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0005774996, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.0003704412, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0005406204, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.0008636459, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.0025465004, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.0019525113, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.00023316238, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00032443352, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 21000, training loss= 0.00036252916, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.00015894527, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 8.462362e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 8.692312e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 5.231625e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.093060106, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.00025862677, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 28000, training loss= 0.00014554866, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 29000, training loss= 0.00011481044, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00020541582, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 4.630021e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 8.5718086e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 33000, training loss= 0.000672849, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 5.292667e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 2.5906736e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 8.546246e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 37000, training loss= 6.864618e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 7.161076e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.000106830965, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 3.589517e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 2.8093584e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 3.972451e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 5.4457636e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 4.6829085e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 3.0351026e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 1.8088715e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 6.182116e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 48000, training loss= 3.193467e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 5.3393083e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.5 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 63.044067, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.010228889, training acc= 100.0%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.00254766, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.0035227127, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0037736183, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.002088179, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.002031562, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0028585966, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.001951899, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.061290484, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.002963292, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0012452471, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0010583817, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.000956153, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0016913384, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0006988498, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.0023615714, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 17000, training loss= 0.0006697911, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.0010123708, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.0008379339, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0005564597, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.0007173491, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0014142615, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0003408806, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.00069313083, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.0002767393, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.00030085494, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 9.53455e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.00034679682, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.00019566537, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 0.00011446773, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 0.00010028798, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 32000, training loss= 0.00024568834, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 0.00023882078, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 8.580778e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.0008307891, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.000287311, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 7.786422e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 0.0001844439, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 6.679156e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 0.00012098465, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 41000, training loss= 0.00046661313, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 8.6050604e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 43000, training loss= 5.1952713e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 9.2388036e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 9.792174e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 9.853186e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 5.3021424e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 0.00076045864, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 3.4489956e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 9.507825, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0019308588, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0016413061, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0006582144, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 4000, training loss= 0.0008856742, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0032895203, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0066837356, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0009052595, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0015025428, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 9000, training loss= 0.07396565, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0006716667, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0010392758, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0005035933, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.003013663, training acc= 100.0%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0005852843, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0005460219, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.00034056522, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.00050066324, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.00064832746, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.00015212856, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.00035261989, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00013901474, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.00045341693, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.0005233129, training acc= 100.0%\n",
            "Validation Accuracy valid 98.79999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.0001452381, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 25000, training loss= 0.00046000726, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00014069898, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00021795722, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 9.680143e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 9.824317e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00014755083, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 4.299806e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 8.396697e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 33000, training loss= 0.0017053477, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.000109719964, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 2.9985378e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.00014778117, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 6.338338e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00011966854, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 7.357224e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 6.775737e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 3.7739497e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00012263237, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 8.208302e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 0.00011140212, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 3.4352033e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 0.00018976002, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 8.343434e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 3.5108253e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 2.7330836e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 38.83724, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 1000, training loss= 0.012587804, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.003333205, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.0017840109, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.01240716, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 5000, training loss= 0.006147576, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.018037569, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.002358365, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.001379618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.0032385464, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 10000, training loss= 0.0010561919, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 11000, training loss= 0.0028408582, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 12000, training loss= 0.000888956, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0010994257, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 14000, training loss= 0.00045710662, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.0006388137, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00050928275, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.0005803866, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.00088818796, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.00078987697, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 20000, training loss= 0.00035328596, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 21000, training loss= 0.0007057481, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 22000, training loss= 0.00055368093, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.00064079196, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 24000, training loss= 0.0003054998, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.0002887608, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00019502135, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00045679256, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00040315147, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.00034841598, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00035397607, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 0.0016410254, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 8.860038e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00055351376, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.0004576232, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.0005222509, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.00010047827, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 0.00011569739, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 0.0003262729, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 5.0799474e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.00017785741, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 6.5201064e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 0.0001617459, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 43000, training loss= 4.7350655e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 44000, training loss= 0.00061843666, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 45000, training loss= 8.392309e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 7.786832e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 4.242342e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 3.1765103e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 0.0003146089, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 6.4076166, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.021422757, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.010118667, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0020749802, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 4000, training loss= 0.00085359276, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0021606607, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.00078698085, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.00049102097, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 8000, training loss= 0.00019618712, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.00051503634, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.00047999428, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0016319988, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.00018271785, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 13000, training loss= 0.00018846954, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0013536316, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.00036179868, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 16000, training loss= 0.00010941712, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 5.181972e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.00025442668, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.00011143562, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 9.3767e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 3.1757627e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00076170213, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 8.130384e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 2.0473351e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 3.6362228e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 2.7182168e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 3.276929e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00084378605, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 29000, training loss= 6.5065375e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 1.6703669e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 3.7738835e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 3.9768296e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 1.2538617e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.0012422602, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 35000, training loss= 9.703192e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 1.3691386e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 2.5083391e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 9.439698e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 8.770936e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 1.4034197e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 3.9470444e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 1.0059181e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 1.8031678e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 1.5790467e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 1.1863407e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 6.764629e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 47000, training loss= 1.2997974e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 1.0805519e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 1.6840793e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 30.082428, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0052161943, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0027760433, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.002936202, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.004776509, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.0021286502, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.002881435, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.007640653, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0019425994, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.0021784953, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.0013619832, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0015061211, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0008105658, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.001611648, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.00080012926, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.0011269797, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.001687932, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.000850417, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.0003174828, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.0010035096, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.00035620373, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.00031142408, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.00045951785, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00045707443, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 9.408304e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00028449047, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.000462546, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 27000, training loss= 0.00041233512, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0004605786, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.00025343758, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.0052153463, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.0008530513, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.0001769707, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.0007849215, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.00023461721, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 0.0002746144, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.0001819413, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 0.00012544164, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 7.6836695e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 3.677769e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 0.000115879906, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 0.00012849153, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 3.1258263e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 0.0003617136, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 44000, training loss= 5.464114e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 2.8862069e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 2.8835764e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 8.7952554e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 0.00024520955, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 3.5395875e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 5.6652923, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 1000, training loss= 0.0036082498, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0025583091, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 3000, training loss= 0.0019259421, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.002728432, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0018031333, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0009232508, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.00081430865, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0008636085, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.006204424, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.00074652827, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00047485743, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.00022933078, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0002488872, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0009900895, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00074940996, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00013825907, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00015202498, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00037291818, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0003545267, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.00017961163, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 7.489773e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 6.044326e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.0001529459, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.00036845083, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 9.8111894e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 26000, training loss= 5.6641336e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00012573387, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 28000, training loss= 5.579323e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 29000, training loss= 0.00093884836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 0.0010998556, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 6.306179e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 4.0101833e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 0.0006393667, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 34000, training loss= 9.8796e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.00025320923, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 36000, training loss= 9.400291e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 0.00032116583, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 1.1349438e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 3.3504813e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 4.4694138e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 41000, training loss= 4.7604855e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 42000, training loss= 3.9927716e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 43000, training loss= 3.505366e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 44000, training loss= 1.0630578e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 45000, training loss= 6.6527966e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 46000, training loss= 0.00019276254, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 47000, training loss= 3.0309788e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 9.3405815e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 1.1239937e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 37.600964, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.026258934, training acc= 99.00000095367432%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0076454086, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0039421, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.003402963, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0019812023, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0023302843, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 7000, training loss= 0.0026419107, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0010185448, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.0017424468, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.009202448, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.004343443, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0009017056, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0018768559, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.00057336583, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.00052932434, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00055847084, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.0010224693, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00041768726, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.0015569967, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0006910205, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.000509842, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0011880407, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 23000, training loss= 0.00034125953, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.00030982026, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00077013514, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 26000, training loss= 0.0003321302, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00023257587, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0002378411, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 29000, training loss= 9.80025e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 0.00018980737, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 0.001026528, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 32000, training loss= 0.0001716762, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 0.000112550515, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.00054500997, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00029475472, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.00014105656, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.00034584536, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 6.0560444e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 1.5802758e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 0.0002000079, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 9.2003065e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 0.0001043772, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 0.0012124261, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 5.090795e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 4.749513e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 1.8922528e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.0011394841, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 48000, training loss= 6.672964e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 5.11354e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 13.132348, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0024030036, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0033785487, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0038728025, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0013816827, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.001069842, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0014515937, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0021605096, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0010938018, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 9000, training loss= 0.0010260334, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.035135478, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.0005285523, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0004078444, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 13000, training loss= 0.00060028216, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.00024545265, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00020520041, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.00026453644, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00026291976, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00026840268, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0007928621, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.0004464112, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00031060202, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.00021992116, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.0017705779, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 24000, training loss= 0.0009719627, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00019507045, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 6.135578e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00034469232, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 28000, training loss= 0.00011460823, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 29000, training loss= 0.00021575914, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00018320339, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 8.34047e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 32000, training loss= 4.4819466e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 6.4849126e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.0005673587, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.00011187396, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 6.7711284e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.000253465, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 38000, training loss= 0.00013107322, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.000112558, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 0.00027600507, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 6.209789e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 4.564514e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 3.784025e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 2.7302478e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.00011287954, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 0.00034645267, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 4.884199e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 3.397109e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 2.307826e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 32.952156, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 1000, training loss= 0.03414801, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0037098594, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0020449085, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0023117834, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0028106053, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.0059856353, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.001708018, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0024788578, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.049049955, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0011820105, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.053841677, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 12000, training loss= 0.0004900281, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 13000, training loss= 0.005161817, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.0005993937, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.015531544, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.0006219822, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00082908914, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.0012406146, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.00066909683, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0034887174, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.0005695187, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0003418386, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.0027786049, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 24000, training loss= 0.00031386924, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.005817442, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.00013549722, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.00035498705, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.0002122404, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.0005602479, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00026403173, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.00040934782, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.0003984097, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 0.000156715, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.00048379062, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.0011115569, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.00031460673, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 0.0002535291, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.0006909179, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 0.00043407787, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.0007445575, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.0001259616, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 3.4837365e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 0.0001238861, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 6.99371e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 3.5128654e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 0.00013268912, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 47000, training loss= 0.00010078253, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 48000, training loss= 0.000120264514, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 8.3613566e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 14.30188, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0030782947, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.023418231, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.001045348, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.0040695444, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0026445484, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.005225024, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0007813385, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 8000, training loss= 0.001280639, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.00035023916, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.000817372, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.0005680907, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0034986334, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.014628326, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0004235688, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00035335697, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.0004218418, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.0001667727, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.00038063334, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.0002183207, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 9.150444e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.0003318225, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00015771505, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0001783453, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 9.593997e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 25000, training loss= 0.00016850852, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 8.1153616e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00015795314, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 3.5949128e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 5.583471e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00013492258, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 8.398566e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00033843378, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 0.00018547305, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.0009888969, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 35000, training loss= 9.73743e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 36000, training loss= 5.252248e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 4.982618e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 3.5330107e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.00029533115, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 4.1302923e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 2.4171322e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 42000, training loss= 6.412391e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 1.6140528e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 5.3438755e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.000111130415, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 46000, training loss= 8.163604e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 47000, training loss= 0.000114288996, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 2.489838e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 3.1958105e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.5 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 38.8368, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0040194006, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.007061059, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.004413146, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.0057931705, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.002697408, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.048155226, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0017817313, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0013632282, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0016141807, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.0021278397, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 11000, training loss= 0.0009017735, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0016697169, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.0011481332, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0004481497, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00078187464, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00045762723, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00060248346, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.0005643151, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.008297025, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.00043694564, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.0002892287, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.00035822502, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0010360557, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.00033961746, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00039865408, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.00051563745, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.0009970003, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.00018032444, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00033644805, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.0006105715, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0001433725, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 32000, training loss= 0.001451461, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 33000, training loss= 0.0001677738, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 0.00017628064, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 9.98624e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.0001305565, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.000273555, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 0.00022538744, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 39000, training loss= 8.064531e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 0.0011045872, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 6.0954957e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 4.5124307e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 3.532459e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 0.00022406364, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 45000, training loss= 6.487285e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 46000, training loss= 5.8103655e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 47000, training loss= 6.127643e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 4.121214e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 7.821963e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 18.208961, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0047779144, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0017634173, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.002336667, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 4000, training loss= 0.0029819852, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 5000, training loss= 0.0023874908, training acc= 100.0%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.00083458045, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 7000, training loss= 0.0010547242, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.0009609187, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.001183954, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0004756771, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00024118907, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 12000, training loss= 0.0005947742, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.00041755784, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 14000, training loss= 0.0042651165, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.00045326326, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.00030984703, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00033677756, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.0010414814, training acc= 100.0%\n",
            "Validation Accuracy valid 98.79999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.00018736561, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.00017471034, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.0009192062, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0011192331, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.00022783205, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.00012341818, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 25000, training loss= 0.0002444879, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 9.743554e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 27000, training loss= 0.00061023195, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.0017598469, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 9.556001e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00012805455, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.00024205449, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 4.8555095e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 8.254738e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 34000, training loss= 0.00014746659, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 6.308896e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 36000, training loss= 0.0005483812, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 37000, training loss= 6.710257e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00015370014, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.00016680353, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 3.6923728e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 0.0001258013, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 2.5900697e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 3.2741707e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 44000, training loss= 2.1017586e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 45000, training loss= 2.7460539e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 5.713099e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 5.8757025e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 48000, training loss= 1.0365737e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 2.626365e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.5 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 34.16212, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.01850878, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0036523563, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0028926805, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0026913434, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0034459983, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0022552975, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 7000, training loss= 0.0036121001, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0006643143, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 9000, training loss= 0.0011554447, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.0015685126, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 11000, training loss= 0.001617632, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 12000, training loss= 0.000845958, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0015074689, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.00199629, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.0005893322, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 16000, training loss= 0.005340632, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00038562357, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.0007175102, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.0003831334, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 20000, training loss= 0.00037335753, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.00049580575, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 22000, training loss= 0.0005017034, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.0029018242, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.00019898487, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 25000, training loss= 0.0001699427, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00023767419, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.0013760125, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00012632941, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 29000, training loss= 0.00021957334, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 0.005668556, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 0.00039387046, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.00011518448, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.000320704, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 0.0002520346, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 0.0007485019, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.00021020781, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.0001831468, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 5.0790462e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 39000, training loss= 3.5791192e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.000101144, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 6.956678e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 7.006297e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 6.05525e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 5.570723e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 1.989932e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 1.2402472e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 47000, training loss= 0.0005953907, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 0.00012934257, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 49000, training loss= 2.4843133e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 4.7912774, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0059136553, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.007774615, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0028803188, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.00053331873, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.00092435844, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.0026847017, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.0006863415, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.00027472165, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0002854466, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0002098544, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00037432945, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.00013548645, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.00012052521, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.00015432842, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 4.2350723e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 8.0886646e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00012134104, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 6.775329e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 8.185309e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 20000, training loss= 0.00013728764, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.00012420632, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 2.9567733e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00025292483, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 4.909536e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 4.1460753e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 1.4339816e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 7.121918e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 4.821274e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 4.82151e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 2.1433383e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 1.8161176e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 2.5964322e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 2.2630653e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 1.4714041e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 1.62369e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 7.282867e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 0.00013681696, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 2.9866214e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 1.868137e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 7.531654e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 1.3400148e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 1.4328592e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 3.7703914e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 2.3052278e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 1.4375306e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 5.1903303e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 4.7019525e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 7.714875e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 4.4683624e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 56.464214, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.077597804, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0105547, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0040413947, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0039712396, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.0057130666, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.004015265, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0028943196, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0016943846, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.0020572573, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0026862225, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0017299155, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.00080684444, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0027324741, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0025952898, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 15000, training loss= 0.00083389063, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.003349729, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 17000, training loss= 0.00078058307, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 18000, training loss= 0.0002800917, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.0010786467, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.0016490682, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.0007615945, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00040515195, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0017169692, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.0011962072, training acc= 100.0%\n",
            "Validation Accuracy valid 99.80000305175781 ...\n",
            "\n",
            "step 25000, training loss= 0.00035288057, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00015199136, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 27000, training loss= 0.0020715992, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00025662075, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00032044225, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00037011405, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 0.00026575322, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 0.00026872492, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00016763696, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.00025890075, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 0.0007287662, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.0001742258, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 0.00020172786, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 38000, training loss= 5.9860213e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 39000, training loss= 0.00012059439, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.00030158006, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 4.7372334e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 4.19254e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 0.00014534009, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 7.9342426e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.00010221224, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 0.00046433444, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.00025278734, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 0.00044634892, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 0.00076139974, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.8 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 12.1945915, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0049960087, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0030280775, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0035493497, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0026950745, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 5000, training loss= 0.0020745222, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.001181868, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0013887803, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.00060266716, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.00022965814, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.00049013953, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 11000, training loss= 0.0013366679, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.00019067113, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.00019804972, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.00023053806, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00016904366, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00024256883, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 7.444343e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.00030737568, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.00017449926, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00066216674, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00012339561, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0003364932, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 8.791314e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 4.8465234e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00013602569, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00014926713, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 7.286134e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 5.7925008e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00014593086, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 5.7925514e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 4.3057415e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.0001887825, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00015633478, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 2.4336674e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.00010913078, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 0.00011909929, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 4.8499216e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 3.0091305e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 2.675217e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 1.6926111e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 5.4518532e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 4.30109e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 7.554642e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 6.4461133e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 3.052627e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 2.4635e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.00011224401, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 0.00011219431, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 1.3801289e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 36.117256, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.009518863, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0026314736, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0031735837, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0021464278, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0023544016, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0016656244, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0013580066, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.001111466, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0008288436, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0014852254, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.00078625215, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0011403474, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.010269701, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.00037165443, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.005677253, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.0008417175, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 17000, training loss= 0.0002872849, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 18000, training loss= 0.0006988992, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.00048451545, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00041185904, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00037042928, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00019819154, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.00018630127, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.0003256669, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.0006369646, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00049887615, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00046335993, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.00080294604, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.00022716248, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 0.00029463493, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.00016121342, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 9.458072e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00013325612, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.0001377618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00016486102, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.00035077517, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00027586904, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 9.7870754e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 6.133633e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 3.775173e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 2.5030946e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00040274797, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 5.347818e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 6.059228e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 4.164049e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 5.916979e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 5.2699732e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 4.39233e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 0.00023712376, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 14.012971, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.021749387, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.0028683865, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.002802153, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0021318856, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.0010086917, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.0007495386, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0009007523, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.001412307, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.0011999921, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.00043630807, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.004256629, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.00051472115, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.00024592265, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 14000, training loss= 0.0008844104, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.00027794382, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.00024527204, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 17000, training loss= 0.0002765759, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.00028260218, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.0001687608, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.0001264958, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.00016432496, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0002346858, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.00013573396, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.00010374967, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 25000, training loss= 9.7870165e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.00012467228, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 27000, training loss= 0.0007665289, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 8.833042e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00049726426, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00015555717, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 8.1229584e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 8.09432e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00031099026, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 6.736973e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 9.594141e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 3.1407406e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.0001339377, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 6.706185e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 0.00027409184, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 2.1636175e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.00010184563, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 5.269784e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 2.1294438e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 2.144398e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 3.5427558e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 6.190682e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 2.8217963e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 1.0237192e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 3.90671e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 33.743187, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.022301158, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0016990186, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0025723567, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.0023604934, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.017851753, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0022881955, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.00379042, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.004978695, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0022578728, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.0013313851, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0009179837, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 12000, training loss= 0.00059858424, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0006937408, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0008314945, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.0004630135, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.00082822534, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.00083222165, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.0026978145, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.00049588195, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 20000, training loss= 0.0010778924, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.00024803754, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00027836135, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.00040696227, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.0002998849, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00034053504, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.00012702585, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00036480793, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.00022754306, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00016913655, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 8.108675e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0001102151, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 7.769582e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 0.0061769094, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.00020644595, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 0.000615135, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 36000, training loss= 6.580342e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 37000, training loss= 0.0002512864, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 3.903203e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 0.00012461792, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 8.00378e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 0.00022067933, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 42000, training loss= 8.366009e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 3.458335e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 2.3008037e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 2.2202415e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 5.094611e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.0002294445, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 2.5309686e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 8.7944565e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 19.113325, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.005861318, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 2000, training loss= 0.002171359, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0018479036, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0015950401, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0034523346, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.002280575, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0021152196, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 8000, training loss= 0.0012436481, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.0008239965, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0012631081, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0004587463, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0003449291, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.00031384124, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0009430192, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00066787715, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 16000, training loss= 0.00023448763, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.0015271426, training acc= 100.0%\n",
            "Validation Accuracy valid 98.69999694824219 ...\n",
            "\n",
            "step 18000, training loss= 0.00019542899, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0014627184, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.0007571327, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.0007824384, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0005860391, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 9.1680944e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.0002530536, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 25000, training loss= 6.170503e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 7.38128e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.0001764957, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0005251618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 3.452355e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 9.157751e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.00029429674, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 8.281565e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.000108290515, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 0.00027111682, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 5.2740914e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.00017953492, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 4.125092e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 0.00016293305, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 3.569669e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 2.6278385e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 2.4395049e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 2.6962307e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 0.00041150177, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 2.3501962e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 4.4317218e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 6.0574293e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 2.9066552e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 0.0008099299, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 3.9051618e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 31.377977, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0049078166, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.014044112, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0020195553, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.00236322, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0023786689, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0027832487, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0021612667, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0019158395, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0032049296, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0018812991, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.000908276, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.00075052556, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.001701956, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0031825071, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00074411044, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.005765909, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00029129285, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.0007177514, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.0001570115, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.009836853, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 21000, training loss= 0.0007600042, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.010707349, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 0.0006074292, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.00014554261, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 25000, training loss= 0.00039809733, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.0002279864, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.0003868522, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.0002272988, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00021788667, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 6.9432564e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.000177922, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 8.103492e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 0.00045572908, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.00031740847, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00029113548, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.0001429372, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00057074503, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00092792284, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 0.00013158718, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 40000, training loss= 0.00016770128, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 6.4200896e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 0.00010767054, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 8.071576e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 0.0013796027, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 45000, training loss= 8.503135e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 46000, training loss= 6.7807465e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 4.8834085e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 48000, training loss= 1.9528505e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 9.8268705e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.39879608154297 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 13.81223, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0035545118, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.0050006323, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.0015054945, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0018750513, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0013766714, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0009822922, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 7000, training loss= 0.0006727733, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0016881174, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.002631492, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0017191714, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0014207972, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.016721621, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0003596248, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0012396879, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 15000, training loss= 0.0003737724, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00014716036, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00029239437, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.0028010083, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.00036437972, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.00052971917, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 8.148777e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 22000, training loss= 0.00038176298, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 0.0004505256, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 24000, training loss= 0.0006289105, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.0001477607, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00015567034, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00020647117, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 4.453093e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 8.578203e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.002748245, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 31000, training loss= 0.00013489653, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00016851527, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 33000, training loss= 0.00011627802, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 34000, training loss= 0.0001079475, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 35000, training loss= 0.0004939232, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 6.938304e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 37000, training loss= 9.332828e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 3.0591655e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 39000, training loss= 0.00012845697, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 3.286683e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.00012211018, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 4.974124e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 0.0010268592, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 44000, training loss= 6.482829e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 4.9414695e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 1.684508e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 1.6421565e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 3.9802828e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 4.700542e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 37.033875, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 1000, training loss= 0.008211044, training acc= 100.0%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.0017336013, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.0022376194, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 4000, training loss= 0.0014062847, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.004001706, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.0017860455, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.014292454, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.0018419127, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.0015713506, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0030831103, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0018812622, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0009056585, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0010197688, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0011860643, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00068055745, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00041016386, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00032806338, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.00085419, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0010804256, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0005557831, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.0003227839, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.0005809764, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 0.00033076282, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.0001838241, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 25000, training loss= 0.00024422357, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.0006307573, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00082597334, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.014984084, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.000382212, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.0003310309, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 0.0003273672, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 32000, training loss= 9.617749e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00011887959, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 34000, training loss= 0.00011244009, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 35000, training loss= 0.0009048615, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 36000, training loss= 0.00013296296, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 0.00011555894, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 0.0006455334, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 6.1351646e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 5.4248696e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 41000, training loss= 0.00027455806, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00013770758, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 43000, training loss= 0.00010066149, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 4.3226828e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 2.7549351e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 4.6740777e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 0.00020568044, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 0.00017737062, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 9.4004004e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 17.912884, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 1000, training loss= 0.0047752946, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.003924487, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0023865388, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0030839527, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0015534399, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.001647205, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.0011697477, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0005665363, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0018288608, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0015981395, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0034935332, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.00055595627, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.00090095773, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.00029428076, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00082572864, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.0003376371, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00061129767, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.007034912, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.00017053966, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00033884106, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00016772682, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0026430653, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00017172091, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.0003308418, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00025915087, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.0002858792, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 27000, training loss= 0.0005827119, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00022435088, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00038301674, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00051090866, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 8.088483e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00013242158, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 3.2177828e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.00016227644, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.00025282416, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 36000, training loss= 8.260548e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00029083417, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 0.00010593845, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 8.1678525e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 4.967781e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 0.00068595086, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 3.0444839e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 9.294522e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 0.0001676061, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 0.0002985589, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 0.00011884836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 7.644581e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 0.00041985867, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 2.5048012e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 34.586056, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.0033550048, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.0038664606, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0051631853, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.0017997047, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.003201254, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.001639541, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 7000, training loss= 0.0034991773, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 8000, training loss= 0.0012673372, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0014511597, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.00093946507, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0006458397, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0020452882, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0009547111, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.00077922235, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.00066475704, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.0009739567, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.00064644736, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00022668573, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0007265964, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.00031327995, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.00072632445, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0959341, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 23000, training loss= 0.00023444714, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.00088720216, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 0.00031687232, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.0026007113, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00029742942, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.00020377325, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00014271197, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00040042188, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.00031438854, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 7.121403e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00022117229, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.00016043606, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 5.9499147e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.00101002, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00064949197, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 0.00010698722, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 3.9179686e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 0.00010904329, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.00033870304, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00014891948, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 0.00012667719, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 44000, training loss= 3.0378038e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 0.0005053737, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 4.1519346e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 4.202205e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 0.00085374626, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 49000, training loss= 9.8989825e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 6.6781173, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0043186806, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.0030708425, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0029577003, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0017182498, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.00083335233, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.0010557588, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.00039213308, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0009028142, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.00029771688, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0007596213, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00025584758, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.00012746309, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0004910609, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.00012205681, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.000995228, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.0001827248, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 17000, training loss= 8.5568434e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 18000, training loss= 5.643367e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 19000, training loss= 0.00019142203, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 7.145778e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.0004406903, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 22000, training loss= 5.3791817e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.00015261128, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 7.721004e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 25000, training loss= 2.1208733e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 2.053646e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 3.941639e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 1.6520658e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.00018451002, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 5.6060646e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 7.681401e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 9.457071e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 1.2400487e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 5.2901236e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 35000, training loss= 2.3446393e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 8.5677675e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 37000, training loss= 2.882291e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 38000, training loss= 6.805039e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 39000, training loss= 4.020889e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 40000, training loss= 8.893783e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 1.9368157e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 1.29667e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 3.4925386e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 6.018619e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 2.490768e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 7.414331e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 1.7478707e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 48000, training loss= 8.279463e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 49000, training loss= 6.484595e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 38.41963, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.009387867, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.030859811, training acc= 99.00000095367432%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.003870385, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0036800087, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0034809646, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0010027133, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 7000, training loss= 0.003030733, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0015836066, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.0017478953, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 10000, training loss= 0.001777591, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.0013688486, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.00062314235, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0010195988, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0009268461, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0009222841, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00057709916, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00045775637, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.00054953014, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.0007808287, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.00037441077, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 21000, training loss= 0.00019369375, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0046363864, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.69999694824219 ...\n",
            "\n",
            "step 23000, training loss= 0.0003757218, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.00017246004, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00030813162, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00051718357, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 27000, training loss= 0.00013792132, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.00064840936, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 6.82934e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00025689413, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.00019684264, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 5.6341658e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 0.000112055466, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 34000, training loss= 0.00014684128, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 35000, training loss= 0.00012361835, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 36000, training loss= 0.000102208534, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.00025286892, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 7.1283175e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 2.166082e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 0.0012060103, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 7.735586e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 7.493767e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 2.140702e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 44000, training loss= 0.00011573037, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 4.164314e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 2.6730215e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 2.80138e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 1.3426505e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 2.1202151e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 9.31573, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.002857685, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.004929288, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0060906247, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.0055190152, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 5000, training loss= 0.0030688222, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.0012948659, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.02220029, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.00025067726, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.0005896561, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 10000, training loss= 0.00026986955, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.0034814514, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.00034066886, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 13000, training loss= 0.00020358221, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.00010869464, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.00017201359, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 8.8718116e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00012825697, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.0002333696, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 2.6425278e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 20000, training loss= 6.3563304e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00038294183, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00013725231, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 5.266406e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.0003800907, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 25000, training loss= 7.7829456e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 4.6441437e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00013825351, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 6.306723e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 29000, training loss= 7.58374e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 3.484637e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 3.8827955e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00048126592, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 3.1229727e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.0002610392, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 8.807872e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 6.6619914e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 5.4168988e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 2.1291295e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 4.7597372e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 7.970408e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 1.6553688e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 1.0375057e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 0.00014074985, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 3.7412738e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 7.6118313e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 1.449385e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 47000, training loss= 1.2269214e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 48000, training loss= 2.4162598e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 49000, training loss= 4.454057e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 30.681232, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.053260107, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.004802169, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0055247634, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.003650523, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.006172777, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0013826056, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0013159631, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 8000, training loss= 0.0018215269, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.001851655, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0022488376, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0007001167, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0012629911, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0013280836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 14000, training loss= 0.0005589984, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.00055574265, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.0011412809, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.0009668238, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 18000, training loss= 0.00066444004, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.00063731393, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0012591998, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.002173192, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 22000, training loss= 0.00061685214, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00045984206, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.00029596448, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00029777698, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 0.00037112954, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 27000, training loss= 0.0010369347, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.00014534043, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.0002123429, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.0006447847, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.00020413275, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 32000, training loss= 0.00015609582, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 0.0001370849, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.000117948046, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.00012720344, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 36000, training loss= 0.00012619505, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.0021003678, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 8.500394e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 7.202007e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 7.626741e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 41000, training loss= 9.1985086e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00022055305, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 43000, training loss= 7.8286845e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 44000, training loss= 0.000111612455, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 6.0522434e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 0.00056431047, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.00032712304, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 2.3499146e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 0.00072160544, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 12.263403, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0020916567, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0021811558, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0026945572, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0021116997, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0014756463, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0023363552, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 7000, training loss= 0.001041857, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.0021353157, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 9000, training loss= 0.00040401827, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.0007385146, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.00030834135, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.00069736084, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0012250127, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.00016937563, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00030634095, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00015056004, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 17000, training loss= 0.00016412955, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.00022408416, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.00021963874, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 8.145901e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 7.466024e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0002987491, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 23000, training loss= 0.00021659757, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.00010787069, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 6.3747146e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 26000, training loss= 0.000121272395, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 9.872875e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 2.1719225e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 2.9887062e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 9.7345e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 3.925507e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 0.00041141157, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 0.0006668244, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 0.000100731806, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 1.6369424e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 36000, training loss= 2.9659013e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 37000, training loss= 4.309094e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 38000, training loss= 8.101298e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.80000305175781 ...\n",
            "\n",
            "step 39000, training loss= 7.416764e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.80000305175781 ...\n",
            "\n",
            "step 40000, training loss= 7.4851137e-06, training acc= 100.0%\n",
            "Validation Accuracy valid 99.80000305175781 ...\n",
            "\n",
            "step 41000, training loss= 1.3510726e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.80000305175781 ...\n",
            "\n",
            "step 42000, training loss= 8.652177e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 4.3554654e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 2.5753814e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 3.9103394e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 6.6206754e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 5.6758065e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 5.907666e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 49000, training loss= 1.8778357e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "Valid acc= 99.8 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 29.655027, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.008956926, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 2000, training loss= 0.00578968, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.004492564, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.003175271, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0018570258, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.03453665, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0020518769, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.017764613, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.001872989, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.0013957269, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00078076456, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.0007890464, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.0005508138, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.0030505199, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.00084842637, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.00036520005, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.0006046767, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00049073016, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.00028333944, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00030208335, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.0016376646, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.0019658487, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00029473018, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 24000, training loss= 0.0001888724, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 25000, training loss= 0.0015906139, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 26000, training loss= 0.00018792598, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.00015704414, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.000266201, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00026711135, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00026616314, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 31000, training loss= 0.00048736492, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 5.980135e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 8.975557e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.00021787266, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.00014143986, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 0.00040660487, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.00034614446, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 0.00014760696, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 39000, training loss= 0.00016303267, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 0.00016701032, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 7.0056005e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 0.0001562374, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 1.069442e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 0.000116565796, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 45000, training loss= 0.000101586076, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 2.1625738e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 1.6213866e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 0.0001321359, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 0.00017405528, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 18.172853, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0076377536, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.0037836058, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 3000, training loss= 0.009118925, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 4000, training loss= 0.0011267227, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.004693807, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0039585386, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0021360558, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.005030641, training acc= 100.0%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0004147151, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 10000, training loss= 0.0006372536, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.002678388, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0007961647, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 13000, training loss= 0.0005285322, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.01574947, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 15000, training loss= 0.0005688829, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.00040029828, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.002359253, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 18000, training loss= 0.00027004356, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 19000, training loss= 0.0005583881, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.000111489426, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 9.514675e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.00016169138, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0010090836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 4.6852703e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00021086835, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 5.2361887e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.00012545648, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0001558747, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.0001255391, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00016178761, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 7.2675255e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 5.077454e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.00016394118, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.0004063231, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.00020477425, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 4.258852e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 0.0004236017, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00016575151, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 5.7124238e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 40000, training loss= 2.6845679e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 0.00011417956, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 1.8552502e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 43000, training loss= 4.1438052e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 5.0570416e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 4.10159e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 5.0881597e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 3.6981644e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 4.218919e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 4.0742812e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 29.896626, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0075657046, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.007258614, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0039171902, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 4000, training loss= 0.0022285862, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.0052664387, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.0011462483, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.012884213, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 8000, training loss= 0.0015147903, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 9000, training loss= 0.0009831167, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 10000, training loss= 0.0011895886, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0046834704, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 12000, training loss= 0.00191007, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 13000, training loss= 0.0010700442, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0007967027, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00059436413, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 16000, training loss= 0.00079281506, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00025605876, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.00071243267, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.0006647871, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00053682935, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.0019925414, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0007027183, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.00059941114, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.00038555765, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.0001982342, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00031336007, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 27000, training loss= 0.0002604078, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 28000, training loss= 0.000424599, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00027905128, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.00013815849, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 31000, training loss= 0.00023813231, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 0.00026810632, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 33000, training loss= 0.0002020178, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 0.00012102508, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 0.0003204153, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 7.061896e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 7.346598e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.00046444507, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.00020761086, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 6.1874176e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 7.0642884e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 7.908128e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 0.00031126005, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 44000, training loss= 0.00024973787, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 45000, training loss= 8.043387e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 46000, training loss= 4.586346e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 0.00036610174, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 48000, training loss= 0.00014297978, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 49000, training loss= 5.5594588e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 19.687286, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.02640957, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 2000, training loss= 0.0029818239, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 3000, training loss= 0.0016457783, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0031146556, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 5000, training loss= 0.016574781, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0010562361, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.014953385, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0013378247, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 9000, training loss= 0.0009900472, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.008337623, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0012558599, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0005158477, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 13000, training loss= 0.00038346212, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 14000, training loss= 0.0002639669, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00053669506, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.0007256657, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.00304608, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 18000, training loss= 0.00016571724, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.00027897058, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 20000, training loss= 0.0003592673, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.00031012838, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.00024755805, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 8.896818e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.000105798725, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 25000, training loss= 6.501907e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 26000, training loss= 3.2433454e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.0006096075, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 28000, training loss= 8.3749095e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 29000, training loss= 0.0008881616, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 30000, training loss= 0.00018968519, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.0001244467, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 0.00014006195, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 2.5118457e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 5.5651963e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 8.4088606e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 2.4954255e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00012648317, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 2.5803995e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 4.2609274e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 2.5712761e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 3.397182e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 8.532489e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 1.5181234e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 1.7610027e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 3.131338e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 1.6282154e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 4.02534e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 0.00010609791, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 49000, training loss= 5.632509e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 53.829983, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0073007927, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.005572961, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 3000, training loss= 0.0034808388, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0040017413, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 5000, training loss= 0.008298427, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0009237185, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0032861214, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 8000, training loss= 0.0016792719, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 9000, training loss= 0.0046553994, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0011625374, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 11000, training loss= 0.0011876381, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0008370331, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.00079569494, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.0008397627, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.0005888596, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 16000, training loss= 0.0003701512, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 17000, training loss= 0.0005445257, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 18000, training loss= 0.00040089243, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 19000, training loss= 0.00065803697, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 20000, training loss= 0.00030983673, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 21000, training loss= 0.00022348463, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 22000, training loss= 0.0017190463, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.0003566724, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.00017415633, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.0010252057, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.0001284699, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 0.0013601983, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 28000, training loss= 0.00021180182, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00021519694, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 30000, training loss= 0.0001088632, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.00019080995, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.00017925394, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 4.4200482e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.0006988073, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.00015461932, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 6.843901e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 3.108728e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 9.818995e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 39000, training loss= 3.582148e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 40000, training loss= 8.0140635e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 41000, training loss= 0.0004458771, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 42000, training loss= 0.00010732914, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 43000, training loss= 4.953069e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 44000, training loss= 0.00015560084, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 45000, training loss= 6.0027567e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 46000, training loss= 1.6268206e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 47000, training loss= 2.7949627e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 48000, training loss= 2.1516846e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 2.4295708e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.5 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 10.702294, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0035236385, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 2000, training loss= 0.0022047397, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0016800496, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 4000, training loss= 0.0030372692, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 5000, training loss= 0.001089079, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 6000, training loss= 0.00095523696, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0014336013, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.00081015774, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.000528816, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.00069854985, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.004374566, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.053841073, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 13000, training loss= 0.0004658386, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 14000, training loss= 0.0004490585, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 15000, training loss= 0.00112426, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 16000, training loss= 0.00038022888, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.0004186202, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 18000, training loss= 0.0008960886, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 19000, training loss= 0.00066623534, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.00022899108, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 21000, training loss= 0.00021699695, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 22000, training loss= 0.00052289997, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.0003179539, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 24000, training loss= 0.0001261144, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.0001296618, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00013404206, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 27000, training loss= 6.402572e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 28000, training loss= 0.0041555683, training acc= 100.0%\n",
            "Validation Accuracy valid 98.9000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00022309361, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 0.00010392775, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0002505517, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 32000, training loss= 9.197092e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 33000, training loss= 0.0004994324, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 34000, training loss= 0.00010045522, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.0002166324, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 36000, training loss= 7.161025e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 37000, training loss= 5.704843e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 38000, training loss= 0.0003057661, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 5.4066786e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 7.281733e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 41000, training loss= 1.4036564e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 42000, training loss= 2.6899901e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 43000, training loss= 7.320873e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 9.9261444e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 45000, training loss= 6.0355927e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 3.14691e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 47000, training loss= 2.3153936e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 2.4798515e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 0.00040745529, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 6 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 25.557884, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.005098653, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.0017389038, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 3000, training loss= 0.0031632418, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0055597806, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 5000, training loss= 0.0029211196, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 6000, training loss= 0.0010068109, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0023838577, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.0017775388, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 9000, training loss= 0.0023163669, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 10000, training loss= 0.0008375144, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 11000, training loss= 0.0017827051, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 12000, training loss= 0.0015577605, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.0064798277, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.00043276302, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 15000, training loss= 0.0064348183, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 16000, training loss= 0.0005838581, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 17000, training loss= 0.0005329067, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00061939354, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 19000, training loss= 0.00031129943, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.00019178254, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.00036794148, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.0015301446, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 23000, training loss= 0.0008329387, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 24000, training loss= 0.00038364736, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 25000, training loss= 0.0003076038, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 26000, training loss= 0.00027102517, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.004585491, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.0006051755, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 29000, training loss= 0.00025890896, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 30000, training loss= 0.00038017685, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.00058419054, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 32000, training loss= 0.00010597555, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 33000, training loss= 0.000107796426, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 34000, training loss= 0.0004859275, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 35000, training loss= 0.00043809472, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.00023553315, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00020514724, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 38000, training loss= 4.5057634e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 39000, training loss= 3.7710095e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 40000, training loss= 2.7558932e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.69999694824219 ...\n",
            "\n",
            "step 41000, training loss= 0.00017458788, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 0.00012642056, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 2.0387817e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 0.00016714705, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 45000, training loss= 7.016198e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 46000, training loss= 1.9666832e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 47000, training loss= 0.00022392785, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 48000, training loss= 6.87208e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 9.709592e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.7 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 6 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 19.555338, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 1000, training loss= 0.0061785677, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 2000, training loss= 0.0034813683, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 3000, training loss= 0.0022786942, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 4000, training loss= 0.0036257608, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 5000, training loss= 0.0017966429, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 6000, training loss= 0.0010784207, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 7000, training loss= 0.0015682717, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 8000, training loss= 0.0022980047, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 9000, training loss= 0.00050276075, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 10000, training loss= 0.00059966213, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 11000, training loss= 0.0021685832, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0004137595, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 13000, training loss= 0.00046706837, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 14000, training loss= 0.00048494423, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 15000, training loss= 0.00042691524, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.0006611034, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00072534324, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 18000, training loss= 0.0001920344, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.000534299, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 20000, training loss= 0.0006046243, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.00016728402, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.00031100592, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 23000, training loss= 0.00043504688, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.00014818265, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00014722194, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 26000, training loss= 0.00027674026, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 27000, training loss= 0.00018701788, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00013097229, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00041917912, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 30000, training loss= 8.273662e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 31000, training loss= 0.0003289365, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 32000, training loss= 0.0008078178, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 33000, training loss= 0.0002333117, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 34000, training loss= 0.0009624171, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 35000, training loss= 0.00019486605, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 36000, training loss= 0.0002520848, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 37000, training loss= 0.00027053, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 38000, training loss= 3.160486e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 39000, training loss= 0.0001602455, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 40000, training loss= 6.341048e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 41000, training loss= 9.168944e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 42000, training loss= 2.6885762e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 8.573526e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 0.00028779366, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 0.0001124758, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 6.58721e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 8.515013e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 0.00012178931, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 49000, training loss= 4.6469042e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.59919738769531 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 7 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "step 0, training loss= 20.39285, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 1000, training loss= 0.028813517, training acc= 99.00000095367432%\n",
            "Validation Accuracy valid 98.69999694824219 ...\n",
            "\n",
            "step 2000, training loss= 0.007533588, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0 ...\n",
            "\n",
            "step 3000, training loss= 0.004960901, training acc= 100.0%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 4000, training loss= 0.002477399, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 5000, training loss= 0.0022465636, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 6000, training loss= 0.027211862, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 7000, training loss= 0.0006029445, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 8000, training loss= 0.00083297136, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 9000, training loss= 0.0021059313, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 10000, training loss= 0.0009833485, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 11000, training loss= 0.00085346453, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 12000, training loss= 0.0035290231, training acc= 99.50000047683716%\n",
            "Validation Accuracy valid 99.0999984741211 ...\n",
            "\n",
            "step 13000, training loss= 0.0015149743, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 14000, training loss= 0.0012562673, training acc= 100.0%\n",
            "Validation Accuracy valid 99.19999694824219 ...\n",
            "\n",
            "step 15000, training loss= 0.0004777957, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 16000, training loss= 0.00040249995, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 17000, training loss= 0.00081424805, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 18000, training loss= 0.00041072254, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 19000, training loss= 0.000597694, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 20000, training loss= 0.0007033848, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 21000, training loss= 0.0007011091, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 22000, training loss= 0.00065001135, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 23000, training loss= 0.00061130256, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 24000, training loss= 0.00027482797, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 25000, training loss= 0.00032952407, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 26000, training loss= 9.602941e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 27000, training loss= 0.00023518383, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 28000, training loss= 0.00017212538, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 29000, training loss= 0.00033242966, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 30000, training loss= 0.0005566079, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 31000, training loss= 0.00013797365, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 32000, training loss= 0.00043981944, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 33000, training loss= 8.9194415e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 34000, training loss= 0.00027558836, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 35000, training loss= 2.6748123e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 36000, training loss= 0.00023090829, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 37000, training loss= 8.397032e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.29999542236328 ...\n",
            "\n",
            "step 38000, training loss= 4.67573e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 39000, training loss= 0.00013112949, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 40000, training loss= 0.00011652387, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 41000, training loss= 0.00034474785, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 42000, training loss= 5.7222478e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 43000, training loss= 0.00046684186, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 44000, training loss= 6.516163e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 45000, training loss= 7.1883536e-05, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 46000, training loss= 0.00073754106, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5999984741211 ...\n",
            "\n",
            "step 47000, training loss= 0.0010515933, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "step 48000, training loss= 0.00012411876, training acc= 100.0%\n",
            "Validation Accuracy valid 99.4000015258789 ...\n",
            "\n",
            "step 49000, training loss= 0.00066961924, training acc= 100.0%\n",
            "Validation Accuracy valid 99.5 ...\n",
            "\n",
            "Valid acc= 99.6 %\n",
            "Validation Accuracy Test 99.79959869384766 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 7 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZSMp4Gb3iOl9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ValidAccuracy_Track"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ePrqsPmypas",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Valid acc= 99.8 %\n",
        "#### Validation Accuracy Test 99.79959869384766 ...\n",
        "==================================================\n",
        "W1 = 6 ...\n",
        "W2 = 1 ...\n",
        "W3 = 1 ...\n",
        "#### Both are highest for this choice. 2nd (orange is used to brake tie)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CY134ppPj5MJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUWLTLWuypay",
        "colab_type": "code",
        "outputId": "3b3a8153-0b41-4838-d557-2c11097b607b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(ValidAccuracy_Track)\n",
        "plt.plot(ValidAccuracy_Test_track)\n",
        "\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nntcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x2239231af28>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "71aTIMYhnC7S",
        "outputId": "b7008e34-a816-4e90-91fb-26f2d2fd9f8f",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(ValidAccuracy_Track)\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPK3h3XiPnQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x223614a7240>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W_JuCqQ0nJ32",
        "outputId": "00ec87e2-2fdd-4c7d-8827-2263110b36ce",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist([ValidAccuracy_Track],bins=30)\n",
        "plt.ylabel('Iter')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x2239227c9b0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qvG-8iJdiN0h"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "-CTLSyrUypbV",
        "colab_type": "code",
        "outputId": "8f7125d3-b72c-4768-d412-19d88d9dc9a8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(ValidAccuracy_Test_track)\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x22392003dd8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "eRVYMADEypbc",
        "colab_type": "code",
        "outputId": "2bca1d89-5eb7-4f89-86ee-a13629624e0c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist([ValidAccuracy_Test_track],bins=30)\n",
        "plt.ylabel('Iter')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCSpcQaBJDXu/wDq9G0dWOHZOgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x2239209c8d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LC5Bq1ZffTHJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Tune Performance"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bru4xdcFhNLX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}