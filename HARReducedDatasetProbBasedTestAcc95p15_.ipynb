{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HARReducedDatasetProbBasedTestAcc95p15 .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/HARReducedDatasetProbBasedTestAcc95p15_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qWL1XvRKt0EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yuoNYnTshaZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4FSwFTChgdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_label = np.concatenate((train_label, validation_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j5erFTN6-BRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef325cd-d624-4ef0-a569-fbc596609718"
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7352, 374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "VloppGYRyrEX",
        "colab_type": "code",
        "outputId": "43b8731e-123b-4987-da0a-ffa7b421a567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(250, ), max_iter=500, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.79281312\n",
            "Iteration 2, loss = 1.78409267\n",
            "Iteration 3, loss = 1.78286704\n",
            "Iteration 4, loss = 1.78234338\n",
            "Iteration 5, loss = 1.78186988\n",
            "Iteration 6, loss = 1.78146073\n",
            "Iteration 7, loss = 1.78103332\n",
            "Iteration 8, loss = 1.78018217\n",
            "Iteration 9, loss = 1.77990886\n",
            "Iteration 10, loss = 1.77963231\n",
            "Iteration 11, loss = 1.77915781\n",
            "Iteration 12, loss = 1.77869779\n",
            "Iteration 13, loss = 1.77834140\n",
            "Iteration 14, loss = 1.77787317\n",
            "Iteration 15, loss = 1.77766253\n",
            "Iteration 16, loss = 1.77710127\n",
            "Iteration 17, loss = 1.77673977\n",
            "Iteration 18, loss = 1.77652416\n",
            "Iteration 19, loss = 1.77615836\n",
            "Iteration 20, loss = 1.77562556\n",
            "Iteration 21, loss = 1.77536142\n",
            "Iteration 22, loss = 1.77485804\n",
            "Iteration 23, loss = 1.77442456\n",
            "Iteration 24, loss = 1.77402926\n",
            "Iteration 25, loss = 1.77361005\n",
            "Iteration 26, loss = 1.77312919\n",
            "Iteration 27, loss = 1.77303202\n",
            "Iteration 28, loss = 1.77234520\n",
            "Iteration 29, loss = 1.77202528\n",
            "Iteration 30, loss = 1.77184732\n",
            "Iteration 31, loss = 1.77103097\n",
            "Iteration 32, loss = 1.77059855\n",
            "Iteration 33, loss = 1.77007916\n",
            "Iteration 34, loss = 1.76966236\n",
            "Iteration 35, loss = 1.76916232\n",
            "Iteration 36, loss = 1.76872120\n",
            "Iteration 37, loss = 1.76810343\n",
            "Iteration 38, loss = 1.76781573\n",
            "Iteration 39, loss = 1.76712834\n",
            "Iteration 40, loss = 1.76662540\n",
            "Iteration 41, loss = 1.76608147\n",
            "Iteration 42, loss = 1.76564334\n",
            "Iteration 43, loss = 1.76511593\n",
            "Iteration 44, loss = 1.76437817\n",
            "Iteration 45, loss = 1.76380413\n",
            "Iteration 46, loss = 1.76328401\n",
            "Iteration 47, loss = 1.76279495\n",
            "Iteration 48, loss = 1.76221200\n",
            "Iteration 49, loss = 1.76131343\n",
            "Iteration 50, loss = 1.76075347\n",
            "Iteration 51, loss = 1.76006866\n",
            "Iteration 52, loss = 1.75937440\n",
            "Iteration 53, loss = 1.75870029\n",
            "Iteration 54, loss = 1.75800624\n",
            "Iteration 55, loss = 1.75730717\n",
            "Iteration 56, loss = 1.75651184\n",
            "Iteration 57, loss = 1.75582322\n",
            "Iteration 58, loss = 1.75493969\n",
            "Iteration 59, loss = 1.75435959\n",
            "Iteration 60, loss = 1.75359470\n",
            "Iteration 61, loss = 1.75284606\n",
            "Iteration 62, loss = 1.75178244\n",
            "Iteration 63, loss = 1.75124621\n",
            "Iteration 64, loss = 1.75029248\n",
            "Iteration 65, loss = 1.74930052\n",
            "Iteration 66, loss = 1.74846630\n",
            "Iteration 67, loss = 1.74789368\n",
            "Iteration 68, loss = 1.74689579\n",
            "Iteration 69, loss = 1.74561584\n",
            "Iteration 70, loss = 1.74493671\n",
            "Iteration 71, loss = 1.74377198\n",
            "Iteration 72, loss = 1.74287766\n",
            "Iteration 73, loss = 1.74181195\n",
            "Iteration 74, loss = 1.74076840\n",
            "Iteration 75, loss = 1.73962697\n",
            "Iteration 76, loss = 1.73835495\n",
            "Iteration 77, loss = 1.73752875\n",
            "Iteration 78, loss = 1.73637230\n",
            "Iteration 79, loss = 1.73529344\n",
            "Iteration 80, loss = 1.73403520\n",
            "Iteration 81, loss = 1.73273996\n",
            "Iteration 82, loss = 1.73120967\n",
            "Iteration 83, loss = 1.73067012\n",
            "Iteration 84, loss = 1.72888087\n",
            "Iteration 85, loss = 1.72725950\n",
            "Iteration 86, loss = 1.72614569\n",
            "Iteration 87, loss = 1.72479423\n",
            "Iteration 88, loss = 1.72313047\n",
            "Iteration 89, loss = 1.72178041\n",
            "Iteration 90, loss = 1.72021457\n",
            "Iteration 91, loss = 1.71888839\n",
            "Iteration 92, loss = 1.71698273\n",
            "Iteration 93, loss = 1.71549914\n",
            "Iteration 94, loss = 1.71397748\n",
            "Iteration 95, loss = 1.71239100\n",
            "Iteration 96, loss = 1.71033918\n",
            "Iteration 97, loss = 1.70882074\n",
            "Iteration 98, loss = 1.70713932\n",
            "Iteration 99, loss = 1.70501804\n",
            "Iteration 100, loss = 1.70305927\n",
            "Iteration 101, loss = 1.70132388\n",
            "Iteration 102, loss = 1.69938039\n",
            "Iteration 103, loss = 1.69745682\n",
            "Iteration 104, loss = 1.69522565\n",
            "Iteration 105, loss = 1.69314697\n",
            "Iteration 106, loss = 1.69087197\n",
            "Iteration 107, loss = 1.68899594\n",
            "Iteration 108, loss = 1.68665841\n",
            "Iteration 109, loss = 1.68413277\n",
            "Iteration 110, loss = 1.68195558\n",
            "Iteration 111, loss = 1.67962063\n",
            "Iteration 112, loss = 1.67710113\n",
            "Iteration 113, loss = 1.67457256\n",
            "Iteration 114, loss = 1.67241807\n",
            "Iteration 115, loss = 1.66967015\n",
            "Iteration 116, loss = 1.66649762\n",
            "Iteration 117, loss = 1.66426134\n",
            "Iteration 118, loss = 1.66147146\n",
            "Iteration 119, loss = 1.65828584\n",
            "Iteration 120, loss = 1.65557340\n",
            "Iteration 121, loss = 1.65323625\n",
            "Iteration 122, loss = 1.64914910\n",
            "Iteration 123, loss = 1.64637517\n",
            "Iteration 124, loss = 1.64339607\n",
            "Iteration 125, loss = 1.64018156\n",
            "Iteration 126, loss = 1.63659162\n",
            "Iteration 127, loss = 1.63361355\n",
            "Iteration 128, loss = 1.62969100\n",
            "Iteration 129, loss = 1.62635372\n",
            "Iteration 130, loss = 1.62260403\n",
            "Iteration 131, loss = 1.61917504\n",
            "Iteration 132, loss = 1.61564139\n",
            "Iteration 133, loss = 1.61161771\n",
            "Iteration 134, loss = 1.60768693\n",
            "Iteration 135, loss = 1.60339083\n",
            "Iteration 136, loss = 1.60091626\n",
            "Iteration 137, loss = 1.59588699\n",
            "Iteration 138, loss = 1.59165586\n",
            "Iteration 139, loss = 1.58785131\n",
            "Iteration 140, loss = 1.58272990\n",
            "Iteration 141, loss = 1.57777490\n",
            "Iteration 142, loss = 1.57411561\n",
            "Iteration 143, loss = 1.56903226\n",
            "Iteration 144, loss = 1.56384924\n",
            "Iteration 145, loss = 1.55936001\n",
            "Iteration 146, loss = 1.55431398\n",
            "Iteration 147, loss = 1.54949851\n",
            "Iteration 148, loss = 1.54413273\n",
            "Iteration 149, loss = 1.53894368\n",
            "Iteration 150, loss = 1.53389135\n",
            "Iteration 151, loss = 1.52854781\n",
            "Iteration 152, loss = 1.52250376\n",
            "Iteration 153, loss = 1.51739497\n",
            "Iteration 154, loss = 1.51155802\n",
            "Iteration 155, loss = 1.50604025\n",
            "Iteration 156, loss = 1.49989578\n",
            "Iteration 157, loss = 1.49344928\n",
            "Iteration 158, loss = 1.48751940\n",
            "Iteration 159, loss = 1.48148236\n",
            "Iteration 160, loss = 1.47514475\n",
            "Iteration 161, loss = 1.46856742\n",
            "Iteration 162, loss = 1.46231197\n",
            "Iteration 163, loss = 1.45562236\n",
            "Iteration 164, loss = 1.44911458\n",
            "Iteration 165, loss = 1.44258015\n",
            "Iteration 166, loss = 1.43480145\n",
            "Iteration 167, loss = 1.42780067\n",
            "Iteration 168, loss = 1.42078843\n",
            "Iteration 169, loss = 1.41389141\n",
            "Iteration 170, loss = 1.40645242\n",
            "Iteration 171, loss = 1.39827348\n",
            "Iteration 172, loss = 1.39115515\n",
            "Iteration 173, loss = 1.38440858\n",
            "Iteration 174, loss = 1.37640224\n",
            "Iteration 175, loss = 1.36822196\n",
            "Iteration 176, loss = 1.35966267\n",
            "Iteration 177, loss = 1.35202405\n",
            "Iteration 178, loss = 1.34495145\n",
            "Iteration 179, loss = 1.33644896\n",
            "Iteration 180, loss = 1.32752939\n",
            "Iteration 181, loss = 1.31865589\n",
            "Iteration 182, loss = 1.30948497\n",
            "Iteration 183, loss = 1.30091850\n",
            "Iteration 184, loss = 1.29213119\n",
            "Iteration 185, loss = 1.28434076\n",
            "Iteration 186, loss = 1.27479207\n",
            "Iteration 187, loss = 1.26538931\n",
            "Iteration 188, loss = 1.25701398\n",
            "Iteration 189, loss = 1.24767004\n",
            "Iteration 190, loss = 1.23764730\n",
            "Iteration 191, loss = 1.22949642\n",
            "Iteration 192, loss = 1.22017922\n",
            "Iteration 193, loss = 1.20886874\n",
            "Iteration 194, loss = 1.20022175\n",
            "Iteration 195, loss = 1.19032422\n",
            "Iteration 196, loss = 1.18124303\n",
            "Iteration 197, loss = 1.17251415\n",
            "Iteration 198, loss = 1.16261541\n",
            "Iteration 199, loss = 1.15087901\n",
            "Iteration 200, loss = 1.14137905\n",
            "Iteration 201, loss = 1.13205633\n",
            "Iteration 202, loss = 1.12137007\n",
            "Iteration 203, loss = 1.11165330\n",
            "Iteration 204, loss = 1.10175935\n",
            "Iteration 205, loss = 1.09096471\n",
            "Iteration 206, loss = 1.08125221\n",
            "Iteration 207, loss = 1.07021979\n",
            "Iteration 208, loss = 1.06160631\n",
            "Iteration 209, loss = 1.05097282\n",
            "Iteration 210, loss = 1.04064711\n",
            "Iteration 211, loss = 1.03010443\n",
            "Iteration 212, loss = 1.02034570\n",
            "Iteration 213, loss = 1.00934314\n",
            "Iteration 214, loss = 0.99941967\n",
            "Iteration 215, loss = 0.98864076\n",
            "Iteration 216, loss = 0.97908886\n",
            "Iteration 217, loss = 0.96820780\n",
            "Iteration 218, loss = 0.95787124\n",
            "Iteration 219, loss = 0.94822422\n",
            "Iteration 220, loss = 0.93717335\n",
            "Iteration 221, loss = 0.92721581\n",
            "Iteration 222, loss = 0.91870558\n",
            "Iteration 223, loss = 0.90685787\n",
            "Iteration 224, loss = 0.89676028\n",
            "Iteration 225, loss = 0.88746234\n",
            "Iteration 226, loss = 0.87725228\n",
            "Iteration 227, loss = 0.86721126\n",
            "Iteration 228, loss = 0.85676809\n",
            "Iteration 229, loss = 0.84782601\n",
            "Iteration 230, loss = 0.83689056\n",
            "Iteration 231, loss = 0.82762291\n",
            "Iteration 232, loss = 0.81778297\n",
            "Iteration 233, loss = 0.80846239\n",
            "Iteration 234, loss = 0.79868497\n",
            "Iteration 235, loss = 0.78942367\n",
            "Iteration 236, loss = 0.78056141\n",
            "Iteration 237, loss = 0.77053040\n",
            "Iteration 238, loss = 0.76135945\n",
            "Iteration 239, loss = 0.75196509\n",
            "Iteration 240, loss = 0.74303447\n",
            "Iteration 241, loss = 0.73465447\n",
            "Iteration 242, loss = 0.72531039\n",
            "Iteration 243, loss = 0.71646742\n",
            "Iteration 244, loss = 0.70728212\n",
            "Iteration 245, loss = 0.69816443\n",
            "Iteration 246, loss = 0.69058328\n",
            "Iteration 247, loss = 0.68190522\n",
            "Iteration 248, loss = 0.67318479\n",
            "Iteration 249, loss = 0.66463317\n",
            "Iteration 250, loss = 0.65678909\n",
            "Iteration 251, loss = 0.64893406\n",
            "Iteration 252, loss = 0.64097807\n",
            "Iteration 253, loss = 0.63214904\n",
            "Iteration 254, loss = 0.62490790\n",
            "Iteration 255, loss = 0.61754808\n",
            "Iteration 256, loss = 0.60954835\n",
            "Iteration 257, loss = 0.60201850\n",
            "Iteration 258, loss = 0.59498210\n",
            "Iteration 259, loss = 0.58727264\n",
            "Iteration 260, loss = 0.58009708\n",
            "Iteration 261, loss = 0.57279619\n",
            "Iteration 262, loss = 0.56573947\n",
            "Iteration 263, loss = 0.55899599\n",
            "Iteration 264, loss = 0.55199065\n",
            "Iteration 265, loss = 0.54500301\n",
            "Iteration 266, loss = 0.53895248\n",
            "Iteration 267, loss = 0.53209846\n",
            "Iteration 268, loss = 0.52513012\n",
            "Iteration 269, loss = 0.51891530\n",
            "Iteration 270, loss = 0.51318550\n",
            "Iteration 271, loss = 0.50650863\n",
            "Iteration 272, loss = 0.50030175\n",
            "Iteration 273, loss = 0.49516078\n",
            "Iteration 274, loss = 0.48886056\n",
            "Iteration 275, loss = 0.48276535\n",
            "Iteration 276, loss = 0.47732707\n",
            "Iteration 277, loss = 0.47133789\n",
            "Iteration 278, loss = 0.46609367\n",
            "Iteration 279, loss = 0.46024434\n",
            "Iteration 280, loss = 0.45479134\n",
            "Iteration 281, loss = 0.44963415\n",
            "Iteration 282, loss = 0.44433916\n",
            "Iteration 283, loss = 0.43945533\n",
            "Iteration 284, loss = 0.43392231\n",
            "Iteration 285, loss = 0.42940101\n",
            "Iteration 286, loss = 0.42420147\n",
            "Iteration 287, loss = 0.41967976\n",
            "Iteration 288, loss = 0.41456791\n",
            "Iteration 289, loss = 0.40994313\n",
            "Iteration 290, loss = 0.40579160\n",
            "Iteration 291, loss = 0.40154648\n",
            "Iteration 292, loss = 0.39621624\n",
            "Iteration 293, loss = 0.39256004\n",
            "Iteration 294, loss = 0.38723922\n",
            "Iteration 295, loss = 0.38383551\n",
            "Iteration 296, loss = 0.38033731\n",
            "Iteration 297, loss = 0.37507063\n",
            "Iteration 298, loss = 0.37099426\n",
            "Iteration 299, loss = 0.36697034\n",
            "Iteration 300, loss = 0.36317250\n",
            "Iteration 301, loss = 0.35902437\n",
            "Iteration 302, loss = 0.35563501\n",
            "Iteration 303, loss = 0.35155967\n",
            "Iteration 304, loss = 0.34833378\n",
            "Iteration 305, loss = 0.34450084\n",
            "Iteration 306, loss = 0.34108165\n",
            "Iteration 307, loss = 0.33728510\n",
            "Iteration 308, loss = 0.33419144\n",
            "Iteration 309, loss = 0.33090720\n",
            "Iteration 310, loss = 0.32683550\n",
            "Iteration 311, loss = 0.32426930\n",
            "Iteration 312, loss = 0.32080083\n",
            "Iteration 313, loss = 0.31717623\n",
            "Iteration 314, loss = 0.31474044\n",
            "Iteration 315, loss = 0.31136728\n",
            "Iteration 316, loss = 0.30840439\n",
            "Iteration 317, loss = 0.30537202\n",
            "Iteration 318, loss = 0.30238795\n",
            "Iteration 319, loss = 0.29949066\n",
            "Iteration 320, loss = 0.29673205\n",
            "Iteration 321, loss = 0.29412204\n",
            "Iteration 322, loss = 0.29105642\n",
            "Iteration 323, loss = 0.28870676\n",
            "Iteration 324, loss = 0.28611086\n",
            "Iteration 325, loss = 0.28380749\n",
            "Iteration 326, loss = 0.28060971\n",
            "Iteration 327, loss = 0.27793920\n",
            "Iteration 328, loss = 0.27568118\n",
            "Iteration 329, loss = 0.27357784\n",
            "Iteration 330, loss = 0.27098126\n",
            "Iteration 331, loss = 0.26821809\n",
            "Iteration 332, loss = 0.26602403\n",
            "Iteration 333, loss = 0.26367006\n",
            "Iteration 334, loss = 0.26136521\n",
            "Iteration 335, loss = 0.25897067\n",
            "Iteration 336, loss = 0.25701686\n",
            "Iteration 337, loss = 0.25476939\n",
            "Iteration 338, loss = 0.25222043\n",
            "Iteration 339, loss = 0.24993471\n",
            "Iteration 340, loss = 0.24824819\n",
            "Iteration 341, loss = 0.24595829\n",
            "Iteration 342, loss = 0.24399144\n",
            "Iteration 343, loss = 0.24199151\n",
            "Iteration 344, loss = 0.24004800\n",
            "Iteration 345, loss = 0.23770432\n",
            "Iteration 346, loss = 0.23659437\n",
            "Iteration 347, loss = 0.23417951\n",
            "Iteration 348, loss = 0.23235897\n",
            "Iteration 349, loss = 0.23042313\n",
            "Iteration 350, loss = 0.22870873\n",
            "Iteration 351, loss = 0.22714155\n",
            "Iteration 352, loss = 0.22503718\n",
            "Iteration 353, loss = 0.22341739\n",
            "Iteration 354, loss = 0.22182777\n",
            "Iteration 355, loss = 0.22010282\n",
            "Iteration 356, loss = 0.21848662\n",
            "Iteration 357, loss = 0.21648586\n",
            "Iteration 358, loss = 0.21495127\n",
            "Iteration 359, loss = 0.21310556\n",
            "Iteration 360, loss = 0.21180446\n",
            "Iteration 361, loss = 0.21086147\n",
            "Iteration 362, loss = 0.20912461\n",
            "Iteration 363, loss = 0.20715798\n",
            "Iteration 364, loss = 0.20578643\n",
            "Iteration 365, loss = 0.20422406\n",
            "Iteration 366, loss = 0.20284225\n",
            "Iteration 367, loss = 0.20154749\n",
            "Iteration 368, loss = 0.19982227\n",
            "Iteration 369, loss = 0.19837967\n",
            "Iteration 370, loss = 0.19692527\n",
            "Iteration 371, loss = 0.19623387\n",
            "Iteration 372, loss = 0.19453482\n",
            "Iteration 373, loss = 0.19290129\n",
            "Iteration 374, loss = 0.19153073\n",
            "Iteration 375, loss = 0.19033137\n",
            "Iteration 376, loss = 0.18936722\n",
            "Iteration 377, loss = 0.18800724\n",
            "Iteration 378, loss = 0.18678361\n",
            "Iteration 379, loss = 0.18538304\n",
            "Iteration 380, loss = 0.18429160\n",
            "Iteration 381, loss = 0.18296025\n",
            "Iteration 382, loss = 0.18178657\n",
            "Iteration 383, loss = 0.18061263\n",
            "Iteration 384, loss = 0.18024736\n",
            "Iteration 385, loss = 0.17833417\n",
            "Iteration 386, loss = 0.17711794\n",
            "Iteration 387, loss = 0.17573493\n",
            "Iteration 388, loss = 0.17488727\n",
            "Iteration 389, loss = 0.17387723\n",
            "Iteration 390, loss = 0.17333966\n",
            "Iteration 391, loss = 0.17178641\n",
            "Iteration 392, loss = 0.17124825\n",
            "Iteration 393, loss = 0.16976597\n",
            "Iteration 394, loss = 0.16869909\n",
            "Iteration 395, loss = 0.16779526\n",
            "Iteration 396, loss = 0.16681553\n",
            "Iteration 397, loss = 0.16551245\n",
            "Iteration 398, loss = 0.16480501\n",
            "Iteration 399, loss = 0.16388326\n",
            "Iteration 400, loss = 0.16382854\n",
            "Iteration 401, loss = 0.16206510\n",
            "Iteration 402, loss = 0.16092735\n",
            "Iteration 403, loss = 0.15982795\n",
            "Iteration 404, loss = 0.15938999\n",
            "Iteration 405, loss = 0.15828980\n",
            "Iteration 406, loss = 0.15738746\n",
            "Iteration 407, loss = 0.15630601\n",
            "Iteration 408, loss = 0.15539912\n",
            "Iteration 409, loss = 0.15430802\n",
            "Iteration 410, loss = 0.15393162\n",
            "Iteration 411, loss = 0.15278968\n",
            "Iteration 412, loss = 0.15208570\n",
            "Iteration 413, loss = 0.15124782\n",
            "Iteration 414, loss = 0.15033600\n",
            "Iteration 415, loss = 0.14963427\n",
            "Iteration 416, loss = 0.14873312\n",
            "Iteration 417, loss = 0.14818946\n",
            "Iteration 418, loss = 0.14717558\n",
            "Iteration 419, loss = 0.14620539\n",
            "Iteration 420, loss = 0.14564035\n",
            "Iteration 421, loss = 0.14474810\n",
            "Iteration 422, loss = 0.14422071\n",
            "Iteration 423, loss = 0.14333928\n",
            "Iteration 424, loss = 0.14260485\n",
            "Iteration 425, loss = 0.14173106\n",
            "Iteration 426, loss = 0.14106279\n",
            "Iteration 427, loss = 0.14036921\n",
            "Iteration 428, loss = 0.13980864\n",
            "Iteration 429, loss = 0.13909145\n",
            "Iteration 430, loss = 0.13807445\n",
            "Iteration 431, loss = 0.13784559\n",
            "Iteration 432, loss = 0.13710162\n",
            "Iteration 433, loss = 0.13610878\n",
            "Iteration 434, loss = 0.13593509\n",
            "Iteration 435, loss = 0.13481313\n",
            "Iteration 436, loss = 0.13449321\n",
            "Iteration 437, loss = 0.13334644\n",
            "Iteration 438, loss = 0.13297898\n",
            "Iteration 439, loss = 0.13240042\n",
            "Iteration 440, loss = 0.13173190\n",
            "Iteration 441, loss = 0.13098352\n",
            "Iteration 442, loss = 0.13102125\n",
            "Iteration 443, loss = 0.13006664\n",
            "Iteration 444, loss = 0.12936974\n",
            "Iteration 445, loss = 0.12876717\n",
            "Iteration 446, loss = 0.12833005\n",
            "Iteration 447, loss = 0.12794600\n",
            "Iteration 448, loss = 0.12693210\n",
            "Iteration 449, loss = 0.12654019\n",
            "Iteration 450, loss = 0.12559329\n",
            "Iteration 451, loss = 0.12504356\n",
            "Iteration 452, loss = 0.12464722\n",
            "Iteration 453, loss = 0.12412521\n",
            "Iteration 454, loss = 0.12349989\n",
            "Iteration 455, loss = 0.12311222\n",
            "Iteration 456, loss = 0.12256944\n",
            "Iteration 457, loss = 0.12248171\n",
            "Iteration 458, loss = 0.12139413\n",
            "Iteration 459, loss = 0.12097964\n",
            "Iteration 460, loss = 0.12031586\n",
            "Iteration 461, loss = 0.11996786\n",
            "Iteration 462, loss = 0.11941564\n",
            "Iteration 463, loss = 0.11940470\n",
            "Iteration 464, loss = 0.11854340\n",
            "Iteration 465, loss = 0.11821890\n",
            "Iteration 466, loss = 0.11763473\n",
            "Iteration 467, loss = 0.11689724\n",
            "Iteration 468, loss = 0.11647208\n",
            "Iteration 469, loss = 0.11626966\n",
            "Iteration 470, loss = 0.11552505\n",
            "Iteration 471, loss = 0.11512202\n",
            "Iteration 472, loss = 0.11452314\n",
            "Iteration 473, loss = 0.11415262\n",
            "Iteration 474, loss = 0.11385679\n",
            "Iteration 475, loss = 0.11324885\n",
            "Iteration 476, loss = 0.11311076\n",
            "Iteration 477, loss = 0.11215511\n",
            "Iteration 478, loss = 0.11189266\n",
            "Iteration 479, loss = 0.11121751\n",
            "Iteration 480, loss = 0.11089278\n",
            "Iteration 481, loss = 0.11070220\n",
            "Iteration 482, loss = 0.11000425\n",
            "Iteration 483, loss = 0.10954341\n",
            "Iteration 484, loss = 0.10936055\n",
            "Iteration 485, loss = 0.10868100\n",
            "Iteration 486, loss = 0.10846762\n",
            "Iteration 487, loss = 0.10812901\n",
            "Iteration 488, loss = 0.10769528\n",
            "Iteration 489, loss = 0.10702001\n",
            "Iteration 490, loss = 0.10729627\n",
            "Iteration 491, loss = 0.10643208\n",
            "Iteration 492, loss = 0.10633126\n",
            "Iteration 493, loss = 0.10592406\n",
            "Iteration 494, loss = 0.10522121\n",
            "Iteration 495, loss = 0.10486911\n",
            "Iteration 496, loss = 0.10421313\n",
            "Iteration 497, loss = 0.10430259\n",
            "Iteration 498, loss = 0.10366775\n",
            "Iteration 499, loss = 0.10312161\n",
            "Iteration 500, loss = 0.10324319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(250,), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "8m9_X9bUdZJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "# train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo_lFxdIc85n",
        "colab_type": "code",
        "outputId": "533bae1f-3911-45c8-aff1-fe004d4eab45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8690
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf2 = MLPClassifier(hidden_layer_sizes=(250, ), max_iter=500, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "clf2.fit(train_valid_combined, train_valid_label)\n",
        "# clf2.fit(train_data, train_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.79132991\n",
            "Iteration 2, loss = 1.78372116\n",
            "Iteration 3, loss = 1.78269095\n",
            "Iteration 4, loss = 1.78189126\n",
            "Iteration 5, loss = 1.78137186\n",
            "Iteration 6, loss = 1.78067961\n",
            "Iteration 7, loss = 1.78014791\n",
            "Iteration 8, loss = 1.77973240\n",
            "Iteration 9, loss = 1.77911500\n",
            "Iteration 10, loss = 1.77872453\n",
            "Iteration 11, loss = 1.77823439\n",
            "Iteration 12, loss = 1.77779942\n",
            "Iteration 13, loss = 1.77735660\n",
            "Iteration 14, loss = 1.77691445\n",
            "Iteration 15, loss = 1.77638985\n",
            "Iteration 16, loss = 1.77609271\n",
            "Iteration 17, loss = 1.77546230\n",
            "Iteration 18, loss = 1.77495020\n",
            "Iteration 19, loss = 1.77459955\n",
            "Iteration 20, loss = 1.77400248\n",
            "Iteration 21, loss = 1.77358778\n",
            "Iteration 22, loss = 1.77292523\n",
            "Iteration 23, loss = 1.77262133\n",
            "Iteration 24, loss = 1.77188219\n",
            "Iteration 25, loss = 1.77144058\n",
            "Iteration 26, loss = 1.77083183\n",
            "Iteration 27, loss = 1.77025382\n",
            "Iteration 28, loss = 1.76987026\n",
            "Iteration 29, loss = 1.76906441\n",
            "Iteration 30, loss = 1.76862000\n",
            "Iteration 31, loss = 1.76806994\n",
            "Iteration 32, loss = 1.76745643\n",
            "Iteration 33, loss = 1.76666532\n",
            "Iteration 34, loss = 1.76589965\n",
            "Iteration 35, loss = 1.76518805\n",
            "Iteration 36, loss = 1.76474984\n",
            "Iteration 37, loss = 1.76389589\n",
            "Iteration 38, loss = 1.76306536\n",
            "Iteration 39, loss = 1.76237676\n",
            "Iteration 40, loss = 1.76161705\n",
            "Iteration 41, loss = 1.76074693\n",
            "Iteration 42, loss = 1.76012780\n",
            "Iteration 43, loss = 1.75927089\n",
            "Iteration 44, loss = 1.75825810\n",
            "Iteration 45, loss = 1.75753067\n",
            "Iteration 46, loss = 1.75659923\n",
            "Iteration 47, loss = 1.75566461\n",
            "Iteration 48, loss = 1.75485778\n",
            "Iteration 49, loss = 1.75410435\n",
            "Iteration 50, loss = 1.75303956\n",
            "Iteration 51, loss = 1.75207754\n",
            "Iteration 52, loss = 1.75099276\n",
            "Iteration 53, loss = 1.75003710\n",
            "Iteration 54, loss = 1.74896149\n",
            "Iteration 55, loss = 1.74772773\n",
            "Iteration 56, loss = 1.74671222\n",
            "Iteration 57, loss = 1.74539244\n",
            "Iteration 58, loss = 1.74421121\n",
            "Iteration 59, loss = 1.74293158\n",
            "Iteration 60, loss = 1.74185547\n",
            "Iteration 61, loss = 1.74046935\n",
            "Iteration 62, loss = 1.73908181\n",
            "Iteration 63, loss = 1.73800402\n",
            "Iteration 64, loss = 1.73659120\n",
            "Iteration 65, loss = 1.73489796\n",
            "Iteration 66, loss = 1.73355055\n",
            "Iteration 67, loss = 1.73200896\n",
            "Iteration 68, loss = 1.73039585\n",
            "Iteration 69, loss = 1.72868181\n",
            "Iteration 70, loss = 1.72721117\n",
            "Iteration 71, loss = 1.72543135\n",
            "Iteration 72, loss = 1.72371998\n",
            "Iteration 73, loss = 1.72209702\n",
            "Iteration 74, loss = 1.72020760\n",
            "Iteration 75, loss = 1.71823680\n",
            "Iteration 76, loss = 1.71647651\n",
            "Iteration 77, loss = 1.71426260\n",
            "Iteration 78, loss = 1.71200469\n",
            "Iteration 79, loss = 1.71013268\n",
            "Iteration 80, loss = 1.70790823\n",
            "Iteration 81, loss = 1.70561717\n",
            "Iteration 82, loss = 1.70333700\n",
            "Iteration 83, loss = 1.70141286\n",
            "Iteration 84, loss = 1.69875229\n",
            "Iteration 85, loss = 1.69629819\n",
            "Iteration 86, loss = 1.69368797\n",
            "Iteration 87, loss = 1.69079103\n",
            "Iteration 88, loss = 1.68832517\n",
            "Iteration 89, loss = 1.68569497\n",
            "Iteration 90, loss = 1.68275676\n",
            "Iteration 91, loss = 1.67983927\n",
            "Iteration 92, loss = 1.67681773\n",
            "Iteration 93, loss = 1.67350486\n",
            "Iteration 94, loss = 1.67057659\n",
            "Iteration 95, loss = 1.66693251\n",
            "Iteration 96, loss = 1.66421595\n",
            "Iteration 97, loss = 1.66032177\n",
            "Iteration 98, loss = 1.65677854\n",
            "Iteration 99, loss = 1.65337658\n",
            "Iteration 100, loss = 1.64926354\n",
            "Iteration 101, loss = 1.64578763\n",
            "Iteration 102, loss = 1.64150342\n",
            "Iteration 103, loss = 1.63790303\n",
            "Iteration 104, loss = 1.63328435\n",
            "Iteration 105, loss = 1.62948982\n",
            "Iteration 106, loss = 1.62479219\n",
            "Iteration 107, loss = 1.62060776\n",
            "Iteration 108, loss = 1.61607454\n",
            "Iteration 109, loss = 1.61116153\n",
            "Iteration 110, loss = 1.60620550\n",
            "Iteration 111, loss = 1.60196862\n",
            "Iteration 112, loss = 1.59703508\n",
            "Iteration 113, loss = 1.59144248\n",
            "Iteration 114, loss = 1.58650320\n",
            "Iteration 115, loss = 1.58072827\n",
            "Iteration 116, loss = 1.57491237\n",
            "Iteration 117, loss = 1.56929826\n",
            "Iteration 118, loss = 1.56311962\n",
            "Iteration 119, loss = 1.55719634\n",
            "Iteration 120, loss = 1.55228726\n",
            "Iteration 121, loss = 1.54575623\n",
            "Iteration 122, loss = 1.53886308\n",
            "Iteration 123, loss = 1.53185987\n",
            "Iteration 124, loss = 1.52580678\n",
            "Iteration 125, loss = 1.51965681\n",
            "Iteration 126, loss = 1.51231817\n",
            "Iteration 127, loss = 1.50458917\n",
            "Iteration 128, loss = 1.49716000\n",
            "Iteration 129, loss = 1.49020424\n",
            "Iteration 130, loss = 1.48236056\n",
            "Iteration 131, loss = 1.47469037\n",
            "Iteration 132, loss = 1.46666253\n",
            "Iteration 133, loss = 1.45891949\n",
            "Iteration 134, loss = 1.45067166\n",
            "Iteration 135, loss = 1.44273914\n",
            "Iteration 136, loss = 1.43351091\n",
            "Iteration 137, loss = 1.42571564\n",
            "Iteration 138, loss = 1.41653274\n",
            "Iteration 139, loss = 1.40730830\n",
            "Iteration 140, loss = 1.39825406\n",
            "Iteration 141, loss = 1.38869940\n",
            "Iteration 142, loss = 1.37931501\n",
            "Iteration 143, loss = 1.36953209\n",
            "Iteration 144, loss = 1.35996898\n",
            "Iteration 145, loss = 1.35018783\n",
            "Iteration 146, loss = 1.33969802\n",
            "Iteration 147, loss = 1.32963390\n",
            "Iteration 148, loss = 1.31888385\n",
            "Iteration 149, loss = 1.30923485\n",
            "Iteration 150, loss = 1.29750537\n",
            "Iteration 151, loss = 1.28658402\n",
            "Iteration 152, loss = 1.27610489\n",
            "Iteration 153, loss = 1.26519016\n",
            "Iteration 154, loss = 1.25357838\n",
            "Iteration 155, loss = 1.24267150\n",
            "Iteration 156, loss = 1.23088497\n",
            "Iteration 157, loss = 1.21970150\n",
            "Iteration 158, loss = 1.20793314\n",
            "Iteration 159, loss = 1.19619479\n",
            "Iteration 160, loss = 1.18471954\n",
            "Iteration 161, loss = 1.17204894\n",
            "Iteration 162, loss = 1.15975144\n",
            "Iteration 163, loss = 1.14846631\n",
            "Iteration 164, loss = 1.13616917\n",
            "Iteration 165, loss = 1.12380099\n",
            "Iteration 166, loss = 1.11097322\n",
            "Iteration 167, loss = 1.09988732\n",
            "Iteration 168, loss = 1.08603357\n",
            "Iteration 169, loss = 1.07374137\n",
            "Iteration 170, loss = 1.06127111\n",
            "Iteration 171, loss = 1.04879509\n",
            "Iteration 172, loss = 1.03590295\n",
            "Iteration 173, loss = 1.02402941\n",
            "Iteration 174, loss = 1.01163318\n",
            "Iteration 175, loss = 0.99875740\n",
            "Iteration 176, loss = 0.98526795\n",
            "Iteration 177, loss = 0.97314438\n",
            "Iteration 178, loss = 0.96044747\n",
            "Iteration 179, loss = 0.94786081\n",
            "Iteration 180, loss = 0.93508742\n",
            "Iteration 181, loss = 0.92236645\n",
            "Iteration 182, loss = 0.91107286\n",
            "Iteration 183, loss = 0.89798225\n",
            "Iteration 184, loss = 0.88535029\n",
            "Iteration 185, loss = 0.87332146\n",
            "Iteration 186, loss = 0.86135604\n",
            "Iteration 187, loss = 0.84917764\n",
            "Iteration 188, loss = 0.83730812\n",
            "Iteration 189, loss = 0.82583077\n",
            "Iteration 190, loss = 0.81318917\n",
            "Iteration 191, loss = 0.80081643\n",
            "Iteration 192, loss = 0.78961784\n",
            "Iteration 193, loss = 0.77851014\n",
            "Iteration 194, loss = 0.76672890\n",
            "Iteration 195, loss = 0.75608472\n",
            "Iteration 196, loss = 0.74451917\n",
            "Iteration 197, loss = 0.73301292\n",
            "Iteration 198, loss = 0.72206713\n",
            "Iteration 199, loss = 0.71153232\n",
            "Iteration 200, loss = 0.70052907\n",
            "Iteration 201, loss = 0.69039323\n",
            "Iteration 202, loss = 0.67951514\n",
            "Iteration 203, loss = 0.66949302\n",
            "Iteration 204, loss = 0.65909382\n",
            "Iteration 205, loss = 0.64939090\n",
            "Iteration 206, loss = 0.63956553\n",
            "Iteration 207, loss = 0.62971459\n",
            "Iteration 208, loss = 0.62010891\n",
            "Iteration 209, loss = 0.61069481\n",
            "Iteration 210, loss = 0.60130807\n",
            "Iteration 211, loss = 0.59248456\n",
            "Iteration 212, loss = 0.58320641\n",
            "Iteration 213, loss = 0.57495519\n",
            "Iteration 214, loss = 0.56625323\n",
            "Iteration 215, loss = 0.55754767\n",
            "Iteration 216, loss = 0.54926089\n",
            "Iteration 217, loss = 0.54091013\n",
            "Iteration 218, loss = 0.53266347\n",
            "Iteration 219, loss = 0.52522088\n",
            "Iteration 220, loss = 0.51702653\n",
            "Iteration 221, loss = 0.50948574\n",
            "Iteration 222, loss = 0.50159708\n",
            "Iteration 223, loss = 0.49430530\n",
            "Iteration 224, loss = 0.48707151\n",
            "Iteration 225, loss = 0.47982424\n",
            "Iteration 226, loss = 0.47310172\n",
            "Iteration 227, loss = 0.46606669\n",
            "Iteration 228, loss = 0.45914303\n",
            "Iteration 229, loss = 0.45291922\n",
            "Iteration 230, loss = 0.44628061\n",
            "Iteration 231, loss = 0.44006016\n",
            "Iteration 232, loss = 0.43338772\n",
            "Iteration 233, loss = 0.42748082\n",
            "Iteration 234, loss = 0.42165337\n",
            "Iteration 235, loss = 0.41564978\n",
            "Iteration 236, loss = 0.41027934\n",
            "Iteration 237, loss = 0.40413033\n",
            "Iteration 238, loss = 0.39923152\n",
            "Iteration 239, loss = 0.39331834\n",
            "Iteration 240, loss = 0.38836472\n",
            "Iteration 241, loss = 0.38286068\n",
            "Iteration 242, loss = 0.37786066\n",
            "Iteration 243, loss = 0.37275856\n",
            "Iteration 244, loss = 0.36808903\n",
            "Iteration 245, loss = 0.36285440\n",
            "Iteration 246, loss = 0.35792833\n",
            "Iteration 247, loss = 0.35338688\n",
            "Iteration 248, loss = 0.34907343\n",
            "Iteration 249, loss = 0.34531341\n",
            "Iteration 250, loss = 0.34044302\n",
            "Iteration 251, loss = 0.33634609\n",
            "Iteration 252, loss = 0.33218318\n",
            "Iteration 253, loss = 0.32770454\n",
            "Iteration 254, loss = 0.32369157\n",
            "Iteration 255, loss = 0.31994729\n",
            "Iteration 256, loss = 0.31559212\n",
            "Iteration 257, loss = 0.31227938\n",
            "Iteration 258, loss = 0.30858363\n",
            "Iteration 259, loss = 0.30453794\n",
            "Iteration 260, loss = 0.30154872\n",
            "Iteration 261, loss = 0.29779831\n",
            "Iteration 262, loss = 0.29433204\n",
            "Iteration 263, loss = 0.29083126\n",
            "Iteration 264, loss = 0.28747627\n",
            "Iteration 265, loss = 0.28449115\n",
            "Iteration 266, loss = 0.28090205\n",
            "Iteration 267, loss = 0.27825953\n",
            "Iteration 268, loss = 0.27496498\n",
            "Iteration 269, loss = 0.27183444\n",
            "Iteration 270, loss = 0.26883779\n",
            "Iteration 271, loss = 0.26592789\n",
            "Iteration 272, loss = 0.26275420\n",
            "Iteration 273, loss = 0.26053159\n",
            "Iteration 274, loss = 0.25746366\n",
            "Iteration 275, loss = 0.25474298\n",
            "Iteration 276, loss = 0.25234802\n",
            "Iteration 277, loss = 0.24950535\n",
            "Iteration 278, loss = 0.24699397\n",
            "Iteration 279, loss = 0.24445786\n",
            "Iteration 280, loss = 0.24186533\n",
            "Iteration 281, loss = 0.23982106\n",
            "Iteration 282, loss = 0.23748934\n",
            "Iteration 283, loss = 0.23492844\n",
            "Iteration 284, loss = 0.23266644\n",
            "Iteration 285, loss = 0.23029622\n",
            "Iteration 286, loss = 0.22784822\n",
            "Iteration 287, loss = 0.22628705\n",
            "Iteration 288, loss = 0.22387055\n",
            "Iteration 289, loss = 0.22167977\n",
            "Iteration 290, loss = 0.21962045\n",
            "Iteration 291, loss = 0.21764973\n",
            "Iteration 292, loss = 0.21548740\n",
            "Iteration 293, loss = 0.21391522\n",
            "Iteration 294, loss = 0.21153482\n",
            "Iteration 295, loss = 0.21012194\n",
            "Iteration 296, loss = 0.20804188\n",
            "Iteration 297, loss = 0.20621907\n",
            "Iteration 298, loss = 0.20415184\n",
            "Iteration 299, loss = 0.20249975\n",
            "Iteration 300, loss = 0.20104885\n",
            "Iteration 301, loss = 0.19946702\n",
            "Iteration 302, loss = 0.19750895\n",
            "Iteration 303, loss = 0.19584979\n",
            "Iteration 304, loss = 0.19422125\n",
            "Iteration 305, loss = 0.19245653\n",
            "Iteration 306, loss = 0.19066031\n",
            "Iteration 307, loss = 0.18957457\n",
            "Iteration 308, loss = 0.18775938\n",
            "Iteration 309, loss = 0.18663035\n",
            "Iteration 310, loss = 0.18458532\n",
            "Iteration 311, loss = 0.18297276\n",
            "Iteration 312, loss = 0.18200811\n",
            "Iteration 313, loss = 0.18048878\n",
            "Iteration 314, loss = 0.17899529\n",
            "Iteration 315, loss = 0.17778582\n",
            "Iteration 316, loss = 0.17637833\n",
            "Iteration 317, loss = 0.17505978\n",
            "Iteration 318, loss = 0.17354751\n",
            "Iteration 319, loss = 0.17231089\n",
            "Iteration 320, loss = 0.17068623\n",
            "Iteration 321, loss = 0.16951363\n",
            "Iteration 322, loss = 0.16857242\n",
            "Iteration 323, loss = 0.16715023\n",
            "Iteration 324, loss = 0.16598918\n",
            "Iteration 325, loss = 0.16484581\n",
            "Iteration 326, loss = 0.16369283\n",
            "Iteration 327, loss = 0.16259538\n",
            "Iteration 328, loss = 0.16167769\n",
            "Iteration 329, loss = 0.16038404\n",
            "Iteration 330, loss = 0.15910746\n",
            "Iteration 331, loss = 0.15789557\n",
            "Iteration 332, loss = 0.15707497\n",
            "Iteration 333, loss = 0.15581900\n",
            "Iteration 334, loss = 0.15456824\n",
            "Iteration 335, loss = 0.15416611\n",
            "Iteration 336, loss = 0.15268404\n",
            "Iteration 337, loss = 0.15197219\n",
            "Iteration 338, loss = 0.15071666\n",
            "Iteration 339, loss = 0.14986199\n",
            "Iteration 340, loss = 0.14864672\n",
            "Iteration 341, loss = 0.14818126\n",
            "Iteration 342, loss = 0.14690861\n",
            "Iteration 343, loss = 0.14596815\n",
            "Iteration 344, loss = 0.14515658\n",
            "Iteration 345, loss = 0.14435689\n",
            "Iteration 346, loss = 0.14318875\n",
            "Iteration 347, loss = 0.14229113\n",
            "Iteration 348, loss = 0.14152406\n",
            "Iteration 349, loss = 0.14041933\n",
            "Iteration 350, loss = 0.13945641\n",
            "Iteration 351, loss = 0.13926397\n",
            "Iteration 352, loss = 0.13809381\n",
            "Iteration 353, loss = 0.13726042\n",
            "Iteration 354, loss = 0.13652709\n",
            "Iteration 355, loss = 0.13556106\n",
            "Iteration 356, loss = 0.13475089\n",
            "Iteration 357, loss = 0.13414623\n",
            "Iteration 358, loss = 0.13353075\n",
            "Iteration 359, loss = 0.13288506\n",
            "Iteration 360, loss = 0.13177965\n",
            "Iteration 361, loss = 0.13098841\n",
            "Iteration 362, loss = 0.13042951\n",
            "Iteration 363, loss = 0.12944244\n",
            "Iteration 364, loss = 0.12869701\n",
            "Iteration 365, loss = 0.12823705\n",
            "Iteration 366, loss = 0.12730606\n",
            "Iteration 367, loss = 0.12669208\n",
            "Iteration 368, loss = 0.12604243\n",
            "Iteration 369, loss = 0.12563266\n",
            "Iteration 370, loss = 0.12494343\n",
            "Iteration 371, loss = 0.12410382\n",
            "Iteration 372, loss = 0.12344140\n",
            "Iteration 373, loss = 0.12258893\n",
            "Iteration 374, loss = 0.12204173\n",
            "Iteration 375, loss = 0.12145760\n",
            "Iteration 376, loss = 0.12111196\n",
            "Iteration 377, loss = 0.11994969\n",
            "Iteration 378, loss = 0.11954437\n",
            "Iteration 379, loss = 0.11888069\n",
            "Iteration 380, loss = 0.11822499\n",
            "Iteration 381, loss = 0.11765217\n",
            "Iteration 382, loss = 0.11713351\n",
            "Iteration 383, loss = 0.11626390\n",
            "Iteration 384, loss = 0.11621546\n",
            "Iteration 385, loss = 0.11553572\n",
            "Iteration 386, loss = 0.11485998\n",
            "Iteration 387, loss = 0.11426503\n",
            "Iteration 388, loss = 0.11393389\n",
            "Iteration 389, loss = 0.11333588\n",
            "Iteration 390, loss = 0.11277423\n",
            "Iteration 391, loss = 0.11240596\n",
            "Iteration 392, loss = 0.11156564\n",
            "Iteration 393, loss = 0.11107999\n",
            "Iteration 394, loss = 0.11046291\n",
            "Iteration 395, loss = 0.11001008\n",
            "Iteration 396, loss = 0.10936851\n",
            "Iteration 397, loss = 0.10889920\n",
            "Iteration 398, loss = 0.10877892\n",
            "Iteration 399, loss = 0.10787585\n",
            "Iteration 400, loss = 0.10758967\n",
            "Iteration 401, loss = 0.10709639\n",
            "Iteration 402, loss = 0.10670580\n",
            "Iteration 403, loss = 0.10599974\n",
            "Iteration 404, loss = 0.10564159\n",
            "Iteration 405, loss = 0.10516662\n",
            "Iteration 406, loss = 0.10467317\n",
            "Iteration 407, loss = 0.10437789\n",
            "Iteration 408, loss = 0.10399271\n",
            "Iteration 409, loss = 0.10330642\n",
            "Iteration 410, loss = 0.10295924\n",
            "Iteration 411, loss = 0.10269579\n",
            "Iteration 412, loss = 0.10235680\n",
            "Iteration 413, loss = 0.10160612\n",
            "Iteration 414, loss = 0.10114280\n",
            "Iteration 415, loss = 0.10079283\n",
            "Iteration 416, loss = 0.10056587\n",
            "Iteration 417, loss = 0.10018234\n",
            "Iteration 418, loss = 0.09961802\n",
            "Iteration 419, loss = 0.09909213\n",
            "Iteration 420, loss = 0.09878677\n",
            "Iteration 421, loss = 0.09834359\n",
            "Iteration 422, loss = 0.09788209\n",
            "Iteration 423, loss = 0.09746103\n",
            "Iteration 424, loss = 0.09718798\n",
            "Iteration 425, loss = 0.09674714\n",
            "Iteration 426, loss = 0.09650426\n",
            "Iteration 427, loss = 0.09596586\n",
            "Iteration 428, loss = 0.09564033\n",
            "Iteration 429, loss = 0.09527874\n",
            "Iteration 430, loss = 0.09513985\n",
            "Iteration 431, loss = 0.09476670\n",
            "Iteration 432, loss = 0.09451465\n",
            "Iteration 433, loss = 0.09377179\n",
            "Iteration 434, loss = 0.09364658\n",
            "Iteration 435, loss = 0.09293438\n",
            "Iteration 436, loss = 0.09267861\n",
            "Iteration 437, loss = 0.09244154\n",
            "Iteration 438, loss = 0.09216373\n",
            "Iteration 439, loss = 0.09169959\n",
            "Iteration 440, loss = 0.09176033\n",
            "Iteration 441, loss = 0.09099421\n",
            "Iteration 442, loss = 0.09052710\n",
            "Iteration 443, loss = 0.09037207\n",
            "Iteration 444, loss = 0.08992872\n",
            "Iteration 445, loss = 0.09010768\n",
            "Iteration 446, loss = 0.08943495\n",
            "Iteration 447, loss = 0.08920753\n",
            "Iteration 448, loss = 0.08881382\n",
            "Iteration 449, loss = 0.08861601\n",
            "Iteration 450, loss = 0.08815504\n",
            "Iteration 451, loss = 0.08786461\n",
            "Iteration 452, loss = 0.08756526\n",
            "Iteration 453, loss = 0.08714731\n",
            "Iteration 454, loss = 0.08716622\n",
            "Iteration 455, loss = 0.08662429\n",
            "Iteration 456, loss = 0.08660014\n",
            "Iteration 457, loss = 0.08626922\n",
            "Iteration 458, loss = 0.08570512\n",
            "Iteration 459, loss = 0.08560978\n",
            "Iteration 460, loss = 0.08534880\n",
            "Iteration 461, loss = 0.08483894\n",
            "Iteration 462, loss = 0.08462058\n",
            "Iteration 463, loss = 0.08453406\n",
            "Iteration 464, loss = 0.08413390\n",
            "Iteration 465, loss = 0.08368898\n",
            "Iteration 466, loss = 0.08345192\n",
            "Iteration 467, loss = 0.08337324\n",
            "Iteration 468, loss = 0.08292090\n",
            "Iteration 469, loss = 0.08287734\n",
            "Iteration 470, loss = 0.08234768\n",
            "Iteration 471, loss = 0.08216899\n",
            "Iteration 472, loss = 0.08200204\n",
            "Iteration 473, loss = 0.08188041\n",
            "Iteration 474, loss = 0.08148739\n",
            "Iteration 475, loss = 0.08126944\n",
            "Iteration 476, loss = 0.08107856\n",
            "Iteration 477, loss = 0.08077755\n",
            "Iteration 478, loss = 0.08029934\n",
            "Iteration 479, loss = 0.08012933\n",
            "Iteration 480, loss = 0.08006436\n",
            "Iteration 481, loss = 0.07964617\n",
            "Iteration 482, loss = 0.07934283\n",
            "Iteration 483, loss = 0.07912141\n",
            "Iteration 484, loss = 0.07900277\n",
            "Iteration 485, loss = 0.07881366\n",
            "Iteration 486, loss = 0.07843932\n",
            "Iteration 487, loss = 0.07829445\n",
            "Iteration 488, loss = 0.07797311\n",
            "Iteration 489, loss = 0.07792614\n",
            "Iteration 490, loss = 0.07756126\n",
            "Iteration 491, loss = 0.07720769\n",
            "Iteration 492, loss = 0.07722301\n",
            "Iteration 493, loss = 0.07690814\n",
            "Iteration 494, loss = 0.07674216\n",
            "Iteration 495, loss = 0.07660402\n",
            "Iteration 496, loss = 0.07613300\n",
            "Iteration 497, loss = 0.07622922\n",
            "Iteration 498, loss = 0.07572659\n",
            "Iteration 499, loss = 0.07549323\n",
            "Iteration 500, loss = 0.07535872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(250,), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "Wi_0y1C6e9Er",
        "colab_type": "code",
        "outputId": "422fc9e4-cf70-406d-91e6-bbcb82eda1df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7352, 374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "oy5MNJqFys-H",
        "colab_type": "code",
        "outputId": "fa07680e-7fc5-4e7d-d0f4-2cc5756fae18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9896276143513008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "T0aiYNsdyuBR",
        "colab_type": "code",
        "outputId": "bc92176a-85e9-4fc8-cf57-2351981a0637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9843643779741672"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "w7DSKjQcyvL0",
        "colab_type": "code",
        "outputId": "0978728f-5a60-4aae-da90-d8292bfc70f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9416355615880556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "bOJLm3y6dkCs",
        "colab_type": "code",
        "outputId": "88c31d42-dadf-499f-86d9-090db6dd9e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(train_data,train_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9906478490052713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "hnUFUI4rdjwi",
        "colab_type": "code",
        "outputId": "0424cccf-0cf2-4cba-b39c-bc30a5bbb6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(validation_data,validation_label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9891230455472467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "1u7u5mLsirGr",
        "colab_type": "code",
        "outputId": "c5db740a-8234-4d85-81ac-ef7864aaa333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9416355615880556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "BtNN_G2Bdkpr",
        "colab_type": "code",
        "outputId": "60301e97-2cb2-47a0-c675-306cd5fd315f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(test_data,test_label)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9504580929759077"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "63SIF9YCiJ9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9T8IoNGpkVv_",
        "colab_type": "code",
        "outputId": "d9f33970-f06f-4bc8-fdfd-c14bca2fefba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2947, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "kxWBf1tjiJm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYHOkr0CiOxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDauxfwliQZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "\n",
        "\n",
        "# saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlClhIpViZoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Divide valid in two parts for validation and validation-test"
      ]
    },
    {
      "metadata": {
        "id": "NhtjHgUwl0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLrSUA61iQW_",
        "colab_type": "code",
        "outputId": "8e52b68a-29ef-477d-f109-28f8e9a250c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(train_label_one_hot,axis = 1))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 981.,    0.,  858.,    0.,  789.,    0., 1029.,    0., 1099.,\n",
              "        1125.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEmd+j9tO4uX1fDiHQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XfNKrbvrmDl6",
        "colab_type": "code",
        "outputId": "f53b11f5-5c4e-4316-d4af-4a123069ab37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([245.,   0., 215.,   0., 197.,   0., 257.,   0., 275., 282.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fX0eodcgiQTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYJZs7GgCy5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle  #train_data, train_label\n",
        "\n",
        "X_train, y_train = shuffle(train_data, train_label_one_hot)\n",
        "validation_data, validation_label_one_hot = validation_data, validation_label_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4WFIdiuCy53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup TensorFlow\n",
        "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "est-t83SCy55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vgad6Ny0OjqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "connection_probability = tf.Variable(1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZqUctyopqzZ",
        "colab_type": "code",
        "outputId": "99f7e83b-2cfe-4408-beb1-b9c386eae584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "# print(G_W1.shape)\n",
        "# print(G_W2.shape)\n",
        "# print(G_W3.shape)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5881, 374)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UYVUi6tIf1mG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# len(clf.coefs_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6aNutv83RcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define the network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "stGmwMYz8vws",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "# G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "# G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "# G_w_out_h1 = tf.Variable(xavier_init([10,80]))\n",
        "# G_b_out_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "# G_w_h2_h1 = tf.Variable(xavier_init([40,80]))\n",
        "# G_b_h2_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "\n",
        "# G_w_h1_input = tf.Variable(xavier_init([80,16]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([16]))\n",
        "\n",
        "\n",
        "# G_w_input_h1_h2 = tf.Variable(xavier_init([16,40]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([40]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bajK44GmiR1",
        "colab_type": "code",
        "outputId": "703c3676-b211-4e63-a6bb-6368398638f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5881, 374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "hncjRrBbmRDY",
        "colab_type": "code",
        "outputId": "0d3c24e7-a737-40c4-ebe1-72b5ddf54168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.coefs_[0].shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(374, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "-sfSdtHU3JfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x, test_mode = False):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    layer_depth = {\n",
        "        'layer_1' : 6,\n",
        "        'layer_2' : 16,\n",
        "        'layer_3' : 120,\n",
        "        'layer_f1' : 84\n",
        "    }\n",
        "\n",
        "\n",
        "    \n",
        "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
        "    x_flat = flatten(x)\n",
        "    fc1 = flatten(x)\n",
        "    fdense = fc1\n",
        "    \n",
        "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fc1_w = G_W1# tf.Variable(tf.truncated_normal(shape = (X_train.shape[1]*X_train.shape[2],300), mean = mu, stddev = sigma))\n",
        "    fc1_b = G_b1# tf.Variable(tf.zeros(300))\n",
        "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "#     # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "#     fc2_w = G_W2# tf.Variable(tf.truncated_normal(shape = (300,100), mean = mu, stddev = sigma))\n",
        "#     fc2_b = G_b2# tf.Variable(tf.zeros(100))\n",
        "#     fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
        "#     # TODO: Activation.\n",
        "#     fc2 = tf.nn.relu(fc2)\n",
        "    fc3_w = G_W2\n",
        "    fc3_b = G_b2\n",
        "    \n",
        "    logits = tf.matmul(fc1, fc3_w) + fc3_b\n",
        "    \n",
        "    #################\n",
        "    ##### Inset probability connection from x to conv2\n",
        "    fc2p_w = tf.Variable(xavier_init([X_train.shape[1],clf.coefs_[1].shape[1]]))\n",
        "    fc2p_b = tf.Variable(xavier_init([clf.coefs_[1].shape[1]]))\n",
        "    fc2_2nd_input = tf.matmul(x_flat,fc2p_w) + fc2p_b\n",
        "#     fc2_2nd_input = tf.nn.relu(fc2_2nd_input)\n",
        "    connect2 = tf.logical_and(tf.random.uniform(shape = tf.shape(connection_probability)) < connection_probability, tf.equal(test_mode,False))\n",
        "    logits = tf.cond(connect2,lambda: logits + fc2_2nd_input, lambda: logits )    \n",
        "    ################    \n",
        "#     fc3_w = G_W3\n",
        "#     fc3_b = G_b3\n",
        "    \n",
        "#     logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
        "#     print(logits.shape)\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGmN34tg3_tv",
        "colab_type": "code",
        "outputId": "97670e9c-625e-4fa8-8529-7186a03c96cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label_one_hot.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5881, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "-NX_lWUB6zue",
        "colab_type": "code",
        "outputId": "6133cd75-1afb-4b8f-f7a4-2d3200945eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 1, 0, ..., 4, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "M3U_MKYr34Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.name_scope('Input'):\n",
        "\n",
        "  x = tf.placeholder(tf.float32, (None, train_data.shape[1]), name='X')\n",
        "  y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# one_hot_y = tf.one_hot(y, train_label_one_hot.shape[1])\n",
        "is_testing= tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rtTKpeM4P8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-e6BG9DI3Jb3",
        "colab_type": "code",
        "outputId": "da6b5cb2-7986-4047-8685-c30243bc493d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rate = 0.001\n",
        "decay_rate = 1.0005**(X_train.shape[0]/BATCH_SIZE);\n",
        "decay_rate =1.36\n",
        "print(decay_rate)\n",
        "logits = LeNet(x,is_testing)\n",
        "with tf.name_scope('Train'):\n",
        "#   cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
        "#   loss_operation = tf.reduce_mean(cross_entropy, name='loss')\n",
        "  loss_operation = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=LeNet(x, test_mode=False), labels=y))\n",
        "  tf.summary.scalar('loss', loss_operation)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate = rate,momentum=.9)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "# tf.train.natural_exp_decay()\n",
        "training_operation = optimizer.minimize(loss_operation)\n",
        "new_prob = connection_probability.assign(connection_probability/decay_rate)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8eQKHOw7PHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def evaluate(X_data, y_data):\n",
        "correct_pred = tf.equal(tf.argmax(LeNet(x,test_mode=True), 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tb-sFE34OGp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "# accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# saver = tf.train.Saver()\n",
        "\n",
        "# def evaluate(X_data, y_data):\n",
        "#     num_examples = len(X_data)\n",
        "#     total_accuracy = 0\n",
        "#     sess = tf.get_default_session()\n",
        "#     for offset in range(0, num_examples, BATCH_SIZE):\n",
        "#         batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "#         accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, is_testing: True})\n",
        "#         total_accuracy += (accuracy * len(batch_x))\n",
        "#     tot_acc = total_accuracy / num_examples\n",
        "#     with tf.name_scope('Accuracy'):\n",
        "#       tf.summary.scalar('accuracy', tot_acc)\n",
        "#     return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCovfr0E4oJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the mode"
      ]
    },
    {
      "metadata": {
        "id": "NGF2PRgw9nLL",
        "colab_type": "code",
        "outputId": "e4d05ae6-781e-46ef-e718-54e3a9b458ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "2056*2"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cl89uCj9hjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4112"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4H5Dgkf69TOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nk5Kihg685LI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQaEeNO285BK",
        "colab_type": "code",
        "outputId": "e0a570a1-3cf1-49ab-b1c1-a62f769c3d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5881, 374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "metadata": {
        "id": "2kUXlIo82NoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "576a85e2-314a-4db7-bdf2-eb37b5a7b102"
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1471, 374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "metadata": {
        "id": "4VP4QWKJ4ODG",
        "colab_type": "code",
        "outputId": "e4fffdb4-9693-4ac3-ddc6-5a5fe01e45ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9200
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = X_train.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "epoch_track = []\n",
        "print_every = 1\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          epoch_track.append(i)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "            saver.save(sess, './DNAAdamBased')\n",
        "        \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 98.96276\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 98.16452\n",
            "0.7352941\n",
            "\n",
            "Train Accuracy = 98.86074\n",
            "EPOCH 2 ...\n",
            "Validation Accuracy = 97.96057\n",
            "0.5406574\n",
            "\n",
            "Train Accuracy = 98.91175\n",
            "EPOCH 3 ...\n",
            "Validation Accuracy = 98.02855\n",
            "0.3975422\n",
            "\n",
            "Train Accuracy = 98.92876\n",
            "EPOCH 4 ...\n",
            "Validation Accuracy = 98.02855\n",
            "0.29231045\n",
            "\n",
            "Train Accuracy = 98.99677\n",
            "EPOCH 5 ...\n",
            "Validation Accuracy = 98.02855\n",
            "0.21493414\n",
            "\n",
            "Train Accuracy = 98.97977\n",
            "EPOCH 6 ...\n",
            "Validation Accuracy = 98.02855\n",
            "0.15803981\n",
            "\n",
            "Train Accuracy = 99.06479\n",
            "EPOCH 7 ...\n",
            "Validation Accuracy = 98.09653\n",
            "0.11620574\n",
            "\n",
            "Train Accuracy = 99.04778\n",
            "EPOCH 8 ...\n",
            "Validation Accuracy = 98.16452\n",
            "0.0854454\n",
            "\n",
            "Train Accuracy = 99.01378\n",
            "EPOCH 9 ...\n",
            "Validation Accuracy = 98.16452\n",
            "0.0628275\n",
            "\n",
            "Train Accuracy = 99.04778\n",
            "EPOCH 10 ...\n",
            "Validation Accuracy = 98.16452\n",
            "0.046196688\n",
            "\n",
            "Train Accuracy = 99.08179\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 98.23250\n",
            "0.03396815\n",
            "\n",
            "Train Accuracy = 99.11579\n",
            "EPOCH 12 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.024976581\n",
            "\n",
            "Train Accuracy = 99.09879\n",
            "EPOCH 13 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.018365134\n",
            "\n",
            "Train Accuracy = 99.06479\n",
            "EPOCH 14 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.013503774\n",
            "\n",
            "Train Accuracy = 99.04778\n",
            "EPOCH 15 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.009929245\n",
            "\n",
            "Train Accuracy = 99.08179\n",
            "EPOCH 16 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0073009157\n",
            "\n",
            "Train Accuracy = 99.08179\n",
            "EPOCH 17 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0053683203\n",
            "\n",
            "Train Accuracy = 99.11579\n",
            "EPOCH 18 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0039472943\n",
            "\n",
            "Train Accuracy = 99.14980\n",
            "EPOCH 19 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0029024223\n",
            "\n",
            "Train Accuracy = 99.14980\n",
            "EPOCH 20 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.002134134\n",
            "\n",
            "Train Accuracy = 99.14980\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0015692161\n",
            "\n",
            "Train Accuracy = 99.13280\n",
            "EPOCH 22 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0011538353\n",
            "\n",
            "Train Accuracy = 99.13280\n",
            "EPOCH 23 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0008484083\n",
            "\n",
            "Train Accuracy = 99.13280\n",
            "EPOCH 24 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.00062382966\n",
            "\n",
            "Train Accuracy = 99.16681\n",
            "EPOCH 25 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.00045869828\n",
            "\n",
            "Train Accuracy = 99.16681\n",
            "EPOCH 26 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.00033727815\n",
            "\n",
            "Train Accuracy = 99.16681\n",
            "EPOCH 27 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.00024799863\n",
            "\n",
            "Train Accuracy = 99.16681\n",
            "EPOCH 28 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.00018235193\n",
            "\n",
            "Train Accuracy = 99.16681\n",
            "EPOCH 29 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0001340823\n",
            "\n",
            "Train Accuracy = 99.18381\n",
            "EPOCH 30 ...\n",
            "Validation Accuracy = 98.30048\n",
            "9.858992e-05\n",
            "\n",
            "Train Accuracy = 99.18381\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 98.30048\n",
            "7.2492585e-05\n",
            "\n",
            "Train Accuracy = 99.18381\n",
            "EPOCH 32 ...\n",
            "Validation Accuracy = 98.30048\n",
            "5.330337e-05\n",
            "\n",
            "Train Accuracy = 99.18381\n",
            "EPOCH 33 ...\n",
            "Validation Accuracy = 98.30048\n",
            "3.9193656e-05\n",
            "\n",
            "Train Accuracy = 99.18381\n",
            "EPOCH 34 ...\n",
            "Validation Accuracy = 98.30048\n",
            "2.8818864e-05\n",
            "\n",
            "Train Accuracy = 99.20081\n",
            "EPOCH 35 ...\n",
            "Validation Accuracy = 98.30048\n",
            "2.119034e-05\n",
            "\n",
            "Train Accuracy = 99.20081\n",
            "EPOCH 36 ...\n",
            "Validation Accuracy = 98.23250\n",
            "1.5581132e-05\n",
            "\n",
            "Train Accuracy = 99.23483\n",
            "EPOCH 37 ...\n",
            "Validation Accuracy = 98.23250\n",
            "1.1456715e-05\n",
            "\n",
            "Train Accuracy = 99.23483\n",
            "EPOCH 38 ...\n",
            "Validation Accuracy = 98.16452\n",
            "8.424055e-06\n",
            "\n",
            "Train Accuracy = 99.26883\n",
            "EPOCH 39 ...\n",
            "Validation Accuracy = 98.09653\n",
            "6.194158e-06\n",
            "\n",
            "Train Accuracy = 99.30284\n",
            "EPOCH 40 ...\n",
            "Validation Accuracy = 98.09653\n",
            "4.5545275e-06\n",
            "\n",
            "Train Accuracy = 99.26883\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 98.09653\n",
            "3.3489173e-06\n",
            "\n",
            "Train Accuracy = 99.26883\n",
            "EPOCH 42 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.4624392e-06\n",
            "\n",
            "Train Accuracy = 99.25183\n",
            "EPOCH 43 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.810617e-06\n",
            "\n",
            "Train Accuracy = 99.25183\n",
            "EPOCH 44 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.3313361e-06\n",
            "\n",
            "Train Accuracy = 99.25183\n",
            "EPOCH 45 ...\n",
            "Validation Accuracy = 98.16452\n",
            "9.789236e-07\n",
            "\n",
            "Train Accuracy = 99.26883\n",
            "EPOCH 46 ...\n",
            "Validation Accuracy = 98.16452\n",
            "7.1979673e-07\n",
            "\n",
            "Train Accuracy = 99.28584\n",
            "EPOCH 47 ...\n",
            "Validation Accuracy = 98.16452\n",
            "5.292623e-07\n",
            "\n",
            "Train Accuracy = 99.28584\n",
            "EPOCH 48 ...\n",
            "Validation Accuracy = 98.09653\n",
            "3.8916346e-07\n",
            "\n",
            "Train Accuracy = 99.30284\n",
            "EPOCH 49 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.861496e-07\n",
            "\n",
            "Train Accuracy = 99.30284\n",
            "EPOCH 50 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.1040411e-07\n",
            "\n",
            "Train Accuracy = 99.30284\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.547089e-07\n",
            "\n",
            "Train Accuracy = 99.31985\n",
            "EPOCH 52 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.1375654e-07\n",
            "\n",
            "Train Accuracy = 99.30284\n",
            "EPOCH 53 ...\n",
            "Validation Accuracy = 98.09653\n",
            "8.3644515e-08\n",
            "\n",
            "Train Accuracy = 99.31985\n",
            "EPOCH 54 ...\n",
            "Validation Accuracy = 98.09653\n",
            "6.150332e-08\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 55 ...\n",
            "Validation Accuracy = 98.23250\n",
            "4.522303e-08\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 56 ...\n",
            "Validation Accuracy = 98.16452\n",
            "3.3252228e-08\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 57 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.4450166e-08\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 58 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.7978063e-08\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 59 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.3219164e-08\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 60 ...\n",
            "Validation Accuracy = 98.16452\n",
            "9.719973e-09\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 98.16452\n",
            "7.147039e-09\n",
            "\n",
            "Train Accuracy = 99.35385\n",
            "EPOCH 62 ...\n",
            "Validation Accuracy = 98.09653\n",
            "5.2551754e-09\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 63 ...\n",
            "Validation Accuracy = 98.09653\n",
            "3.8640997e-09\n",
            "\n",
            "Train Accuracy = 99.33685\n",
            "EPOCH 64 ...\n",
            "Validation Accuracy = 98.09653\n",
            "2.8412497e-09\n",
            "\n",
            "Train Accuracy = 99.37086\n",
            "EPOCH 65 ...\n",
            "Validation Accuracy = 98.09653\n",
            "2.0891542e-09\n",
            "\n",
            "Train Accuracy = 99.38786\n",
            "EPOCH 66 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.5361428e-09\n",
            "\n",
            "Train Accuracy = 99.37086\n",
            "EPOCH 67 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.1295167e-09\n",
            "\n",
            "Train Accuracy = 99.37086\n",
            "EPOCH 68 ...\n",
            "Validation Accuracy = 98.16452\n",
            "8.30527e-10\n",
            "\n",
            "Train Accuracy = 99.37086\n",
            "EPOCH 69 ...\n",
            "Validation Accuracy = 98.16452\n",
            "6.106816e-10\n",
            "\n",
            "Train Accuracy = 99.38786\n",
            "EPOCH 70 ...\n",
            "Validation Accuracy = 98.09653\n",
            "4.490306e-10\n",
            "\n",
            "Train Accuracy = 99.43887\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 98.09653\n",
            "3.3016956e-10\n",
            "\n",
            "Train Accuracy = 99.43887\n",
            "EPOCH 72 ...\n",
            "Validation Accuracy = 98.09653\n",
            "2.4277172e-10\n",
            "\n",
            "Train Accuracy = 99.43887\n",
            "EPOCH 73 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.7850861e-10\n",
            "\n",
            "Train Accuracy = 99.42187\n",
            "EPOCH 74 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.3125633e-10\n",
            "\n",
            "Train Accuracy = 99.42187\n",
            "EPOCH 75 ...\n",
            "Validation Accuracy = 98.16452\n",
            "9.6512e-11\n",
            "\n",
            "Train Accuracy = 99.42187\n",
            "EPOCH 76 ...\n",
            "Validation Accuracy = 98.16452\n",
            "7.0964706e-11\n",
            "\n",
            "Train Accuracy = 99.42187\n",
            "EPOCH 77 ...\n",
            "Validation Accuracy = 98.09653\n",
            "5.217993e-11\n",
            "\n",
            "Train Accuracy = 99.45587\n",
            "EPOCH 78 ...\n",
            "Validation Accuracy = 98.09653\n",
            "3.8367594e-11\n",
            "\n",
            "Train Accuracy = 99.45587\n",
            "EPOCH 79 ...\n",
            "Validation Accuracy = 98.09653\n",
            "2.8211466e-11\n",
            "\n",
            "Train Accuracy = 99.45587\n",
            "EPOCH 80 ...\n",
            "Validation Accuracy = 98.09653\n",
            "2.0743724e-11\n",
            "\n",
            "Train Accuracy = 99.45587\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.5252738e-11\n",
            "\n",
            "Train Accuracy = 99.45587\n",
            "EPOCH 82 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.1215248e-11\n",
            "\n",
            "Train Accuracy = 99.45587\n",
            "EPOCH 83 ...\n",
            "Validation Accuracy = 98.09653\n",
            "8.246506e-12\n",
            "\n",
            "Train Accuracy = 99.47288\n",
            "EPOCH 84 ...\n",
            "Validation Accuracy = 98.09653\n",
            "6.063607e-12\n",
            "\n",
            "Train Accuracy = 99.48988\n",
            "EPOCH 85 ...\n",
            "Validation Accuracy = 98.09653\n",
            "4.4585347e-12\n",
            "\n",
            "Train Accuracy = 99.48988\n",
            "EPOCH 86 ...\n",
            "Validation Accuracy = 98.09653\n",
            "3.2783342e-12\n",
            "\n",
            "Train Accuracy = 99.48988\n",
            "EPOCH 87 ...\n",
            "Validation Accuracy = 98.09653\n",
            "2.4105399e-12\n",
            "\n",
            "Train Accuracy = 99.48988\n",
            "EPOCH 88 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.7724558e-12\n",
            "\n",
            "Train Accuracy = 99.48988\n",
            "EPOCH 89 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.3032763e-12\n",
            "\n",
            "Train Accuracy = 99.50688\n",
            "EPOCH 90 ...\n",
            "Validation Accuracy = 98.09653\n",
            "9.582914e-13\n",
            "\n",
            "Train Accuracy = 99.50688\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 98.09653\n",
            "7.0462603e-13\n",
            "\n",
            "Train Accuracy = 99.48988\n",
            "EPOCH 92 ...\n",
            "Validation Accuracy = 98.09653\n",
            "5.1810737e-13\n",
            "\n",
            "Train Accuracy = 99.48988\n",
            "EPOCH 93 ...\n",
            "Validation Accuracy = 98.09653\n",
            "3.809613e-13\n",
            "\n",
            "Train Accuracy = 99.50688\n",
            "EPOCH 94 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.801186e-13\n",
            "\n",
            "Train Accuracy = 99.50688\n",
            "EPOCH 95 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.0596955e-13\n",
            "\n",
            "Train Accuracy = 99.50688\n",
            "EPOCH 96 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.5144819e-13\n",
            "\n",
            "Train Accuracy = 99.50688\n",
            "EPOCH 97 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.1135896e-13\n",
            "\n",
            "Train Accuracy = 99.50688\n",
            "EPOCH 98 ...\n",
            "Validation Accuracy = 98.16452\n",
            "8.1881584e-14\n",
            "\n",
            "Train Accuracy = 99.52389\n",
            "EPOCH 99 ...\n",
            "Validation Accuracy = 98.16452\n",
            "6.020705e-14\n",
            "\n",
            "Train Accuracy = 99.52389\n",
            "EPOCH 100 ...\n",
            "Validation Accuracy = 98.16452\n",
            "4.4269886e-14\n",
            "\n",
            "Train Accuracy = 99.54089\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 98.16452\n",
            "3.2551385e-14\n",
            "\n",
            "Train Accuracy = 99.54089\n",
            "EPOCH 102 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.393484e-14\n",
            "\n",
            "Train Accuracy = 99.54089\n",
            "EPOCH 103 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.7599147e-14\n",
            "\n",
            "Train Accuracy = 99.54089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-c0250b5638f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_testing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# evaluate(X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy = {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalidation_label_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_testing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#evaluate(X_validation, y_validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0mvalidation_accuracy_track\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mtrain_accuracy_track\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zydEA48dDeNA",
        "colab_type": "code",
        "outputId": "e521225d-639d-4819-ac4b-e27cc28ebefe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.argmax(validation_accuracy_track)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "oDevMH2N3JZu",
        "colab_type": "code",
        "outputId": "51655c12-91b8-4e74-dbc2-2ef4052c4bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "best_accuracy_valid"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.43644"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "uhbuJEM0-sVm",
        "colab_type": "code",
        "outputId": "3f486354-6c89-4f75-e00a-c291028fbcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './DNAAdamBased')\n",
        "    saver.save(sess, './HarReducedAdam')\n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./DNAAdamBased\n",
            "Validation Accuracy = 98.436440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D5okZYa4CSqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import spline\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ou2-UqDCXZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, 4861, print_every)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6G17qZY_Pxq",
        "colab_type": "code",
        "outputId": "5e591992-2e97-4187-9d1f-f9105410c7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(validation_accuracy_track)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "sNS-YE0XCg4s",
        "colab_type": "code",
        "outputId": "374db3e9-2789-49d5-d984-00662dc761f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "# plt.plot( savgol_filter(np.asarray(validation_accuracy_track),51,1))\n",
        "plt.plot( (validation_accuracy_track))\n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f005aaec438>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZIgo9EVEhohCX0RkiCj0RUSGyP8HFMtaAVyTLXMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BBlFJwfn-45J",
        "colab_type": "code",
        "outputId": "262fd825-07e8-4b0f-9dff-08e82a4940f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot( (connection_probability_track))\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f005c999c18>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIIe7mVkGOdzNzDLI4W5mlkH/H/Vg+ygy7V6WAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MCKAiyEFCg2F",
        "colab_type": "code",
        "outputId": "8f6e57dc-2763-4a93-efb2-4e2bf16c1bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "steps_plot = epoch_track# [step for step in range(0, 2291, print_every)]\n",
        "# plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "# plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "plt.plot(steps_plot, np.asarray(train_accuracy_track))  \n",
        "plt.plot(steps_plot, validation_accuracy_track)\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEYCAYAAAAwH9PuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwGaMMSak/H/RXlKYoeZ60QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u7hYf8U_CXWh",
        "colab_type": "code",
        "outputId": "c5eff9aa-903c-4827-f84d-9d6437c67baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "valid_accuracy_filtered = validation_accuracy_track\n",
        "print(max(valid_accuracy_filtered))\n",
        "valid_accuracy_filtered_np = np.asarray(valid_accuracy_filtered)\n",
        "print(np.argmax(valid_accuracy_filtered))\n",
        "print(steps_plot[np.argmax(valid_accuracy_filtered)])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.43644\n",
            "13\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKucNn6bEZzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lrwM4UUCXTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('HarReducedDatasetADAM_ProbabilityBasedValid98p44.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "                                       'train_accuracy_track':train_accuracy_track,\n",
        "                                       'connection_probability_track':connection_probability_track,\n",
        "                                       'epochTrack':epoch_track, 'TestAcc':'test_accuracy',\n",
        "                                                         'BestValidation':best_accuracy_valid})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFCL8d7-CIxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now  retrain til 24 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "qoqMxUbZCIRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 1\n",
        "aside_valid_test = train_valid_combined[-aside_examples:]\n",
        "aside_valid_test_label = validation_test_label_one_hot[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = validation_test_label_one_hot[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5n7bYp9-FCn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puoBTQ3fCIOT",
        "colab_type": "code",
        "outputId": "c1442690-65ee-4089-c90d-14d1f4536e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2108
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "\n",
        "print_every = 1\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(combined_train_valid, combined_train_valid_label)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: aside_valid_test,y:aside_valid_test_label, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "#             saver.save(sess, './PendigitSGDBased')\n",
        "    saver.save(sess, './DNAAdamBasedAllPass')   \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 98.77568\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.7352941\n",
            "\n",
            "Train Accuracy = 98.72126\n",
            "EPOCH 2 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.5406574\n",
            "\n",
            "Train Accuracy = 98.73487\n",
            "EPOCH 3 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.3975422\n",
            "\n",
            "Train Accuracy = 98.78928\n",
            "EPOCH 4 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.29231045\n",
            "\n",
            "Train Accuracy = 98.84370\n",
            "EPOCH 5 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.21493414\n",
            "\n",
            "Train Accuracy = 98.87090\n",
            "EPOCH 6 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.15803981\n",
            "\n",
            "Train Accuracy = 98.92532\n",
            "EPOCH 7 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.11620574\n",
            "\n",
            "Train Accuracy = 98.99334\n",
            "EPOCH 8 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0854454\n",
            "\n",
            "Train Accuracy = 98.96613\n",
            "EPOCH 9 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0628275\n",
            "\n",
            "Train Accuracy = 98.99334\n",
            "EPOCH 10 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.046196688\n",
            "\n",
            "Train Accuracy = 99.00694\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.03396815\n",
            "\n",
            "Train Accuracy = 99.00694\n",
            "EPOCH 12 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.024976581\n",
            "\n",
            "Train Accuracy = 99.02054\n",
            "EPOCH 13 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.018365134\n",
            "\n",
            "Train Accuracy = 98.99334\n",
            "EPOCH 14 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.013503774\n",
            "\n",
            "Train Accuracy = 99.02054\n",
            "EPOCH 15 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.009929245\n",
            "\n",
            "Train Accuracy = 99.04775\n",
            "EPOCH 16 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0073009157\n",
            "\n",
            "Train Accuracy = 99.06136\n",
            "EPOCH 17 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0053683203\n",
            "\n",
            "Train Accuracy = 99.06136\n",
            "EPOCH 18 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0039472943\n",
            "\n",
            "Train Accuracy = 99.04775\n",
            "EPOCH 19 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0029024223\n",
            "\n",
            "Train Accuracy = 99.06136\n",
            "EPOCH 20 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.002134134\n",
            "\n",
            "Train Accuracy = 99.04775\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0015692161\n",
            "\n",
            "Train Accuracy = 99.04775\n",
            "EPOCH 22 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0011538353\n",
            "\n",
            "Train Accuracy = 99.07495\n",
            "EPOCH 23 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0008484083\n",
            "\n",
            "Train Accuracy = 99.07495\n",
            "EPOCH 24 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.00062382966\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ImtSTCNJF9E_",
        "colab_type": "code",
        "outputId": "c1bf1f39-772a-46b3-fc95-376a0f49b0ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './DNAAdamBasedAllPass')\n",
        "    saver.save(sess, './HarReducedAdamAllPass')  \n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./DNAAdamBasedAllPass\n",
            "Validation Accuracy = 98.912308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FVMAeXpCIL8",
        "colab_type": "code",
        "outputId": "efc4ee8b-fa06-405c-a3b2-915848091510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Without all pass\n",
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './HarReducedAdamAllPass')\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "    print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./HarReducedAdamAllPass\n",
            "Test Accuracy = 95.147614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VGUWHQR3CIJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HXCpiAuCIGT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGYzlt7KCIDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hr57zkSqCIAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q08ZXCdr-sTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Without all pass\n",
        "# with tf.Session() as sess:\n",
        "# #     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "#     saver.restore(sess, './PendigitSGDBased')\n",
        "#     test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "#     print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9z1P1DG-sQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW6V7O1e-sNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochTrack = epoch_track\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r7lCgbXR3JXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFfzfjOB3JTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}