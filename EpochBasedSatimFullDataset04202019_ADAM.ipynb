{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EpochBasedSatimFullDataset04202019_ADAM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/EpochBasedSatimFullDataset04202019_ADAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0SINGreLFCRz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import packages"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "My4EmvydE3bW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dTAGPqvlFEuQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ovvpmlXeFH1x",
        "outputId": "1fed3ee2-676f-482d-b357-9c844310362d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "train_data_pandas = pd.DataFrame(train_data)\n",
        "train_data_labels = pd.DataFrame(train_label)\n",
        "train_data_pandas.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.127273</td>\n",
              "      <td>-0.095238</td>\n",
              "      <td>-0.289256</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.054545</td>\n",
              "      <td>-0.157895</td>\n",
              "      <td>-0.265625</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.106796</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>-0.138462</td>\n",
              "      <td>-0.188119</td>\n",
              "      <td>-0.431579</td>\n",
              "      <td>-0.546875</td>\n",
              "      <td>-0.15625</td>\n",
              "      <td>-0.126214</td>\n",
              "      <td>-0.431579</td>\n",
              "      <td>-0.484375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.090909</td>\n",
              "      <td>-0.571429</td>\n",
              "      <td>-0.603306</td>\n",
              "      <td>-0.096774</td>\n",
              "      <td>-0.090909</td>\n",
              "      <td>-0.494737</td>\n",
              "      <td>-0.562500</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.106796</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.517241</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>-0.015385</td>\n",
              "      <td>-0.049505</td>\n",
              "      <td>-0.431579</td>\n",
              "      <td>-0.609375</td>\n",
              "      <td>-0.15625</td>\n",
              "      <td>-0.126214</td>\n",
              "      <td>-0.494737</td>\n",
              "      <td>-0.609375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5625</td>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.074380</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.327273</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>-0.187500</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>0.242718</td>\n",
              "      <td>...</td>\n",
              "      <td>0.103448</td>\n",
              "      <td>-0.233333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.207921</td>\n",
              "      <td>-0.010526</td>\n",
              "      <td>-0.312500</td>\n",
              "      <td>-0.15625</td>\n",
              "      <td>0.009709</td>\n",
              "      <td>-0.326316</td>\n",
              "      <td>-0.437500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.018182</td>\n",
              "      <td>-0.380952</td>\n",
              "      <td>-0.471074</td>\n",
              "      <td>-0.225806</td>\n",
              "      <td>-0.163636</td>\n",
              "      <td>-0.410526</td>\n",
              "      <td>-0.437500</td>\n",
              "      <td>-0.3750</td>\n",
              "      <td>-0.242718</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011494</td>\n",
              "      <td>-0.383333</td>\n",
              "      <td>-0.138462</td>\n",
              "      <td>-0.049505</td>\n",
              "      <td>-0.347368</td>\n",
              "      <td>-0.484375</td>\n",
              "      <td>0.09375</td>\n",
              "      <td>0.087379</td>\n",
              "      <td>-0.031579</td>\n",
              "      <td>-0.218750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.018182</td>\n",
              "      <td>-0.285714</td>\n",
              "      <td>-0.471074</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>-0.090909</td>\n",
              "      <td>-0.326316</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>-0.1250</td>\n",
              "      <td>-0.184466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.287356</td>\n",
              "      <td>-0.183333</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.267327</td>\n",
              "      <td>-0.031579</td>\n",
              "      <td>-0.281250</td>\n",
              "      <td>-0.03125</td>\n",
              "      <td>-0.126214</td>\n",
              "      <td>-0.431579</td>\n",
              "      <td>-0.546875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2         3         4         5         6   \\\n",
              "0  0.0000  0.127273 -0.095238 -0.289256  0.032258  0.054545 -0.157895   \n",
              "1  0.0000 -0.090909 -0.571429 -0.603306 -0.096774 -0.090909 -0.494737   \n",
              "2  0.5625  0.490909  0.333333 -0.074380  0.354839  0.327273  0.052632   \n",
              "3  0.0000 -0.018182 -0.380952 -0.471074 -0.225806 -0.163636 -0.410526   \n",
              "4  0.0000 -0.018182 -0.285714 -0.471074  0.032258 -0.090909 -0.326316   \n",
              "\n",
              "         7       8         9     ...           26        27        28  \\\n",
              "0 -0.265625 -0.2500 -0.106796    ...    -0.517241 -0.600000 -0.138462   \n",
              "1 -0.562500 -0.2500 -0.106796    ...    -0.517241 -0.600000 -0.015385   \n",
              "2 -0.187500  0.1875  0.242718    ...     0.103448 -0.233333  0.200000   \n",
              "3 -0.437500 -0.3750 -0.242718    ...    -0.011494 -0.383333 -0.138462   \n",
              "4 -0.500000 -0.1250 -0.184466    ...     0.287356 -0.183333  0.230769   \n",
              "\n",
              "         29        30        31       32        33        34        35  \n",
              "0 -0.188119 -0.431579 -0.546875 -0.15625 -0.126214 -0.431579 -0.484375  \n",
              "1 -0.049505 -0.431579 -0.609375 -0.15625 -0.126214 -0.494737 -0.609375  \n",
              "2  0.207921 -0.010526 -0.312500 -0.15625  0.009709 -0.326316 -0.437500  \n",
              "3 -0.049505 -0.347368 -0.484375  0.09375  0.087379 -0.031579 -0.218750  \n",
              "4  0.267327 -0.031579 -0.281250 -0.03125 -0.126214 -0.431579 -0.546875  \n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "epqBn1YjFlII",
        "outputId": "8de0fae7-e6f2-4365-85fb-35fbdacd8803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "train_data_labels.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  5\n",
              "1  5\n",
              "2  5\n",
              "3  5\n",
              "4  5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ewLyg3iuFqkO",
        "outputId": "550d6c62-9cd3-4a83-e4ad-3a50d4479f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w5wMHmhIFthO",
        "outputId": "df22b4c8-0465-4708-8ea9-dafd1fa1324b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1331, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5Jg0BONTGBA1"
      },
      "cell_type": "markdown",
      "source": [
        "#### Combine Validation and train data for MLP classifier - and set validation fraction to 4500/15000 = 0.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8krXltl9GPfv",
        "outputId": "937dc328-1776-4281-b4a0-e2628b06c11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_combined.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4435, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NUWNzsz4v04T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bWN_sAWEFNtb"
      },
      "cell_type": "markdown",
      "source": [
        "#### Fit MLP Classifier"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QSdXJQLnFKa2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clf = MLPClassifier(hidden_layer_sizes=(104),validation_fraction=0.3)\n",
        "# clf.fit(train_data, train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnBnT6NdTqyO",
        "colab_type": "code",
        "outputId": "6579d730-51c4-49b7-e496-14b64718732e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "20*90/36"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xfKh_nDUvj5G",
        "outputId": "c51490a0-219e-4bf2-8e75-2c01da607b6b",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3590
        }
      },
      "cell_type": "code",
      "source": [
        "clf =MLPClassifier(hidden_layer_sizes=(90, ), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "\n",
        "# Test set score: 0.950119\n",
        "\n",
        "# clf.fit(train_valid_combined, train_valid_label)\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.95444174\n",
            "Iteration 2, loss = 0.41361696\n",
            "Iteration 3, loss = 0.37376292\n",
            "Iteration 4, loss = 0.35310299\n",
            "Iteration 5, loss = 0.33669458\n",
            "Iteration 6, loss = 0.32397037\n",
            "Iteration 7, loss = 0.31519296\n",
            "Iteration 8, loss = 0.30943933\n",
            "Iteration 9, loss = 0.30110286\n",
            "Iteration 10, loss = 0.29639559\n",
            "Iteration 11, loss = 0.29181714\n",
            "Iteration 12, loss = 0.28519907\n",
            "Iteration 13, loss = 0.27945018\n",
            "Iteration 14, loss = 0.27446988\n",
            "Iteration 15, loss = 0.27230486\n",
            "Iteration 16, loss = 0.26854954\n",
            "Iteration 17, loss = 0.26347512\n",
            "Iteration 18, loss = 0.25999897\n",
            "Iteration 19, loss = 0.25824005\n",
            "Iteration 20, loss = 0.25456133\n",
            "Iteration 21, loss = 0.24952501\n",
            "Iteration 22, loss = 0.24607191\n",
            "Iteration 23, loss = 0.24673228\n",
            "Iteration 24, loss = 0.24442729\n",
            "Iteration 25, loss = 0.24115307\n",
            "Iteration 26, loss = 0.23827513\n",
            "Iteration 27, loss = 0.23520357\n",
            "Iteration 28, loss = 0.23354455\n",
            "Iteration 29, loss = 0.22978524\n",
            "Iteration 30, loss = 0.22810795\n",
            "Iteration 31, loss = 0.22567502\n",
            "Iteration 32, loss = 0.22661689\n",
            "Iteration 33, loss = 0.22176630\n",
            "Iteration 34, loss = 0.22185255\n",
            "Iteration 35, loss = 0.21909550\n",
            "Iteration 36, loss = 0.21857916\n",
            "Iteration 37, loss = 0.21802408\n",
            "Iteration 38, loss = 0.21273616\n",
            "Iteration 39, loss = 0.21271724\n",
            "Iteration 40, loss = 0.21180686\n",
            "Iteration 41, loss = 0.20911150\n",
            "Iteration 42, loss = 0.20824662\n",
            "Iteration 43, loss = 0.20727753\n",
            "Iteration 44, loss = 0.20444614\n",
            "Iteration 45, loss = 0.20458268\n",
            "Iteration 46, loss = 0.20322771\n",
            "Iteration 47, loss = 0.20093423\n",
            "Iteration 48, loss = 0.19972227\n",
            "Iteration 49, loss = 0.19969319\n",
            "Iteration 50, loss = 0.19881669\n",
            "Iteration 51, loss = 0.19614789\n",
            "Iteration 52, loss = 0.19417483\n",
            "Iteration 53, loss = 0.19168104\n",
            "Iteration 54, loss = 0.19479601\n",
            "Iteration 55, loss = 0.19010506\n",
            "Iteration 56, loss = 0.19106383\n",
            "Iteration 57, loss = 0.19142207\n",
            "Iteration 58, loss = 0.18865416\n",
            "Iteration 59, loss = 0.18610193\n",
            "Iteration 60, loss = 0.18574850\n",
            "Iteration 61, loss = 0.18561203\n",
            "Iteration 62, loss = 0.18429022\n",
            "Iteration 63, loss = 0.18052444\n",
            "Iteration 64, loss = 0.18136414\n",
            "Iteration 65, loss = 0.18031222\n",
            "Iteration 66, loss = 0.17744159\n",
            "Iteration 67, loss = 0.18027992\n",
            "Iteration 68, loss = 0.17645335\n",
            "Iteration 69, loss = 0.17403393\n",
            "Iteration 70, loss = 0.17429126\n",
            "Iteration 71, loss = 0.17618357\n",
            "Iteration 72, loss = 0.17183623\n",
            "Iteration 73, loss = 0.17166370\n",
            "Iteration 74, loss = 0.17217259\n",
            "Iteration 75, loss = 0.16904945\n",
            "Iteration 76, loss = 0.17088875\n",
            "Iteration 77, loss = 0.17259165\n",
            "Iteration 78, loss = 0.16779362\n",
            "Iteration 79, loss = 0.16512526\n",
            "Iteration 80, loss = 0.16614422\n",
            "Iteration 81, loss = 0.16629507\n",
            "Iteration 82, loss = 0.16287885\n",
            "Iteration 83, loss = 0.16415537\n",
            "Iteration 84, loss = 0.16234924\n",
            "Iteration 85, loss = 0.16424623\n",
            "Iteration 86, loss = 0.15978972\n",
            "Iteration 87, loss = 0.16143070\n",
            "Iteration 88, loss = 0.15866024\n",
            "Iteration 89, loss = 0.15941105\n",
            "Iteration 90, loss = 0.15681068\n",
            "Iteration 91, loss = 0.15561312\n",
            "Iteration 92, loss = 0.15621932\n",
            "Iteration 93, loss = 0.15605395\n",
            "Iteration 94, loss = 0.15510723\n",
            "Iteration 95, loss = 0.15339714\n",
            "Iteration 96, loss = 0.15174013\n",
            "Iteration 97, loss = 0.15426171\n",
            "Iteration 98, loss = 0.15044099\n",
            "Iteration 99, loss = 0.15172792\n",
            "Iteration 100, loss = 0.14968459\n",
            "Iteration 101, loss = 0.14847401\n",
            "Iteration 102, loss = 0.14847588\n",
            "Iteration 103, loss = 0.14778554\n",
            "Iteration 104, loss = 0.14824874\n",
            "Iteration 105, loss = 0.14742960\n",
            "Iteration 106, loss = 0.14452012\n",
            "Iteration 107, loss = 0.14947363\n",
            "Iteration 108, loss = 0.14322124\n",
            "Iteration 109, loss = 0.14478542\n",
            "Iteration 110, loss = 0.14181720\n",
            "Iteration 111, loss = 0.14135020\n",
            "Iteration 112, loss = 0.14072554\n",
            "Iteration 113, loss = 0.13985860\n",
            "Iteration 114, loss = 0.13897508\n",
            "Iteration 115, loss = 0.13912146\n",
            "Iteration 116, loss = 0.13914079\n",
            "Iteration 117, loss = 0.13889620\n",
            "Iteration 118, loss = 0.13568594\n",
            "Iteration 119, loss = 0.13788100\n",
            "Iteration 120, loss = 0.13676511\n",
            "Iteration 121, loss = 0.13544847\n",
            "Iteration 122, loss = 0.13301428\n",
            "Iteration 123, loss = 0.13581044\n",
            "Iteration 124, loss = 0.13166328\n",
            "Iteration 125, loss = 0.13556177\n",
            "Iteration 126, loss = 0.13061816\n",
            "Iteration 127, loss = 0.12774804\n",
            "Iteration 128, loss = 0.13414609\n",
            "Iteration 129, loss = 0.12788896\n",
            "Iteration 130, loss = 0.12703824\n",
            "Iteration 131, loss = 0.12845110\n",
            "Iteration 132, loss = 0.12680860\n",
            "Iteration 133, loss = 0.12745193\n",
            "Iteration 134, loss = 0.12698741\n",
            "Iteration 135, loss = 0.12842388\n",
            "Iteration 136, loss = 0.12634677\n",
            "Iteration 137, loss = 0.12352253\n",
            "Iteration 138, loss = 0.12330926\n",
            "Iteration 139, loss = 0.12610248\n",
            "Iteration 140, loss = 0.12203222\n",
            "Iteration 141, loss = 0.12216357\n",
            "Iteration 142, loss = 0.11924790\n",
            "Iteration 143, loss = 0.11957397\n",
            "Iteration 144, loss = 0.12033362\n",
            "Iteration 145, loss = 0.11740384\n",
            "Iteration 146, loss = 0.11871911\n",
            "Iteration 147, loss = 0.11659951\n",
            "Iteration 148, loss = 0.12022110\n",
            "Iteration 149, loss = 0.11646908\n",
            "Iteration 150, loss = 0.11516562\n",
            "Iteration 151, loss = 0.11565456\n",
            "Iteration 152, loss = 0.11418154\n",
            "Iteration 153, loss = 0.11264107\n",
            "Iteration 154, loss = 0.11497207\n",
            "Iteration 155, loss = 0.11319484\n",
            "Iteration 156, loss = 0.11410818\n",
            "Iteration 157, loss = 0.11406045\n",
            "Iteration 158, loss = 0.10999054\n",
            "Iteration 159, loss = 0.11248917\n",
            "Iteration 160, loss = 0.11442108\n",
            "Iteration 161, loss = 0.11135488\n",
            "Iteration 162, loss = 0.11127527\n",
            "Iteration 163, loss = 0.10752335\n",
            "Iteration 164, loss = 0.10901415\n",
            "Iteration 165, loss = 0.10863796\n",
            "Iteration 166, loss = 0.10790327\n",
            "Iteration 167, loss = 0.10961160\n",
            "Iteration 168, loss = 0.10851879\n",
            "Iteration 169, loss = 0.10602720\n",
            "Iteration 170, loss = 0.10630070\n",
            "Iteration 171, loss = 0.10501613\n",
            "Iteration 172, loss = 0.10583980\n",
            "Iteration 173, loss = 0.10541182\n",
            "Iteration 174, loss = 0.10096666\n",
            "Iteration 175, loss = 0.10404634\n",
            "Iteration 176, loss = 0.10221237\n",
            "Iteration 177, loss = 0.10075696\n",
            "Iteration 178, loss = 0.09931985\n",
            "Iteration 179, loss = 0.10203152\n",
            "Iteration 180, loss = 0.10096311\n",
            "Iteration 181, loss = 0.09975398\n",
            "Iteration 182, loss = 0.09955987\n",
            "Iteration 183, loss = 0.09993594\n",
            "Iteration 184, loss = 0.09770182\n",
            "Iteration 185, loss = 0.09802935\n",
            "Iteration 186, loss = 0.09803881\n",
            "Iteration 187, loss = 0.09618500\n",
            "Iteration 188, loss = 0.09616072\n",
            "Iteration 189, loss = 0.09800732\n",
            "Iteration 190, loss = 0.09534339\n",
            "Iteration 191, loss = 0.09352430\n",
            "Iteration 192, loss = 0.09372701\n",
            "Iteration 193, loss = 0.09739990\n",
            "Iteration 194, loss = 0.09370456\n",
            "Iteration 195, loss = 0.09311714\n",
            "Iteration 196, loss = 0.09235983\n",
            "Iteration 197, loss = 0.09294382\n",
            "Iteration 198, loss = 0.09261564\n",
            "Iteration 199, loss = 0.09151696\n",
            "Iteration 200, loss = 0.09206768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(90,), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lLNA4D0qGxJi"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "02O8VTAoGqnG",
        "outputId": "ef4809c5-9fdd-45ba-cf2f-894029ced4bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9710051546391752"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "whn8u2m5iY7M"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pii8wXXSG1r7"
      },
      "cell_type": "markdown",
      "source": [
        "#### Validation Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SakclBGkGvI0",
        "outputId": "9cfb7b61-5142-456c-d3d7-b7b788a87dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8858001502629602"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VbIkGX5gG5ZG"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QLo_AzFVG3ca",
        "outputId": "5e010968-98b9-4f24-fc26-e05123fa240f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "InLIF676HEES"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow model using weights initialized from numpy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tcBNfKZNG9Pm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ortxRVBMH7W7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z7mWVCDVEgLm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hid_neuron = [90]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LR62GfKJv_6E",
        "outputId": "8332b696-ac41-496f-8c69-a704b29e2cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EylNp0IJONbz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Base NN model in tensor flow"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VzJnI_o2xD5C"
      },
      "cell_type": "markdown",
      "source": [
        "#### 36 -> 90 -> 6"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "73Way2v2Pbys"
      },
      "cell_type": "markdown",
      "source": [
        "## Train baseline model in tensorflow"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L-hUDOm5xClH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IMHh0nROw5O-",
        "outputId": "5ac1353f-2b94-418b-e8ce-5620b7230e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3104, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yV4xtxJLvyNj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wj_W9eCBvyKy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TKQ6nMqMvyJD",
        "outputId": "c4bc9178-21db-4e03-8420-18662d9491ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jy2mQcHAEn20",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf.train.GradientDescentOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eHe01FffvyEJ",
        "outputId": "caa0ef5d-3d2f-498d-d21b-6623a8423fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        }
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "learning_rate = 0.001\n",
        "hid_neuron = [374]\n",
        "num_steps = 20000\n",
        "batch_size = 200\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "#     layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    return out_layer\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X), labels=Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X), 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "  ### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % 1000 == 0:\n",
        "            train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "            print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "            train_losses.append(train_loss)\n",
        "            validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "            if step%1000 == 0:\n",
        "              print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "              print()\n",
        "              if (validation_accuracy >= best_accuracy_valid):\n",
        "                best_accuracy_valid = validation_accuracy\n",
        "                saver.save(sess, './statlog_letter')\n",
        "                test_Accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_Accuracy), \"%\")\n",
        "    print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "\n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-25-4106bc23d37b>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step 0, training loss= 0.09654394, training acc= 96.49999737739563%\n",
            "Validation Accuracy 88.58000946044922 ...\n",
            "\n",
            "step 1000, training loss= 0.074345045, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 2000, training loss= 0.0795926, training acc= 99.00000095367432%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 3000, training loss= 0.09808998, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 4000, training loss= 0.07493803, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 5000, training loss= 0.095866315, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 6000, training loss= 0.07703011, training acc= 98.00000190734863%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 7000, training loss= 0.074125834, training acc= 98.50000143051147%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 8000, training loss= 0.099417955, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 9000, training loss= 0.09222789, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 10000, training loss= 0.08109907, training acc= 96.49999737739563%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 11000, training loss= 0.11149396, training acc= 95.49999833106995%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 12000, training loss= 0.06741232, training acc= 99.00000095367432%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 13000, training loss= 0.109245956, training acc= 95.99999785423279%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 14000, training loss= 0.07493451, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 15000, training loss= 0.090577155, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 16000, training loss= 0.13477787, training acc= 94.49999928474426%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 17000, training loss= 0.07381674, training acc= 98.00000190734863%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 18000, training loss= 0.07977779, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 19000, training loss= 0.102517195, training acc= 95.99999785423279%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "Test acc= 89.450005 %\n",
            "Valid acc= 88.805405 %\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5A_PHV3bS7ui"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8RFK2bW4JZ7w"
      },
      "cell_type": "markdown",
      "source": [
        "#### My model with feedback"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "G5BxkTLzUAok"
      },
      "cell_type": "markdown",
      "source": [
        "## Divide valid in two parts for validation and validation-testÂ¶"
      ]
    },
    {
      "metadata": {
        "id": "mejHTwMYhEzu",
        "colab_type": "code",
        "outputId": "d460e121-362e-43b2-d294-b2f7cab2a827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(validation_data.shape)\n",
        "print(train_data.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1331, 36)\n",
            "(3104, 36)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jVm6nWpSJn1l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data = validation_data[0:1000,:]\n",
        "valid_validation_data_label = validation_label_one_hot[0:1000,:]\n",
        "valid_test_data = validation_data[1000:,:]\n",
        "valid_test_data_label = validation_label_one_hot[1000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wcT7Xaz1KNcU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ydDcWHWsJcJ-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "num_hidden_neurons = 90\n",
        "GwLoop = tf.Variable(xavier_init([output_shape,num_hidden_neurons]))\n",
        "G_bLoop = tf.Variable(tf.zeros(shape=[num_hidden_neurons]))\n",
        "\n",
        "GwLoop2 = tf.Variable(xavier_init([num_hidden_neurons,num_hidden_neurons]))\n",
        "G_bLoop2 = tf.Variable(tf.zeros(shape=[num_hidden_neurons]))\n",
        "\n",
        "GLossW = tf.Variable(xavier_init([output_shape,output_shape]))\n",
        "GLossb= tf.Variable(tf.zeros(shape=[output_shape]))\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xguK-SPLUrkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data = validation_data[0:1000,:]\n",
        "valid_validation_data_label = validation_label_one_hot[0:1000,:]\n",
        "valid_test_data = validation_data[1000:,:]\n",
        "valid_test_data_label = validation_label_one_hot[1000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgzAMkJCXVq2",
        "colab_type": "code",
        "outputId": "2d13a2b3-b642-413e-bd96-945fc5a2006c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data_label"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "qT_XdektXjmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plt.scatter(np.argmax(valid_validation_data_label,axis = 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RS8J8lVeXGoK",
        "colab_type": "code",
        "outputId": "b31c958e-ec75-4cc4-add8-eef14fadf676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(valid_validation_data_label,axis = 1))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([262.,   0.,  89.,   0., 152.,   0., 103.,   0., 165., 229.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6CWpuf8GKxxT4b3UCxMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AJtaOCHUc8Us",
        "colab_type": "code",
        "outputId": "dbc655c1-7905-487a-c5c0-96b3452897ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3104, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "KrEu6ndlUZh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "metadata": {
        "id": "Q5TyGgw4Ub9n",
        "colab_type": "code",
        "outputId": "e31e32a7-da9a-4b35-974a-fcd95ff1688c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91545
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "# hid_neuron = [90]\n",
        "num_steps = 30000\n",
        "batch_size = 2056\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "plot_every = 100\n",
        "number_of_epoch = 4000\n",
        "# learning_rate = 0.001\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "number_of_ex = train_data.shape[0]\n",
        "\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "weights = {\n",
        "    'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "    'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "    'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "}\n",
        "saver = tf.train.Saver()\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "ValidAccuracy_Test_track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "    \n",
        "for wL1 in range(1,7):\n",
        "  for WL2 in range(1,wL1+1):\n",
        "    for WL3 in range(0,2):\n",
        "\n",
        "        wLoss1 = wL1\n",
        "        wLoss2 = WL2\n",
        "        wLoss3 = WL3\n",
        "        loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "        loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "        loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "        loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        ### Initialization and running the model\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            best_accuracy_valid = 0\n",
        "            for ep in range(0,number_of_epoch):\n",
        "              if ep<1000:\n",
        "                learn = .001\n",
        "              elif ep >=1000 and ep <= 2000:\n",
        "                learn = .001\n",
        "              else:\n",
        "                learn = .0001\n",
        "              for step in range(0, total_steps_for_one_pass):\n",
        "\n",
        "                if step>=number_of_ex//batch_size:\n",
        "                  batch_x, batch_y = train_data[step*batch_size:,:],train_label_one_hot[step*batch_size:,:]\n",
        "#                   print(step,'Finishing',step*batch_size )\n",
        "                  step = 0\n",
        "#                   print('finishing')\n",
        "\n",
        "                else:\n",
        "\n",
        "                  start = step*batch_size\n",
        "                  finish = (step+1)*batch_size\n",
        "#                   print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "                  batch_x, batch_y = train_data[step:finish,:],train_label_one_hot[step:finish,:]\n",
        "        #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})              \n",
        "\n",
        "\n",
        "\n",
        "  #                 batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "  #                 sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "              if ep % plot_every == 0:\n",
        "                  train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                  print(\"epoch \" + str(ep) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                  train_losses.append(train_loss)\n",
        "                  validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                  if ep%plot_every == 0:\n",
        "                    print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                    print()\n",
        "                    if (validation_accuracy >= best_accuracy_valid):\n",
        "                      best_accuracy_valid = validation_accuracy\n",
        "                      saver.save(sess, './statimgTrackAdam')\n",
        "                      G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "            print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "            ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "            this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "            W_track.append(this_params)\n",
        "            # code for checking accuracy of valid_test\n",
        "            validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "            ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "            print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "            print(\"=\"*50)\n",
        "            print(\"W1 = {} ...\".format(wLoss1))\n",
        "            print(\"W2 = {} ...\".format(wLoss2))\n",
        "            print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "            print(\"*\"*50)\n",
        "            print(\"=\"*50)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-33-65f5328f8617>:54: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "epoch 0, training loss= 0.5668403, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.05870537, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.048579626, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.042892184, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.039155915, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.036554143, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.034479167, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.03278617, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.031239456, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.029786874, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0283917, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027060062, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025764717, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024520887, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02329624, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02207201, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020852514, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019637393, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01844693, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017176403, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01592644, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015802315, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015668873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015527826, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015378176, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015219638, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015049796, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014872183, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014687873, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014496491, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.014298481, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.014092969, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013881068, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013665467, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013444701, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013222101, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01299754, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012768291, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012540915, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.012308916, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.7232695, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07151215, training acc= 96.46946787834167%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.053633433, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.044830315, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.039142255, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.03474866, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.031559758, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.029271353, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.027582925, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.02615638, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.024937918, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.023803761, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02287338, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.022043636, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.021311957, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.020639744, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.0200236, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019408707, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.018802539, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.018189836, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.017553765, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.01749164, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.017421076, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.017344572, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017262133, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01717243, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017075378, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.016971648, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.016861096, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.016743332, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01661826, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016487524, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016348034, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016201003, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016045557, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015881095, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015710212, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015532493, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015345651, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015153034, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "Valid acc= 90.100006 %\n",
            "Validation Accuracy Test 83.08157348632812 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.40455517, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.062295496, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.05519199, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.050471455, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.046680696, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.043673195, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.04093183, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.03847419, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.036139227, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03398505, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.031822246, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029763881, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027796118, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025861546, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023958385, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022103747, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020299286, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.018544253, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.016866095, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.015306864, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.013848605, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.013704734, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.013553359, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01339475, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.013226725, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01305333, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.01286736, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.012676224, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.012480635, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.012284725, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.0120934015, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.011895087, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.011691548, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.011486277, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.01127728, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.011069123, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.010860398, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.010652426, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.010445904, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.010238607, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.68580627441406 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.3124382, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.073306024, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.058295667, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.050674412, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.046313774, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.042667404, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.03993109, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.037817948, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.03600081, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03437936, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032956176, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.031599343, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030294385, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.02912567, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.028029047, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026940707, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025838807, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.024728058, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02361421, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.022479588, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.021318115, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.021206861, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.021078628, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.020941872, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.020795856, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.020640548, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.020475652, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.020297728, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.020110024, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.019913185, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.019704476, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.019485606, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.019258173, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.019020708, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018774133, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.018518846, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.018258927, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.017990246, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01771505, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01743433, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.89425659179688 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5668403, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.05870537, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.048579626, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.042892184, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.039155915, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.036554143, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.034479167, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.03278617, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.031239456, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.029786874, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0283917, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027060062, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025764717, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024520887, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02329624, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02207201, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020852514, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019637393, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01844693, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017176403, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01592644, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015802315, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015668873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015527826, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015378176, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015219638, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015049796, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014872183, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014687873, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014496491, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.014298481, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.014092969, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013881068, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013665467, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013444701, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013222101, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01299754, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012768291, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012540915, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.012308916, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.2641834, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07071284, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.054207046, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.046359014, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.040991776, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.03709459, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.034341767, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.032123324, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.030259827, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.02862678, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0272224, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.026081106, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025051674, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024089862, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023182945, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022337152, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02151587, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.020713802, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.019919524, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019118283, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01831638, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0182292, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018137451, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01803747, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017930193, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017816115, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017694488, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017562626, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017424563, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017277969, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017122654, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016961195, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016790465, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.01660995, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.01642202, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016226722, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016022865, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015812986, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015595016, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0153703885, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.59214782714844 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.3234269, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06475467, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.05799052, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.05294171, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.049018107, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.04570168, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.0427216, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.03985396, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.03708065, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034403045, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03170696, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029149694, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02672895, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024450324, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.022244236, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.020143906, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.018175505, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.016355557, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0146667035, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.013119876, training acc= 100.0%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.011721132, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.011589807, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.011445707, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2300, training loss= 0.011292767, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.011136776, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.010976747, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.010812976, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.010644687, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.010472153, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.010298835, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.010124117, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.009949069, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.0097727785, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.00959596, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.009417163, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.009238618, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.009060325, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0088828, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.00870476, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.008530194, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0659977, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07394113, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.061715048, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.05559928, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.050957825, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.047383916, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.044351354, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 700, training loss= 0.04213627, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.040214624, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.038229015, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03655054, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.034981295, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.033466123, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.031944428, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.030368505, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.028869597, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.027397782, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.025919551, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.024427667, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.022945134, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.0214861, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.021342797, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.021182803, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2300, training loss= 0.021012053, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.020830864, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.020639237, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.020435316, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.020219775, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.019993668, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.019758107, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.019513823, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.019260472, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018996144, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018726317, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018446784, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.018162617, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017872153, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.017575903, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.017273173, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.016965963, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 83.68580627441406 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.46946406, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.060685873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.052685596, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.047632612, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.043917812, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.041002415, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.038619183, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.036546737, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.034662895, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.03283292, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.031067103, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029358217, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027698582, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.026055885, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024334347, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022701709, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02108655, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019502444, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.017971886, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.016473679, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.015033527, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.014890784, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01473693, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.014574748, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.014405358, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.014229782, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.014041985, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0138515085, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.013655657, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.013453714, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.013248362, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.013038756, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.012824491, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.012604597, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.012384382, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.012162867, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.011939898, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.011709846, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01148138, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.011263011, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0668195, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07092257, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.058764473, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.05164472, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.046729367, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.042365175, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.039350934, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.037086204, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.03528379, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03362468, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032070197, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03068964, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02941618, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.028143106, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026965259, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02585028, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.024740547, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023629038, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02248433, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021313202, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.020127442, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.02000685, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01987416, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.019732203, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01957965, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019416327, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019243605, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.01906176, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018868724, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018665768, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.018455466, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.0182356, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018005062, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017767003, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.017523784, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017273467, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017016679, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01675319, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016486289, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01621493, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5668403, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.05870537, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.048579626, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.042892184, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.039155915, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.036554143, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.034479167, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.03278617, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.031239456, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.029786874, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0283917, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027060062, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025764717, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024520887, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02329624, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02207201, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020852514, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019637393, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01844693, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017176403, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01592644, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015802315, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015668873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015527826, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015378176, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015219638, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015049796, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014872183, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014687873, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014496491, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.014298481, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.014092969, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013881068, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013665467, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013444701, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013222101, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01299754, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012768291, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012540915, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.012308916, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0672833, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06727259, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.054859582, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.046071485, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.04070691, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.037350196, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.034665983, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.032493718, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.030575754, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.029143976, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.027886545, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.026759606, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025725348, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024727406, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023758333, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022839056, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.021947104, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.021049324, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.020143133, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019227903, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018323373, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2100, training loss= 0.01823467, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018132623, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018021395, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017901562, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017773869, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017637318, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017489616, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017333625, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017168736, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 3000, training loss= 0.016994903, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.01681476, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016628457, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016433176, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.01623023, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016018583, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01580014, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015575367, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01534579, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015111487, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.6 %\n",
            "Validation Accuracy Test 84.59214782714844 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.27475658, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06500908, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.058740813, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.053877223, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.04979613, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.046203982, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.042801015, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.039564095, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.036490504, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.033410795, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.030462477, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027692614, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025061158, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.022599455, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.020262597, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.018096102, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.016137278, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.014370205, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1800, training loss= 0.012785725, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.011350985, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01009139, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.009973402, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.009854678, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.009727925, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.009595354, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.009459785, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.009319932, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.009177115, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.009027551, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.008877268, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.0087257745, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.008572165, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.008401214, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.00824163, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.008086805, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.007931745, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.0077790082, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.007626816, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3800, training loss= 0.0074725635, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.007313532, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.59214782714844 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.9017842, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07385203, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.06141808, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.05663681, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.052797075, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04965601, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.04724071, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.04500275, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.042813048, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.04073076, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03861849, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.036651585, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.0347806, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.032978565, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03116956, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.029391175, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.027605271, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.025851516, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.024131779, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.02240635, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2000, training loss= 0.020738691, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2100, training loss= 0.02057273, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2200, training loss= 0.020391878, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2300, training loss= 0.020200923, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2400, training loss= 0.019998636, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019785155, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019559005, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0193246, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.019078199, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018819079, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.018551704, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.018276148, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.0179928, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.01770337, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.017406251, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017104797, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016800135, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.016491424, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016185079, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015875518, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.6 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.40455517, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.062295496, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.05519199, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.050471455, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.046680696, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.043673195, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.04093183, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.03847419, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.036139227, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03398505, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.031822246, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029763881, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027796118, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025861546, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023958385, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022103747, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020299286, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.018544253, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.016866095, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.015306864, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.013848605, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.013704734, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.013553359, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01339475, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.013226725, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01305333, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.01286736, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.012676224, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.012480635, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.012284725, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.0120934015, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.011895087, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.011691548, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.011486277, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.01127728, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.011069123, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.010860398, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.010652426, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.010445904, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.010238607, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.68580627441406 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.9258463, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.072305664, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.060955822, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.053975515, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.04926275, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.045067504, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.042201284, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 700, training loss= 0.039791733, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 800, training loss= 0.037756205, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.036044702, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.034481075, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03292068, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.031406842, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029918658, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.028484255, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02708603, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025685472, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.024275599, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.022883745, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021519182, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.020142188, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.020006686, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019855559, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.019693756, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.019520069, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019335115, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019140212, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.01893526, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018720802, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018497748, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.018266141, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.01802776, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017782696, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017530132, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.017268023, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.01700078, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01672563, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.016449235, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016167331, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015881395, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 83.68580627441406 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.49730694, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.060070783, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05163071, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.046500567, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.04277058, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.039862745, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.037560474, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.035591654, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.033808723, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.032117516, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.030451527, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.02886782, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027327577, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025820779, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024214981, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02267733, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02115018, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019664384, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.018222474, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.016790591, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2000, training loss= 0.015405596, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015267876, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01511859, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.014959553, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.0147929145, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.014617365, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.014433136, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0142441215, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.0140468115, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.013845088, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.013638416, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.013426435, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.0132132135, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.012996093, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.012775658, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.012545874, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.012320644, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012092259, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.011862641, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.011634997, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.9439037, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06950221, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.05786023, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.049361337, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.04438973, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.040993124, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.038433965, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.03632211, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.034352485, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03262973, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.031196246, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029789396, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.028488055, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027282761, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026077915, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024920734, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023763604, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022611966, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02145549, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.02030447, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019149031, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019030323, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018900044, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018760616, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01860892, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018446585, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.0182756, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018093042, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017900255, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01770059, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017492235, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017273456, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017049573, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.01681873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016579749, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016333561, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01608238, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015825946, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015565571, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015301459, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5668403, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.05870537, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.048579626, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.042892184, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.039155915, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.036554143, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.034479167, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.03278617, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.031239456, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.029786874, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0283917, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027060062, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025764717, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024520887, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02329624, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02207201, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020852514, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019637393, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01844693, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017176403, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01592644, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015802315, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015668873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015527826, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015378176, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015219638, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015049796, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014872183, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014687873, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014496491, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.014298481, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.014092969, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013881068, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013665467, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013444701, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013222101, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01299754, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012768291, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012540915, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.012308916, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.9572201, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06601588, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.051447432, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.044555675, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.040207434, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.03708312, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.034290545, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.0321363, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.03041525, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.029009558, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.027830644, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.02674398, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02572614, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024717005, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023750022, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022793021, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.021829052, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.020878935, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01991501, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.018955177, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.017983869, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.017885525, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.017776316, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01765826, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017530842, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017394396, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017249497, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017096581, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.016934967, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.016764212, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01658319, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016394038, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016195545, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.015990386, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.015777305, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015551318, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015322898, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015092739, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014857692, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01461498, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.89425659179688 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.24227606, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06519942, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05919839, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.05432204, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.049986385, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.046098933, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.042421788, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038979452, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.03564184, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.032272696, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.029148033, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.026219856, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.023499254, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.020978246, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.018676076, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.016589696, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.014723386, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.013072881, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.011582522, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.010289209, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.009151435, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.009030143, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.008913912, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.008795447, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.008671612, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.008543481, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.008411781, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.008275054, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.0081376005, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.007998895, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.007859398, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.007719063, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.007582917, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.0074440106, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3400, training loss= 0.0073032468, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.0071629076, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3600, training loss= 0.0070236847, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0068870783, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3800, training loss= 0.0067542307, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.006625292, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 84.59214782714844 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7843717, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.073933385, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.06350749, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.058177225, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05433481, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.051323403, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.048465088, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.045918737, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.043544065, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.041345485, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03918602, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.037041545, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.035002436, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03295546, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03088974, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02890784, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.026951695, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.025047902, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02319629, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021345828, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019531049, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019353906, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019159248, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018953988, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01873752, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01851098, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.01827459, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018028133, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017771125, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01750583, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017236248, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.01696193, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016675036, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016384441, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016091611, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015805062, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015516676, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015219578, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014923195, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014630966, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.3581799, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.063535854, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.056562465, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.051924057, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.04808509, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.044905905, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04211017, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.039497692, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.037002154, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034527678, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03206236, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029732991, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02747772, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025299732, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023228677, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.021198595, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.019245747, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.01740413, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.015654817, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.014061639, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2000, training loss= 0.012612941, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2100, training loss= 0.012474651, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2200, training loss= 0.012328511, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01217529, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2400, training loss= 0.012012722, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2500, training loss= 0.011844279, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2600, training loss= 0.011672215, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0114920335, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2800, training loss= 0.011309426, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2900, training loss= 0.011123251, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3000, training loss= 0.010935952, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3100, training loss= 0.0107429065, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.010542178, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3300, training loss= 0.010338144, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.010136341, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.00993241, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.009730018, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.009516681, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.009315016, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.00912171, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 84.59214782714844 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.8201133, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07139336, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.061498035, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.055481736, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.0512962, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.047976755, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.04495428, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 700, training loss= 0.042168338, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.039875343, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03786155, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.035996594, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.034143377, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.032423776, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.030732637, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02908823, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.027447214, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025801554, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.024174843, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02253251, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020924026, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019363489, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019202221, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019034093, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018854136, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018665262, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018465437, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018256681, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018037483, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017805861, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017566882, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017313644, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.01705392, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016785273, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016509872, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016228339, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015944619, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015654085, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015362523, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015073455, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0147836525, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.4451195, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.061384127, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.0535973, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.048717905, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.04497218, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.042001, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.03954539, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.03732334, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.03528141, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.03333817, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.031460337, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029646982, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02788313, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.02616157, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024378328, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022605676, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020899879, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019250229, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01762467, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.01606767, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.014599171, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.014453801, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.014301101, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01414129, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.013972036, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.013796379, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.013614334, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.013425267, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.013228744, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.013027299, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.012823342, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.012613605, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.012404202, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.012191666, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.011976329, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.011753115, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.011537408, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.011318299, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.011098637, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.010880667, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.84789133, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.070313044, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.060475826, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.05295754, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.04774527, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.043720543, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.0407548, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.038438268, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.036448956, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.034712356, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0331262, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03161236, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030165719, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.02877074, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027419493, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026073428, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.024719918, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023377245, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.022009844, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020649068, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019285189, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019152695, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.0190038, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018845642, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018675806, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018493002, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018302534, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018100457, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017883176, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017661484, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017431848, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.0171949, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.01695166, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.01670164, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016446054, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016184641, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015920272, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0156532, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015383341, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015111761, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5127639, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.059794743, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.051024348, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.045814756, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.042117197, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.039271638, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.036986828, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.03504457, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.033294886, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.03166395, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.030073166, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.028538039, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027061604, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025613569, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024075199, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022598488, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.021139229, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019694302, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.018295866, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.016912142, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.015557532, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015421437, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015275131, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015119624, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.014955474, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.014784644, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.014603642, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0144159645, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014220167, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014019067, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.013810824, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.013597862, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013382529, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013164277, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.012943104, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.012720233, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.012494473, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012265796, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012031842, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.011800666, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.869481, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06742436, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.053474642, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.046917062, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.042763148, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.039774925, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.037050642, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.03490606, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.033106435, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.031611834, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.030255362, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.02894316, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02769345, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.02654188, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02538145, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024230016, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023079736, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.021929232, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.020778356, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019630287, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018482177, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018363873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018236114, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018099017, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017952092, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017794155, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.0176292, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017453609, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017265767, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017066736, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.016857387, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.01664227, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016418912, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016189871, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.015955014, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015712982, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015464512, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015209923, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014950091, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014688412, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5668403, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.05870537, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.048579626, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.042892184, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.039155915, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.036554143, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.034479167, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.03278617, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.031239456, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.029786874, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0283917, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027060062, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025764717, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024520887, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02329624, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02207201, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020852514, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019637393, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01844693, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017176403, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01592644, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015802315, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015668873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015527826, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015378176, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015219638, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015049796, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014872183, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014687873, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014496491, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.014298481, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.014092969, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013881068, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013665467, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013444701, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013222101, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01299754, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012768291, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012540915, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.012308916, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.88750064, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.064073004, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.051858276, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.045397233, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.040624022, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.03734861, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.034695614, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.03255974, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.030786932, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.029367385, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.028067809, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.02690208, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025807982, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024766743, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023743484, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022742083, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02173023, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02072321, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.019679079, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.018673157, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2000, training loss= 0.017669436, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2100, training loss= 0.01756814, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.017454848, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01733214, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017200766, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01706031, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.016909853, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.016752956, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2800, training loss= 0.016588563, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.016415726, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.016234914, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016045902, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.015848015, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.015641628, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.0154279135, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.01520929, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.014983328, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.014752597, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014516942, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014273839, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 83.68580627441406 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.21908325, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.0652986, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.05916903, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.054115932, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.049777143, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.04566907, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.04182645, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038139965, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.03447376, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.031037396, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.027813943, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.02481211, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.022056019, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.019579906, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.017365351, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.015402339, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.013609322, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.012045725, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.010656198, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.009464149, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.008426233, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.008332422, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.008229083, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.00812187, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.008010532, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.007900232, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.007787304, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.007662405, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.007529003, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2900, training loss= 0.007405182, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.0072772843, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.007143649, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.007008065, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.006877024, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.006743795, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.0066068373, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.0064706863, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0063417954, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.0062148296, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0060890964, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 84.59214782714844 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.6963464, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07261177, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.06435927, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.058491766, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.05404774, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.050985843, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04816939, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.045578297, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.043014433, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.040664446, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.038468022, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03630561, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03418216, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.031985134, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02981407, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.027689373, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02561823, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023592386, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021664778, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019750193, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01796328, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.017793903, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.017606493, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.017405426, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017192766, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01696845, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.016738296, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.01650136, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.0162514, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.015996328, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.015734289, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.015469353, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.015202804, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.014931567, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.014658225, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.014388486, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.014128168, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0138662, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.013592461, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.013314132, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 84.59214782714844 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.3234269, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06475467, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.05799052, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.05294171, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.049018107, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.04570168, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.0427216, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.03985396, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.03708065, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034403045, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03170696, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029149694, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02672895, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024450324, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.022244236, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.020143906, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.018175505, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.016355557, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0146667035, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.013119876, training acc= 100.0%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.011721132, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.011589807, training acc= 100.0%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.011445707, training acc= 100.0%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2300, training loss= 0.011292767, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.011136776, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.010976747, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.010812976, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.010644687, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.010472153, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.010298835, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.010124117, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.009949069, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.0097727785, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.00959596, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.009417163, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.009238618, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.009060325, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0088828, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.00870476, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.008530194, training acc= 100.0%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7378755, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.0724496, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.06288828, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.056625355, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.05239035, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.048961084, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.045826726, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.04311272, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.040770326, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.038679358, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03666604, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03476912, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.032886494, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.031028781, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02919876, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.027383136, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025568534, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023781946, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.022051174, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020348161, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.0187147, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018549688, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01837405, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018188836, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017989047, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017783482, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017567601, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017341988, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017108917, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.016870862, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.016625995, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016375838, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016120909, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.015860373, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.0155928135, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015321482, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015047954, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3700, training loss= 0.014775563, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014498342, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014220417, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 83.68580627441406 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.40455517, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.062295496, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.05519199, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.050471455, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.046680693, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.04367319, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.040931825, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.03847419, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.036139224, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.033985052, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.031822246, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029763877, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027796116, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025861548, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023958385, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022103747, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020299286, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.018544251, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.016866095, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.015306864, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.013848605, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.013704733, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.013553359, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.013394749, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.013226724, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01305333, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.012867359, training acc= 100.0%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.012676224, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.012480636, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.012284725, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.012093401, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.011895088, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.011691549, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.011486277, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.011277279, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.011069123, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.0108603975, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.010652425, training acc= 100.0%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.010445904, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.010238607, training acc= 100.0%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.68580627441406 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.77111113, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.07044671, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.060953148, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.05450475, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.04982273, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.045783717, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04276574, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.040334847, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.038156457, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03627389, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.034472063, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.032710887, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03107645, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029426664, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027799383, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026217053, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.024685146, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023170432, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021661414, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020156652, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.0187078, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018561695, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018404178, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018234603, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01805585, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017867353, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017674213, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017472371, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017261127, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017040107, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.016810065, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.01657266, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016331142, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016084407, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.015833003, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3500, training loss= 0.0155756865, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015315289, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0150505975, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014782104, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014512815, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.46946406, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.060685873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.052685596, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.047632612, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.043917812, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.041002415, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.038619183, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.036546737, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.034662895, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.03283292, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.031067103, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.029358217, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027698582, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.026055885, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024334347, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022701709, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02108655, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019502444, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.017971886, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.016473679, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.015033527, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.014890784, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01473693, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.014574748, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.014405358, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.014229782, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.014041985, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0138515085, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.013655657, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.013453714, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.013248362, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.013038756, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.012824491, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.012604597, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.012384382, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.012162867, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.011939898, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.011709846, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01148138, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.011263011, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7977012, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06852169, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.056701787, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.050397072, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.04597821, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.042389054, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.03954308, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.037147284, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.035148345, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.033395763, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03186001, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030464116, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029091863, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027768075, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02643644, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.025125211, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02378286, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022438155, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021110937, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.01980777, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018507665, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018375002, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.0182315, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018077333, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017914299, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017742703, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017559791, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017368631, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017169956, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.016963774, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.016748425, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016525306, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016294451, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016057761, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.015814051, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.01556324, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015307622, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015046939, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014784682, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014520189, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5226002, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.059767358, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.050666224, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.045391545, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.04165681, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.038841873, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.036576357, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.034664728, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.032942627, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.031341307, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.02979577, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.028294632, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.026863001, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025454633, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02395855, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022518693, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.021111684, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019711949, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.018352814, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.016998857, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.015643567, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015509894, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01536271, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015207924, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015042136, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01487036, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.014692101, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014504667, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014310315, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014108696, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.0139005445, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.013687981, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013468926, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.0132469935, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013023361, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.012798153, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.012572694, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012347471, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.0121208, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.011893272, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.82020986, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06561951, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.054332014, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.04742886, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.042844724, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.039339107, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.036762353, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.03468329, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.032849856, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.03127475, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.029880922, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.028582893, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.027289417, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.026094316, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024933405, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02378169, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.022609133, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.021436933, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.020295274, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.01914243, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2000, training loss= 0.017994687, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2100, training loss= 0.017882938, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.017756935, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2300, training loss= 0.017622048, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017478576, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01732495, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017161239, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.016988665, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2800, training loss= 0.016806563, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01661519, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01641571, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016210116, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.01599541, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.015771225, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.015541034, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015301904, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015051318, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.014795281, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014537662, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0142790675, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5668403, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.05870537, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.048579626, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.042892184, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.039155915, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.036554143, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.034479167, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.03278617, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.031239456, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.029786874, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0283917, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027060062, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025764717, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024520887, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02329624, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02207201, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020852514, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019637393, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01844693, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017176403, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01592644, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015802315, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015668873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015527826, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015378176, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015219638, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015049796, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014872183, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014687873, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014496491, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.014298481, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.014092969, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013881068, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013665467, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013444701, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013222101, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01299754, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012768291, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012540915, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.012308916, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 84.29002380371094 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.83904004, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.06268441, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.051497813, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.044377245, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.040017303, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.03671657, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.034291875, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.032291137, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.030609036, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.02922501, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.027962277, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.026793804, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.025702808, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.024665022, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.023647556, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022622047, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.021600539, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.020574786, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.019550199, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 1900, training loss= 0.018533718, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 2000, training loss= 0.017502345, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2100, training loss= 0.017394964, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01728177, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2300, training loss= 0.017159294, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017027853, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2500, training loss= 0.016887557, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2600, training loss= 0.016735006, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2700, training loss= 0.016572949, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2800, training loss= 0.016401906, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01622269, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01603337, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 3100, training loss= 0.015837878, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.5999984741211 ...\n",
            "\n",
            "epoch 3200, training loss= 0.01563124, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.01541839, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.0151999025, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.014974831, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.014740013, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01450141, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014260217, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01401447, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "Valid acc= 90.6 %\n",
            "Validation Accuracy Test 83.9879150390625 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YkKX7VVx3h9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "f12e574d-360f-40cc-89ba-974eaed60ef8"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(ValidAccuracy_Track)\n",
        "plt.plot(ValidAccuracy_Test_track)\n",
        "\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTN9SUC5Sz5sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DtrGZ1qX3hpo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32f3302b-204e-4579-bf51-783155a8981a"
      },
      "cell_type": "code",
      "source": [
        "print(np.max(ValidAccuracy_Track))\n",
        "print(np.argmax(ValidAccuracy_Track))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90.6\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BrkZIJvoUb6Y",
        "colab_type": "code",
        "outputId": "cd5c51ed-972c-41d5-f053-4fbd89de8322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(ValidAccuracy_Track)\n",
        "plt.plot(ValidAccuracy_Test_track)\n",
        "\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTN9SUC5Sz5sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u3UFxL_bUb3B",
        "colab_type": "code",
        "outputId": "c903d190-4687-43f5-cbee-b8987c019f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.max(ValidAccuracy_Track))\n",
        "print(np.argmax(ValidAccuracy_Track))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90.6\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "54oqBMzcOEmX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Best Wt 3,3,1"
      ]
    },
    {
      "metadata": {
        "id": "SkE0jNoRMpDc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffling_indices_validation_data = np.random.permutation(validation_data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2VQdoHyMo75",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffled_validation_data = validation_data[shuffling_indices_validation_data,:]\n",
        "shuffled_validation_label = validation_label_one_hot[shuffling_indices_validation_data,:]\n",
        "train_valid_combined_shuffled = np.concatenate((train_data, shuffled_validation_data))\n",
        "train_valid_combined_shuffled_label = np.concatenate((train_label_one_hot, shuffled_validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efMdmBhLMo4Q",
        "colab_type": "code",
        "outputId": "11797ce9-8aee-4042-d0db-c32aadd63990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([291.,   0.,  95.,   0., 163.,   0., 109.,   0., 176., 497.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qHMvoOAhMo1G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # keep aside \n",
        "# aside_examples= 300\n",
        "# aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "# aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "# combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "# combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXwjPfSP4Mhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60b3f803-dedc-4e3d-89be-f38e7604023a"
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined_shuffled.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4435, 36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "udUVxeCQMoxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 400\n",
        "aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1InMUDSKT0jx",
        "colab_type": "code",
        "outputId": "f72e3770-76cf-478f-ae43-3e1cf0bbfe97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 150000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 20000\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 1000\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 3\n",
        "wLoss2 = 3\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<1000:\n",
        "        learn = .001\n",
        "      elif ep >=1000 and ep <= 2000:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .0001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 1.6182607, training acc total= 95.01858949661255%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 1000, training loss Total= 0.05214016, training acc total= 96.25774621963501%\n",
            "ValidTest acc= 87.0 %\n",
            "epoch 2000, training loss Total= 0.034236245, training acc total= 97.5712537765503%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 3000, training loss Total= 0.032093067, training acc total= 97.71994948387146%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4000, training loss Total= 0.028651182, training acc total= 98.04213047027588%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 5000, training loss Total= 0.024622599, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.5 %\n",
            "epoch 6000, training loss Total= 0.020758264, training acc total= 98.98388981819153%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 7000, training loss Total= 0.017303653, training acc total= 99.20693635940552%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 8000, training loss Total= 0.0143201295, training acc total= 99.52911734580994%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 9000, training loss Total= 0.011783426, training acc total= 99.7521698474884%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 10000, training loss Total= 0.00964489, training acc total= 99.77695345878601%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 11000, training loss Total= 0.007813648, training acc total= 99.85129833221436%\n",
            "ValidTest acc= 87.5 %\n",
            "epoch 12000, training loss Total= 0.0062528704, training acc total= 99.90086555480957%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 13000, training loss Total= 0.004939201, training acc total= 99.97521638870239%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 14000, training loss Total= 0.0038628608, training acc total= 100.0%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 15000, training loss Total= 0.0029955592, training acc total= 100.0%\n",
            "ValidTest acc= 87.0 %\n",
            "epoch 16000, training loss Total= 0.0023090718, training acc total= 100.0%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 17000, training loss Total= 0.0017700624, training acc total= 100.0%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 18000, training loss Total= 0.0013477415, training acc total= 100.0%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 19000, training loss Total= 0.001019347, training acc total= 100.0%\n",
            "ValidTest acc= 87.0 %\n",
            "ValidValid acc= 96.09316 %\n",
            "ValidTest acc= 87.0 %\n",
            "==================================================\n",
            "W1\n",
            "3\n",
            "W2\n",
            "3\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9-mykvMcWcHH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### fine tune for higher precision for no. epochs"
      ]
    },
    {
      "metadata": {
        "id": "zWCtawMkT0hU",
        "colab_type": "code",
        "outputId": "c1218a94-b635-4d61-d71c-0792a4f3d1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 150000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 6000\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 1000\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 3\n",
        "wLoss2 = 3\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<1000:\n",
        "        learn = .001\n",
        "      elif ep >=1000 and ep <= 2000:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .0001\n",
        "      if ep>4000:\n",
        "        plot_every = 100\n",
        "      \n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 1.6182607, training acc total= 95.01858949661255%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 1000, training loss Total= 0.05214016, training acc total= 96.25774621963501%\n",
            "ValidTest acc= 87.0 %\n",
            "epoch 2000, training loss Total= 0.034236245, training acc total= 97.5712537765503%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 3000, training loss Total= 0.032093067, training acc total= 97.71994948387146%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4000, training loss Total= 0.028651182, training acc total= 98.04213047027588%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 4100, training loss Total= 0.028251514, training acc total= 98.0916976928711%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 4200, training loss Total= 0.027851677, training acc total= 98.16604852676392%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 4300, training loss Total= 0.027449742, training acc total= 98.19083213806152%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 4400, training loss Total= 0.027042005, training acc total= 98.19083213806152%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 4500, training loss Total= 0.02663665, training acc total= 98.19083213806152%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 4600, training loss Total= 0.026234442, training acc total= 98.24039936065674%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 4700, training loss Total= 0.025830783, training acc total= 98.31474423408508%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 4800, training loss Total= 0.025424633, training acc total= 98.31474423408508%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 4900, training loss Total= 0.025022903, training acc total= 98.3890950679779%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 5000, training loss Total= 0.024622599, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.5 %\n",
            "epoch 5100, training loss Total= 0.024221225, training acc total= 98.48822951316833%\n",
            "ValidTest acc= 89.5 %\n",
            "epoch 5200, training loss Total= 0.023824666, training acc total= 98.51301312446594%\n",
            "ValidTest acc= 89.75 %\n",
            "epoch 5300, training loss Total= 0.023430185, training acc total= 98.56258034706116%\n",
            "ValidTest acc= 89.75 %\n",
            "epoch 5400, training loss Total= 0.023040105, training acc total= 98.6369252204895%\n",
            "ValidTest acc= 89.75 %\n",
            "epoch 5500, training loss Total= 0.022655468, training acc total= 98.71127605438232%\n",
            "ValidTest acc= 89.5 %\n",
            "epoch 5600, training loss Total= 0.022273311, training acc total= 98.76084327697754%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 5700, training loss Total= 0.021893935, training acc total= 98.81041049957275%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 5800, training loss Total= 0.021514637, training acc total= 98.85997772216797%\n",
            "ValidTest acc= 89.25 %\n",
            "epoch 5900, training loss Total= 0.021135094, training acc total= 98.88476133346558%\n",
            "ValidTest acc= 89.0 %\n",
            "ValidValid acc= 95.49211 %\n",
            "ValidTest acc= 89.0 %\n",
            "==================================================\n",
            "W1\n",
            "3\n",
            "W2\n",
            "3\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bZXwHMWIbQ4j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### train till 5400 epochs combined data"
      ]
    },
    {
      "metadata": {
        "id": "tQg6Phm6T0e_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 1\n",
        "aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLJITTcvT0ch",
        "colab_type": "code",
        "outputId": "4fba6497-da51-4f05-df89-049bc22649aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 150000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 5300\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 1000\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 3\n",
        "wLoss2 = 3\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<1000:\n",
        "        learn = .001\n",
        "      elif ep >=1000 and ep <= 2000:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .0001\n",
        "      if ep>4000:\n",
        "        plot_every = 100\n",
        "      \n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "#           validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#           print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "#     validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "#     print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "#     validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#     print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "    saver.save(sess, './statim_FullAdamSave')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 1.1940717, training acc total= 94.09111142158508%\n",
            "epoch 1000, training loss Total= 0.06773116, training acc total= 95.39918899536133%\n",
            "epoch 2000, training loss Total= 0.049043458, training acc total= 96.45918011665344%\n",
            "epoch 3000, training loss Total= 0.046488322, training acc total= 96.59449458122253%\n",
            "epoch 4000, training loss Total= 0.041870184, training acc total= 96.86513543128967%\n",
            "epoch 4100, training loss Total= 0.041453984, training acc total= 96.91023826599121%\n",
            "epoch 4200, training loss Total= 0.041029897, training acc total= 96.91023826599121%\n",
            "epoch 4300, training loss Total= 0.04058173, training acc total= 96.95534706115723%\n",
            "epoch 4400, training loss Total= 0.040157817, training acc total= 97.04555869102478%\n",
            "epoch 4500, training loss Total= 0.03975139, training acc total= 97.04555869102478%\n",
            "epoch 4600, training loss Total= 0.039357193, training acc total= 97.04555869102478%\n",
            "epoch 4700, training loss Total= 0.038937412, training acc total= 97.04555869102478%\n",
            "epoch 4800, training loss Total= 0.038433746, training acc total= 97.04555869102478%\n",
            "epoch 4900, training loss Total= 0.03804814, training acc total= 97.11321592330933%\n",
            "epoch 5000, training loss Total= 0.037688263, training acc total= 97.15832471847534%\n",
            "epoch 5100, training loss Total= 0.037334066, training acc total= 97.20342755317688%\n",
            "epoch 5200, training loss Total= 0.036992654, training acc total= 97.22598195075989%\n",
            "Train acc= 0.97271085 %\n",
            "==================================================\n",
            "W1\n",
            "3\n",
            "W2\n",
            "3\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-brfh_zMeNtS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check on test data"
      ]
    },
    {
      "metadata": {
        "id": "rkymr0dpT0aC",
        "colab_type": "code",
        "outputId": "cf6027aa-666f-403a-a32b-d6f2a438a7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, './statim_FullAdamSave')\n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_accuracy), \"%\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statim_FullAdamSave\n",
            "Train acc= 0.97271085 %\n",
            "Test acc= 89.9 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lirrvwQTT0Wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}