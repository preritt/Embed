{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EpochBasedSatimReducedDataset04202019_ADAM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/EpochBasedSatimReducedDataset04202019_ADAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0SINGreLFCRz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import packages"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "My4EmvydE3bW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dTAGPqvlFEuQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ovvpmlXeFH1x",
        "outputId": "03add738-41e3-4233-88ed-d0ba91c1c793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "train_data_pandas = pd.DataFrame(train_data)\n",
        "train_data_labels = pd.DataFrame(train_label)\n",
        "train_data_pandas.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.601267</td>\n",
              "      <td>-1.288475</td>\n",
              "      <td>-0.051051</td>\n",
              "      <td>-0.409032</td>\n",
              "      <td>0.260482</td>\n",
              "      <td>-0.277124</td>\n",
              "      <td>0.102922</td>\n",
              "      <td>-0.210252</td>\n",
              "      <td>-0.037423</td>\n",
              "      <td>0.029499</td>\n",
              "      <td>0.104416</td>\n",
              "      <td>0.165190</td>\n",
              "      <td>-0.154018</td>\n",
              "      <td>-0.061375</td>\n",
              "      <td>0.052261</td>\n",
              "      <td>0.129714</td>\n",
              "      <td>-0.016616</td>\n",
              "      <td>-0.022097</td>\n",
              "      <td>0.105699</td>\n",
              "      <td>-0.111762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.895830</td>\n",
              "      <td>-1.888285</td>\n",
              "      <td>-0.036854</td>\n",
              "      <td>-0.006050</td>\n",
              "      <td>0.119070</td>\n",
              "      <td>-0.008976</td>\n",
              "      <td>0.108056</td>\n",
              "      <td>-0.022527</td>\n",
              "      <td>-0.028848</td>\n",
              "      <td>0.170903</td>\n",
              "      <td>-0.083485</td>\n",
              "      <td>-0.051184</td>\n",
              "      <td>0.041117</td>\n",
              "      <td>-0.055565</td>\n",
              "      <td>0.047334</td>\n",
              "      <td>-0.059557</td>\n",
              "      <td>-0.045411</td>\n",
              "      <td>0.093592</td>\n",
              "      <td>0.036876</td>\n",
              "      <td>-0.028558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.996519</td>\n",
              "      <td>-0.587328</td>\n",
              "      <td>-0.309866</td>\n",
              "      <td>-0.343356</td>\n",
              "      <td>0.891381</td>\n",
              "      <td>0.339134</td>\n",
              "      <td>-0.161639</td>\n",
              "      <td>0.052494</td>\n",
              "      <td>-0.160554</td>\n",
              "      <td>0.038198</td>\n",
              "      <td>0.066820</td>\n",
              "      <td>-0.026711</td>\n",
              "      <td>0.028572</td>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.058724</td>\n",
              "      <td>-0.003362</td>\n",
              "      <td>-0.075539</td>\n",
              "      <td>0.138385</td>\n",
              "      <td>-0.012593</td>\n",
              "      <td>-0.001673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.647370</td>\n",
              "      <td>-1.245622</td>\n",
              "      <td>-0.166432</td>\n",
              "      <td>0.515280</td>\n",
              "      <td>0.105636</td>\n",
              "      <td>0.183723</td>\n",
              "      <td>0.182418</td>\n",
              "      <td>-0.132266</td>\n",
              "      <td>-0.315006</td>\n",
              "      <td>0.097930</td>\n",
              "      <td>0.056246</td>\n",
              "      <td>0.337317</td>\n",
              "      <td>0.024939</td>\n",
              "      <td>-0.048987</td>\n",
              "      <td>-0.013325</td>\n",
              "      <td>-0.015493</td>\n",
              "      <td>-0.031741</td>\n",
              "      <td>-0.270518</td>\n",
              "      <td>-0.104466</td>\n",
              "      <td>0.078826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.077101</td>\n",
              "      <td>-1.052330</td>\n",
              "      <td>-0.259343</td>\n",
              "      <td>0.584375</td>\n",
              "      <td>0.466714</td>\n",
              "      <td>0.471585</td>\n",
              "      <td>-0.144931</td>\n",
              "      <td>0.281123</td>\n",
              "      <td>-0.052295</td>\n",
              "      <td>0.255867</td>\n",
              "      <td>-0.050871</td>\n",
              "      <td>-0.083271</td>\n",
              "      <td>-0.078085</td>\n",
              "      <td>0.001341</td>\n",
              "      <td>-0.195125</td>\n",
              "      <td>-0.020316</td>\n",
              "      <td>0.041219</td>\n",
              "      <td>0.029217</td>\n",
              "      <td>0.142422</td>\n",
              "      <td>-0.079658</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  0.601267 -1.288475 -0.051051 -0.409032  0.260482 -0.277124  0.102922   \n",
              "1  0.895830 -1.888285 -0.036854 -0.006050  0.119070 -0.008976  0.108056   \n",
              "2 -0.996519 -0.587328 -0.309866 -0.343356  0.891381  0.339134 -0.161639   \n",
              "3  0.647370 -1.245622 -0.166432  0.515280  0.105636  0.183723  0.182418   \n",
              "4  0.077101 -1.052330 -0.259343  0.584375  0.466714  0.471585 -0.144931   \n",
              "\n",
              "         7         8         9         10        11        12        13  \\\n",
              "0 -0.210252 -0.037423  0.029499  0.104416  0.165190 -0.154018 -0.061375   \n",
              "1 -0.022527 -0.028848  0.170903 -0.083485 -0.051184  0.041117 -0.055565   \n",
              "2  0.052494 -0.160554  0.038198  0.066820 -0.026711  0.028572  0.016461   \n",
              "3 -0.132266 -0.315006  0.097930  0.056246  0.337317  0.024939 -0.048987   \n",
              "4  0.281123 -0.052295  0.255867 -0.050871 -0.083271 -0.078085  0.001341   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0  0.052261  0.129714 -0.016616 -0.022097  0.105699 -0.111762  \n",
              "1  0.047334 -0.059557 -0.045411  0.093592  0.036876 -0.028558  \n",
              "2  0.058724 -0.003362 -0.075539  0.138385 -0.012593 -0.001673  \n",
              "3 -0.013325 -0.015493 -0.031741 -0.270518 -0.104466  0.078826  \n",
              "4 -0.195125 -0.020316  0.041219  0.029217  0.142422 -0.079658  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "epqBn1YjFlII",
        "outputId": "d8dd4f0c-20ff-45b4-9bdd-603ba0b9c35e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "train_data_labels.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  5\n",
              "1  5\n",
              "2  5\n",
              "3  5\n",
              "4  5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ewLyg3iuFqkO",
        "outputId": "11a88caa-1f05-45a4-e5b0-52e7021070fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w5wMHmhIFthO",
        "outputId": "342c0847-26f9-49a3-d2dc-039f9fac69e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1331, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5Jg0BONTGBA1"
      },
      "cell_type": "markdown",
      "source": [
        "#### Combine Validation and train data for MLP classifier - and set validation fraction to 4500/15000 = 0.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8krXltl9GPfv",
        "outputId": "80f5ab18-18de-478b-c961-66251acd4ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_combined.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4435, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NUWNzsz4v04T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bWN_sAWEFNtb"
      },
      "cell_type": "markdown",
      "source": [
        "#### Fit MLP Classifier"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QSdXJQLnFKa2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clf = MLPClassifier(hidden_layer_sizes=(104),validation_fraction=0.3)\n",
        "# clf.fit(train_data, train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnBnT6NdTqyO",
        "colab_type": "code",
        "outputId": "8ddeb4a5-d9a8-42e5-dd0e-736a78f83764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "20*90/36"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xfKh_nDUvj5G",
        "outputId": "c919995f-2149-4131-fe54-2ea511e80eea",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3590
        }
      },
      "cell_type": "code",
      "source": [
        "clf =MLPClassifier(hidden_layer_sizes=(50, ), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "\n",
        "# Test set score: 0.950119\n",
        "\n",
        "# clf.fit(train_valid_combined, train_valid_label)\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.95074262\n",
            "Iteration 2, loss = 0.40768669\n",
            "Iteration 3, loss = 0.36785070\n",
            "Iteration 4, loss = 0.34458895\n",
            "Iteration 5, loss = 0.33250029\n",
            "Iteration 6, loss = 0.32492560\n",
            "Iteration 7, loss = 0.31602461\n",
            "Iteration 8, loss = 0.31022870\n",
            "Iteration 9, loss = 0.30321219\n",
            "Iteration 10, loss = 0.29668668\n",
            "Iteration 11, loss = 0.29259674\n",
            "Iteration 12, loss = 0.28981648\n",
            "Iteration 13, loss = 0.28237008\n",
            "Iteration 14, loss = 0.27676422\n",
            "Iteration 15, loss = 0.27355262\n",
            "Iteration 16, loss = 0.26758509\n",
            "Iteration 17, loss = 0.26467467\n",
            "Iteration 18, loss = 0.25851363\n",
            "Iteration 19, loss = 0.25696537\n",
            "Iteration 20, loss = 0.25395678\n",
            "Iteration 21, loss = 0.24893087\n",
            "Iteration 22, loss = 0.24532578\n",
            "Iteration 23, loss = 0.24379364\n",
            "Iteration 24, loss = 0.24156889\n",
            "Iteration 25, loss = 0.23882958\n",
            "Iteration 26, loss = 0.23914027\n",
            "Iteration 27, loss = 0.23457658\n",
            "Iteration 28, loss = 0.23173433\n",
            "Iteration 29, loss = 0.23054574\n",
            "Iteration 30, loss = 0.22734081\n",
            "Iteration 31, loss = 0.22742927\n",
            "Iteration 32, loss = 0.22428501\n",
            "Iteration 33, loss = 0.22645777\n",
            "Iteration 34, loss = 0.21922461\n",
            "Iteration 35, loss = 0.21810316\n",
            "Iteration 36, loss = 0.21672321\n",
            "Iteration 37, loss = 0.21417549\n",
            "Iteration 38, loss = 0.21490680\n",
            "Iteration 39, loss = 0.21151040\n",
            "Iteration 40, loss = 0.21006523\n",
            "Iteration 41, loss = 0.21035758\n",
            "Iteration 42, loss = 0.20832934\n",
            "Iteration 43, loss = 0.20600904\n",
            "Iteration 44, loss = 0.20454652\n",
            "Iteration 45, loss = 0.20521039\n",
            "Iteration 46, loss = 0.20503524\n",
            "Iteration 47, loss = 0.20138200\n",
            "Iteration 48, loss = 0.20141667\n",
            "Iteration 49, loss = 0.19987018\n",
            "Iteration 50, loss = 0.19747397\n",
            "Iteration 51, loss = 0.19666531\n",
            "Iteration 52, loss = 0.19596888\n",
            "Iteration 53, loss = 0.19709708\n",
            "Iteration 54, loss = 0.19431888\n",
            "Iteration 55, loss = 0.19416356\n",
            "Iteration 56, loss = 0.19275202\n",
            "Iteration 57, loss = 0.19295534\n",
            "Iteration 58, loss = 0.19004691\n",
            "Iteration 59, loss = 0.19010823\n",
            "Iteration 60, loss = 0.19162169\n",
            "Iteration 61, loss = 0.18730362\n",
            "Iteration 62, loss = 0.18493162\n",
            "Iteration 63, loss = 0.18526362\n",
            "Iteration 64, loss = 0.18524466\n",
            "Iteration 65, loss = 0.18244430\n",
            "Iteration 66, loss = 0.18345577\n",
            "Iteration 67, loss = 0.18381467\n",
            "Iteration 68, loss = 0.18085034\n",
            "Iteration 69, loss = 0.18128665\n",
            "Iteration 70, loss = 0.17971048\n",
            "Iteration 71, loss = 0.17944994\n",
            "Iteration 72, loss = 0.17787072\n",
            "Iteration 73, loss = 0.17641098\n",
            "Iteration 74, loss = 0.17612699\n",
            "Iteration 75, loss = 0.17530516\n",
            "Iteration 76, loss = 0.17583348\n",
            "Iteration 77, loss = 0.17480372\n",
            "Iteration 78, loss = 0.17394294\n",
            "Iteration 79, loss = 0.17068555\n",
            "Iteration 80, loss = 0.17161057\n",
            "Iteration 81, loss = 0.17015480\n",
            "Iteration 82, loss = 0.17131631\n",
            "Iteration 83, loss = 0.16896311\n",
            "Iteration 84, loss = 0.17061081\n",
            "Iteration 85, loss = 0.16816616\n",
            "Iteration 86, loss = 0.16641030\n",
            "Iteration 87, loss = 0.16835225\n",
            "Iteration 88, loss = 0.16684512\n",
            "Iteration 89, loss = 0.16440606\n",
            "Iteration 90, loss = 0.16578737\n",
            "Iteration 91, loss = 0.16221365\n",
            "Iteration 92, loss = 0.16378680\n",
            "Iteration 93, loss = 0.16192391\n",
            "Iteration 94, loss = 0.16388635\n",
            "Iteration 95, loss = 0.16114762\n",
            "Iteration 96, loss = 0.16367723\n",
            "Iteration 97, loss = 0.16101316\n",
            "Iteration 98, loss = 0.15933302\n",
            "Iteration 99, loss = 0.16045387\n",
            "Iteration 100, loss = 0.15951001\n",
            "Iteration 101, loss = 0.15691568\n",
            "Iteration 102, loss = 0.15818791\n",
            "Iteration 103, loss = 0.15837329\n",
            "Iteration 104, loss = 0.15584391\n",
            "Iteration 105, loss = 0.15583827\n",
            "Iteration 106, loss = 0.15599273\n",
            "Iteration 107, loss = 0.15319245\n",
            "Iteration 108, loss = 0.15562912\n",
            "Iteration 109, loss = 0.15512648\n",
            "Iteration 110, loss = 0.15232538\n",
            "Iteration 111, loss = 0.15296338\n",
            "Iteration 112, loss = 0.15145067\n",
            "Iteration 113, loss = 0.15098276\n",
            "Iteration 114, loss = 0.15031846\n",
            "Iteration 115, loss = 0.15027724\n",
            "Iteration 116, loss = 0.14935966\n",
            "Iteration 117, loss = 0.15015494\n",
            "Iteration 118, loss = 0.14984951\n",
            "Iteration 119, loss = 0.14834641\n",
            "Iteration 120, loss = 0.14841561\n",
            "Iteration 121, loss = 0.14871887\n",
            "Iteration 122, loss = 0.15027818\n",
            "Iteration 123, loss = 0.14475348\n",
            "Iteration 124, loss = 0.14854944\n",
            "Iteration 125, loss = 0.14336146\n",
            "Iteration 126, loss = 0.14462108\n",
            "Iteration 127, loss = 0.14434297\n",
            "Iteration 128, loss = 0.14356984\n",
            "Iteration 129, loss = 0.14154698\n",
            "Iteration 130, loss = 0.14273714\n",
            "Iteration 131, loss = 0.14178325\n",
            "Iteration 132, loss = 0.14399314\n",
            "Iteration 133, loss = 0.14456158\n",
            "Iteration 134, loss = 0.14087202\n",
            "Iteration 135, loss = 0.14034021\n",
            "Iteration 136, loss = 0.14056923\n",
            "Iteration 137, loss = 0.13947551\n",
            "Iteration 138, loss = 0.13820539\n",
            "Iteration 139, loss = 0.13862573\n",
            "Iteration 140, loss = 0.13675965\n",
            "Iteration 141, loss = 0.13634741\n",
            "Iteration 142, loss = 0.13758536\n",
            "Iteration 143, loss = 0.13656693\n",
            "Iteration 144, loss = 0.13617684\n",
            "Iteration 145, loss = 0.13668260\n",
            "Iteration 146, loss = 0.13407951\n",
            "Iteration 147, loss = 0.13524165\n",
            "Iteration 148, loss = 0.13269198\n",
            "Iteration 149, loss = 0.13319225\n",
            "Iteration 150, loss = 0.13297231\n",
            "Iteration 151, loss = 0.13461861\n",
            "Iteration 152, loss = 0.13270776\n",
            "Iteration 153, loss = 0.13331697\n",
            "Iteration 154, loss = 0.13484333\n",
            "Iteration 155, loss = 0.13042394\n",
            "Iteration 156, loss = 0.12956801\n",
            "Iteration 157, loss = 0.13188497\n",
            "Iteration 158, loss = 0.12823179\n",
            "Iteration 159, loss = 0.12833942\n",
            "Iteration 160, loss = 0.12958848\n",
            "Iteration 161, loss = 0.13066224\n",
            "Iteration 162, loss = 0.12597281\n",
            "Iteration 163, loss = 0.12552055\n",
            "Iteration 164, loss = 0.12672183\n",
            "Iteration 165, loss = 0.12584478\n",
            "Iteration 166, loss = 0.12604925\n",
            "Iteration 167, loss = 0.12353436\n",
            "Iteration 168, loss = 0.12807580\n",
            "Iteration 169, loss = 0.12233480\n",
            "Iteration 170, loss = 0.12391715\n",
            "Iteration 171, loss = 0.12343118\n",
            "Iteration 172, loss = 0.12162751\n",
            "Iteration 173, loss = 0.12236546\n",
            "Iteration 174, loss = 0.12448056\n",
            "Iteration 175, loss = 0.12086769\n",
            "Iteration 176, loss = 0.11980328\n",
            "Iteration 177, loss = 0.12051658\n",
            "Iteration 178, loss = 0.11831469\n",
            "Iteration 179, loss = 0.11833034\n",
            "Iteration 180, loss = 0.11806005\n",
            "Iteration 181, loss = 0.12028357\n",
            "Iteration 182, loss = 0.11900392\n",
            "Iteration 183, loss = 0.11885993\n",
            "Iteration 184, loss = 0.11994181\n",
            "Iteration 185, loss = 0.11720171\n",
            "Iteration 186, loss = 0.11505591\n",
            "Iteration 187, loss = 0.11379610\n",
            "Iteration 188, loss = 0.11443438\n",
            "Iteration 189, loss = 0.11537570\n",
            "Iteration 190, loss = 0.11728507\n",
            "Iteration 191, loss = 0.11493212\n",
            "Iteration 192, loss = 0.11455919\n",
            "Iteration 193, loss = 0.11469774\n",
            "Iteration 194, loss = 0.11389851\n",
            "Iteration 195, loss = 0.11142616\n",
            "Iteration 196, loss = 0.11165826\n",
            "Iteration 197, loss = 0.11095011\n",
            "Iteration 198, loss = 0.11164786\n",
            "Iteration 199, loss = 0.11068664\n",
            "Iteration 200, loss = 0.11190760\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(50,), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lLNA4D0qGxJi"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "02O8VTAoGqnG",
        "outputId": "9ae80503-7ee0-4f8d-a6d8-421163db1c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.958118556701031"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "whn8u2m5iY7M"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pii8wXXSG1r7"
      },
      "cell_type": "markdown",
      "source": [
        "#### Validation Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SakclBGkGvI0",
        "outputId": "61a33e3e-96c7-4501-a744-31ee6c7a1cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8812922614575507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VbIkGX5gG5ZG"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QLo_AzFVG3ca",
        "outputId": "435494ad-b6c4-4ad0-a70c-70221f90b33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.895"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "InLIF676HEES"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow model using weights initialized from numpy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tcBNfKZNG9Pm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ortxRVBMH7W7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z7mWVCDVEgLm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hid_neuron = [90]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LR62GfKJv_6E",
        "outputId": "f38539e3-7343-4507-f325-011fe9fa81d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EylNp0IJONbz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Base NN model in tensor flow"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VzJnI_o2xD5C"
      },
      "cell_type": "markdown",
      "source": [
        "#### 36 -> 90 -> 6"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "73Way2v2Pbys"
      },
      "cell_type": "markdown",
      "source": [
        "## Train baseline model in tensorflow"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L-hUDOm5xClH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IMHh0nROw5O-",
        "outputId": "c0df9284-52a1-42f1-d39b-d35ecf41c2e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3104, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yV4xtxJLvyNj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wj_W9eCBvyKy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TKQ6nMqMvyJD",
        "outputId": "15d7af47-62f6-4162-f7bd-b8e4ab2e9021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jy2mQcHAEn20",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf.train.GradientDescentOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eHe01FffvyEJ",
        "outputId": "caa0ef5d-3d2f-498d-d21b-6623a8423fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        }
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "learning_rate = 0.001\n",
        "hid_neuron = [374]\n",
        "num_steps = 20000\n",
        "batch_size = 200\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "#     layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    return out_layer\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X), labels=Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X), 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "  ### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % 1000 == 0:\n",
        "            train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "            print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "            train_losses.append(train_loss)\n",
        "            validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "            if step%1000 == 0:\n",
        "              print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "              print()\n",
        "              if (validation_accuracy >= best_accuracy_valid):\n",
        "                best_accuracy_valid = validation_accuracy\n",
        "                saver.save(sess, './statlog_letter')\n",
        "                test_Accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_Accuracy), \"%\")\n",
        "    print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "\n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-25-4106bc23d37b>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step 0, training loss= 0.09654394, training acc= 96.49999737739563%\n",
            "Validation Accuracy 88.58000946044922 ...\n",
            "\n",
            "step 1000, training loss= 0.074345045, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 2000, training loss= 0.0795926, training acc= 99.00000095367432%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 3000, training loss= 0.09808998, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 4000, training loss= 0.07493803, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 5000, training loss= 0.095866315, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 6000, training loss= 0.07703011, training acc= 98.00000190734863%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 7000, training loss= 0.074125834, training acc= 98.50000143051147%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 8000, training loss= 0.099417955, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 9000, training loss= 0.09222789, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 10000, training loss= 0.08109907, training acc= 96.49999737739563%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 11000, training loss= 0.11149396, training acc= 95.49999833106995%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 12000, training loss= 0.06741232, training acc= 99.00000095367432%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 13000, training loss= 0.109245956, training acc= 95.99999785423279%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 14000, training loss= 0.07493451, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 15000, training loss= 0.090577155, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 16000, training loss= 0.13477787, training acc= 94.49999928474426%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 17000, training loss= 0.07381674, training acc= 98.00000190734863%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 18000, training loss= 0.07977779, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 19000, training loss= 0.102517195, training acc= 95.99999785423279%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "Test acc= 89.450005 %\n",
            "Valid acc= 88.805405 %\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5A_PHV3bS7ui"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8RFK2bW4JZ7w"
      },
      "cell_type": "markdown",
      "source": [
        "#### My model with feedback"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "G5BxkTLzUAok"
      },
      "cell_type": "markdown",
      "source": [
        "## Divide valid in two parts for validation and validation-testÂ¶"
      ]
    },
    {
      "metadata": {
        "id": "mejHTwMYhEzu",
        "colab_type": "code",
        "outputId": "575789a3-75a5-4905-9720-2c9b44b17ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(validation_data.shape)\n",
        "print(train_data.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1331, 20)\n",
            "(3104, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jVm6nWpSJn1l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data = validation_data[0:1000,:]\n",
        "valid_validation_data_label = validation_label_one_hot[0:1000,:]\n",
        "valid_test_data = validation_data[1000:,:]\n",
        "valid_test_data_label = validation_label_one_hot[1000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wcT7Xaz1KNcU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ydDcWHWsJcJ-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "num_hidden_neurons = 50\n",
        "GwLoop = tf.Variable(xavier_init([output_shape,num_hidden_neurons]))\n",
        "G_bLoop = tf.Variable(tf.zeros(shape=[num_hidden_neurons]))\n",
        "\n",
        "GwLoop2 = tf.Variable(xavier_init([num_hidden_neurons,num_hidden_neurons]))\n",
        "G_bLoop2 = tf.Variable(tf.zeros(shape=[num_hidden_neurons]))\n",
        "\n",
        "GLossW = tf.Variable(xavier_init([output_shape,output_shape]))\n",
        "GLossb= tf.Variable(tf.zeros(shape=[output_shape]))\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xguK-SPLUrkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data = validation_data[0:1000,:]\n",
        "valid_validation_data_label = validation_label_one_hot[0:1000,:]\n",
        "valid_test_data = validation_data[1000:,:]\n",
        "valid_test_data_label = validation_label_one_hot[1000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgzAMkJCXVq2",
        "colab_type": "code",
        "outputId": "ad41a0fd-6971-4f93-ad97-0f1e92e2fba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data_label"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "qT_XdektXjmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plt.scatter(np.argmax(valid_validation_data_label,axis = 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RS8J8lVeXGoK",
        "colab_type": "code",
        "outputId": "53aa9b55-3bfd-4ada-e2a4-e88e7007bae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(valid_validation_data_label,axis = 1))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([262.,   0.,  89.,   0., 152.,   0., 103.,   0., 165., 229.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6CWpuf8GKxxT4b3UCxMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AJtaOCHUc8Us",
        "colab_type": "code",
        "outputId": "9228c101-0ea6-437c-be12-841f5a65e9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3104, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "KrEu6ndlUZh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "metadata": {
        "id": "Q5TyGgw4Ub9n",
        "colab_type": "code",
        "outputId": "9e05e276-7538-48be-f3a6-1dd4769f268e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91409
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "# hid_neuron = [90]\n",
        "num_steps = 30000\n",
        "batch_size = 2056\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "plot_every = 100\n",
        "number_of_epoch = 4000\n",
        "# learning_rate = 0.001\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "number_of_ex = train_data.shape[0]\n",
        "\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "weights = {\n",
        "    'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "    'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "    'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "}\n",
        "saver = tf.train.Saver()\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "ValidAccuracy_Test_track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "    \n",
        "for wL1 in range(1,7):\n",
        "  for WL2 in range(1,wL1+1):\n",
        "    for WL3 in range(0,2):\n",
        "\n",
        "        wLoss1 = wL1\n",
        "        wLoss2 = WL2\n",
        "        wLoss3 = WL3\n",
        "        loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "        loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "        loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "        loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        ### Initialization and running the model\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            best_accuracy_valid = 0\n",
        "            for ep in range(0,number_of_epoch):\n",
        "              if ep<1000:\n",
        "                learn = .001\n",
        "              elif ep >=1000 and ep <= 2000:\n",
        "                learn = .001\n",
        "              else:\n",
        "                learn = .0001\n",
        "              for step in range(0, total_steps_for_one_pass):\n",
        "\n",
        "                if step>=number_of_ex//batch_size:\n",
        "                  batch_x, batch_y = train_data[step*batch_size:,:],train_label_one_hot[step*batch_size:,:]\n",
        "#                   print(step,'Finishing',step*batch_size )\n",
        "                  step = 0\n",
        "#                   print('finishing')\n",
        "\n",
        "                else:\n",
        "\n",
        "                  start = step*batch_size\n",
        "                  finish = (step+1)*batch_size\n",
        "#                   print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "                  batch_x, batch_y = train_data[step:finish,:],train_label_one_hot[step:finish,:]\n",
        "        #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})              \n",
        "\n",
        "\n",
        "\n",
        "  #                 batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "  #                 sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "              if ep % plot_every == 0:\n",
        "                  train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                  print(\"epoch \" + str(ep) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                  train_losses.append(train_loss)\n",
        "                  validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                  if ep%plot_every == 0:\n",
        "                    print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                    print()\n",
        "                    if (validation_accuracy >= best_accuracy_valid):\n",
        "                      best_accuracy_valid = validation_accuracy\n",
        "                      saver.save(sess, './statimgTrackAdam')\n",
        "                      G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "            print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "            ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "            this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "            W_track.append(this_params)\n",
        "            # code for checking accuracy of valid_test\n",
        "            validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "            ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "            print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "            print(\"=\"*50)\n",
        "            print(\"W1 = {} ...\".format(wLoss1))\n",
        "            print(\"W2 = {} ...\".format(wLoss2))\n",
        "            print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "            print(\"*\"*50)\n",
        "            print(\"=\"*50)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss= 0.6358842, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.068768576, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05676273, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.051263094, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.047038663, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04352328, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04067651, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038240712, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.036144484, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034325212, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032641873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030934151, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029285608, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027736433, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02626765, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024822293, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023507114, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022244634, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021037249, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019869251, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018725684, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0186139, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01849271, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018368354, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018238477, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018101845, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017959893, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017812924, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017662557, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017509976, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017356055, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017195048, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017032344, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016865926, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016695432, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016522923, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01634941, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01617537, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01600146, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015824165, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.1870068, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.08935236, training acc= 96.56488299369812%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.06041733, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.051809162, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.04607153, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.0400772, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.03633765, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 700, training loss= 0.03383881, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.032024086, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.03048995, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.029098108, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.027824251, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.026672592, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.025696976, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024665086, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.023703126, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.022678552, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.021703621, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.020736726, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019831298, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018993452, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018916102, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018827517, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018734524, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018636243, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018531706, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018421687, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0183063, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018185861, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018060299, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017929291, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017792247, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017650563, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017499646, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3400, training loss= 0.017340709, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017176516, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017010538, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.016846003, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01667974, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.016513063, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.4556311, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.074582696, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.06293614, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.057584114, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05321462, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.04962176, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.046578333, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.04376994, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.041244805, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.038835965, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03647993, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.034269977, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.032080557, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03005588, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.028159073, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026411105, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02477341, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023194958, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021674288, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020213077, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018801838, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018662248, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018515345, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018364336, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018202815, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018036515, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017865919, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017692238, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017512795, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017329212, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01714546, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016959682, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016774485, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.01659153, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016407039, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016221495, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016037073, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015851706, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015668353, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0154851675, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.91391665, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.08753498, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.06868903, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.059411578, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.053347874, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 500, training loss= 0.04878254, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.044851083, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 700, training loss= 0.04197078, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.039860867, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.03786941, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.036182214, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.034957405, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03376054, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03263496, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03275953, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.03117517, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.029605508, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.028578412, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02758209, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.026584875, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.025631046, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.025536649, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.025434704, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.025327176, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.025213428, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.025090668, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.02495993, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.024822894, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2800, training loss= 0.02467893, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2900, training loss= 0.024529854, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.024373082, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.024209222, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.024038874, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.023858823, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.023671031, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.023479464, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.023281595, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.023078172, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.022870062, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.02265485, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.6358842, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.068768576, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05676273, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.051263094, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.047038663, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04352328, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04067651, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038240712, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.036144484, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034325212, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032641873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030934151, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029285608, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027736433, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02626765, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024822293, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023507114, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022244634, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021037249, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019869251, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018725684, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0186139, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01849271, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018368354, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018238477, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018101845, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017959893, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017812924, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017662557, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017509976, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017356055, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017195048, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017032344, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016865926, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016695432, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016522923, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01634941, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01617537, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01600146, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015824165, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.968149, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.08681701, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.06085227, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.052999765, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.04688416, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 500, training loss= 0.04244801, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.03902253, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 700, training loss= 0.036591567, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.034706403, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 900, training loss= 0.033140365, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03143498, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.02966325, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1200, training loss= 0.028166031, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.026928386, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.025747696, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024599822, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023553878, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022540357, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021542214, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020615228, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019690631, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019598836, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019499598, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2300, training loss= 0.019395297, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.019285249, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019168248, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019044895, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018915772, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018783689, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018646602, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01850443, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.018358102, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018205775, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018049479, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.017890302, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017727299, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017563941, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01739833, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.017226102, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.017056106, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.36523163, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07561716, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.06515027, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.059402976, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05493491, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.05112283, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.047600806, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.044411138, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.04139268, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.038409747, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03574101, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033170138, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030790769, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.028562665, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026436377, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024520842, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.022728639, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.021058055, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01948205, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017993778, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.016659003, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.016530987, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.016396595, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.016256073, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.016113276, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015965901, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015813608, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.01566049, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.015503116, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.015343913, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.015184308, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.015019954, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.014857492, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.014697719, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.014539243, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.014381638, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.014223175, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.014066911, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.013910123, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01375649, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.75012535, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.08740754, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.071423896, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.06396835, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.05744052, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.052857764, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.04887664, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 700, training loss= 0.045623273, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 800, training loss= 0.04327577, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.041390732, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.0396602, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03790622, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.036153514, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.034421198, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03275188, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0320588, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.030359456, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.029307643, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02822387, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.027121494, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.026057914, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.02595203, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.02583746, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.025715953, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.02558668, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.02545017, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.025305063, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.025153274, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.024992367, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.024821987, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.02464545, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.02446681, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.024272276, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.024078524, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.023881523, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.02367882, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.023469059, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.023252806, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.023030512, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.02280673, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 87.3111801147461 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5277516, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07208062, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.061190296, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.055737764, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.051343072, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.04785153, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.044870663, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.04210223, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.03953535, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.0371797, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.035110723, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033155005, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.031321067, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029557446, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027811805, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02620737, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02472034, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02324911, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021782273, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020410148, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01913227, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019011542, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018878506, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018739138, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018591372, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018437646, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018277122, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018113203, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017944794, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017772036, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017596494, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017419474, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017241213, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017063353, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016883157, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016703494, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016520126, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01633935, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016156383, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015975889, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.82276714, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.0851163, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.06588206, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.056972902, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.05165787, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 500, training loss= 0.047890916, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.04495381, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.04221603, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.03965045, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.037575133, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03570983, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.034280002, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03287668, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03155774, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.030224709, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.028937437, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.027690966, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02643862, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02526862, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.02416817, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.023102557, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.022997623, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.022878747, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2300, training loss= 0.022750672, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.022616297, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.022476075, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.02232833, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.022168465, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.022000924, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2900, training loss= 0.021828655, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.021646567, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.02146149, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.02127793, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.021082975, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.020882212, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.020674646, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.020452272, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.02023026, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.020004902, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.019778311, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 86.70694732666016 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.6358842, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.068768576, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05676273, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.051263094, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.047038663, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04352328, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04067651, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038240712, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.036144484, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034325212, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032641873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030934151, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029285608, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027736433, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02626765, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024822293, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023507114, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022244634, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021037249, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019869251, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018725684, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0186139, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01849271, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018368354, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018238477, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018101845, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017959893, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017812924, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017662557, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017509976, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017356055, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017195048, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017032344, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016865926, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016695432, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016522923, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01634941, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01617537, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01600146, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015824165, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.87438846, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.08399807, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.06126126, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.051705856, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.04722093, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.043662682, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.040405147, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.037875425, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.038980592, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.034082025, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032373227, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030758576, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029230468, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027861612, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026589936, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.025387798, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02421558, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02313408, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.022083452, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021055408, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.02008363, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.01998425, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01987723, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.019762222, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01964064, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019514343, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019383332, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.019246003, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.01910359, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01895566, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01880113, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.018642424, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018480629, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018315127, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018147748, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017975982, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017805329, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.017631909, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.017454302, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01727206, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 87.3111801147461 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.31117377, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.075893335, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.0662246, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.060292196, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.0556495, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.0517095, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.048001274, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.044580292, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.04145787, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.038448595, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.035611592, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.032943796, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030457575, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.028130699, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026000032, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024015106, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.022239765, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.020574622, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.019053722, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.01759587, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.016240615, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.016101215, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01595731, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.015812049, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.015667986, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015519834, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.0153686805, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.015214979, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2800, training loss= 0.015061622, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014900557, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.014737152, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.014574866, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.014412093, training acc= 100.0%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.014251885, training acc= 100.0%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.014092787, training acc= 100.0%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013932161, training acc= 100.0%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.013772085, training acc= 100.0%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.013595921, training acc= 100.0%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.013423696, training acc= 100.0%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0132490685, training acc= 100.0%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 86.70694732666016 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.64097047, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.086882316, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.07371774, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.0658131, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05947971, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.054395344, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.05036141, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.047499716, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 800, training loss= 0.045027137, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.042751662, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.040534616, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.038536526, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.036988366, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.035443727, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03387542, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.032284707, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.030656695, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.029032959, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.027460484, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.025862277, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.024397254, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.024254119, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.024097387, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.023930661, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.023757068, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.023578782, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.02339076, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.023197465, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.022999052, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.02279875, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.022597043, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.022392783, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.02218578, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.021975406, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.021764345, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.02154782, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.021326302, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.021105817, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.020882403, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.020656629, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "Valid acc= 90.100006 %\n",
            "Validation Accuracy Test 86.70694732666016 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.4556311, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.074582696, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.06293614, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.057584114, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05321462, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.04962176, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.046578333, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.04376994, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.041244805, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.038835965, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03647993, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.034269977, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.032080557, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03005588, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.028159073, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026411105, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02477341, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023194958, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021674288, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020213077, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018801838, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018662248, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018515345, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018364336, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018202815, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018036515, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017865919, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017692238, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017512795, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017329212, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01714546, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016959682, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016774485, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.01659153, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016407039, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016221495, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016037073, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015851706, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015668353, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0154851675, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.71875584, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.08563521, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.06798671, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.05961182, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.053965066, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04982191, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.04660187, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.043936927, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.041634824, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.039774917, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03796732, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03604158, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03443272, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03275026, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.031316135, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.029900977, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.028543627, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.027220735, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02592833, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.024717325, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.023487737, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.023366002, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.023234943, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.023095809, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 2400, training loss= 0.022948673, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 2500, training loss= 0.022795502, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 2600, training loss= 0.02263509, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 2700, training loss= 0.022469198, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 2800, training loss= 0.022296714, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 2900, training loss= 0.022117632, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.021935465, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.021747043, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.021551173, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.02134938, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.021145664, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.020945214, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.020743039, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.02054575, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.02035041, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.020156205, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 86.40483856201172 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.55858386, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.071488075, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.060310025, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.054719932, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.05027222, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.046727035, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.043775816, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.041043364, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.038583264, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.03642597, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.034431316, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.032570194, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030772598, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029125657, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027535712, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026029872, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.024605582, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023183856, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021837117, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020561544, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019328794, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019209314, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019081231, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018946603, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01880684, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018658224, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018501878, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018336814, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018165268, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017989775, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017809253, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017622596, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017434595, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017241474, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.01704564, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016850106, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016656771, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.016466863, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016276876, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.016087977, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 86.40483856201172 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.77694786, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.083027214, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.06381396, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.055590227, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.050157264, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 500, training loss= 0.046377815, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.043459345, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.040926285, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 800, training loss= 0.03879993, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.036988772, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03533695, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03344638, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.031754415, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.030058034, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.028547341, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.027059648, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025645303, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.024331363, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.023036381, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021847697, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.02071723, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.020608727, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.020487003, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.020357706, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.020223076, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.020081747, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019934604, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.019777982, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.019615637, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.019449621, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.019280778, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.019108897, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018933125, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018753331, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018572453, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.018385455, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01819541, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.018004587, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.017812828, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01762373, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 86.10271453857422 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.6358842, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.068768576, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05676273, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.051263094, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.047038663, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04352328, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04067651, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038240712, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.036144484, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034325212, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032641873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030934151, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029285608, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027736433, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02626765, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024822293, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023507114, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022244634, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021037249, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019869251, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018725684, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0186139, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01849271, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018368354, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018238477, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018101845, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017959893, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017812924, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017662557, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017509976, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017356055, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017195048, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017032344, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016865926, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016695432, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016522923, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01634941, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01617537, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01600146, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015824165, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.8227148, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.08655669, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.062498286, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.054273933, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.048325878, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.043836474, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 600, training loss= 0.040573403, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 700, training loss= 0.038004667, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.035751373, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.033920992, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032256763, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030638881, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.02903252, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027584616, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02631791, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.025026465, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023823272, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022709915, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021669129, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020651748, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019681009, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019584872, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019479284, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01936748, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.019249039, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019122215, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018991861, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018854959, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018712852, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018567182, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.018414, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.018256184, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018098444, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017934928, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.01777032, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017598948, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01743056, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.017260635, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01708928, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.016920023, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 87.3111801147461 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.275005, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.07582088, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.06688771, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.060677327, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05604906, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.051919263, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.048148956, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.044624414, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.041307177, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.038095243, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.035071142, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.032276947, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029694809, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027337454, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.025138943, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.023162438, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.021347692, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 1700, training loss= 0.019687513, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 1800, training loss= 0.018144365, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 1900, training loss= 0.01669758, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 2000, training loss= 0.015340898, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.015216204, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.015081739, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.0149422465, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01480193, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.014660642, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.01451743, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.014373387, training acc= 99.90457892417908%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.014229382, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.014083876, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.013938095, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.013788087, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.013638215, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.013488706, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.013339131, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.013189013, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.013040106, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012895042, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.012750483, training acc= 100.0%\n",
            "Validation Accuracy valid 88.5999984741211 ...\n",
            "\n",
            "epoch 3900, training loss= 0.012602749, training acc= 100.0%\n",
            "Validation Accuracy valid 88.5999984741211 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 86.40483856201172 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5629148, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.08688541, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.073087215, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.066311434, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.060504757, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.055909965, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.05224882, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.04935742, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 800, training loss= 0.046688013, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.044384763, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.042205162, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.04016523, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03818264, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03612407, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03427065, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.032338277, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.030577058, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02885209, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0272533, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.025672065, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.024145119, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.023993148, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2200, training loss= 0.023829881, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2300, training loss= 0.023657609, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.023478331, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.02329189, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.023097683, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.022898443, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.022688974, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.022474008, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.022254016, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.022033898, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.021814762, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.021595385, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3400, training loss= 0.021375038, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3500, training loss= 0.021152748, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.02093088, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.020716898, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.020503638, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.0202911, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 86.70694732666016 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.40391743, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07536153, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.064550616, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.05884398, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.054392643, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.050704554, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.04739644, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.04433683, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.04154246, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.038781986, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03608833, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033606827, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03126328, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029094018, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027124405, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.025250776, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023416063, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02170489, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02004946, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.018542832, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.017162658, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.017028756, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2200, training loss= 0.016886147, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2300, training loss= 0.016737297, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2400, training loss= 0.016584313, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2500, training loss= 0.016426606, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2600, training loss= 0.01626553, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2700, training loss= 0.01609811, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2800, training loss= 0.015923394, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01575078, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.015577932, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.015404804, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.015232133, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.015059557, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.014885919, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.014712807, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.014543594, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.0143726375, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014201457, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014030722, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.3111801147461 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.64075387, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.08521726, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.069837764, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.06167133, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.055859044, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.051627677, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.04849575, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 700, training loss= 0.045671694, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.04312969, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.04092912, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.039038856, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03711807, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.035326667, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03352628, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03179602, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.030137064, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02854982, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1700, training loss= 0.027063185, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.025669953, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.024316637, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.02297868, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.022844411, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.022697814, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.022543997, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.02238445, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.022217123, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.022044607, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.021869987, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.021689286, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.021501586, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.021310546, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.021118928, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.020921208, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.020719748, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.020515723, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.020309981, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.02010729, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.019902827, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.019694483, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3900, training loss= 0.019485157, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 85.80060577392578 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.50072527, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07390842, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.061909698, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.056582805, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.05218662, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.04873224, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.045674086, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.042767584, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.040186167, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.03785857, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.035663735, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033602454, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03160722, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029725345, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027938973, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026286721, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.024732659, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023233093, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021828849, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020497456, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019205293, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019072935, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018929545, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018779768, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018622313, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01846215, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018296951, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018126523, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.0179517, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01777069, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017585715, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017400604, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017213847, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017027203, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016837373, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016648836, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01646074, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.016274417, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016088396, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015905254, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7010997, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.083220385, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.065913185, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.057588425, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.052545745, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.0487821, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.04563743, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 700, training loss= 0.042884313, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.0403745, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03819978, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03601234, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033999432, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.032169573, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.030491242, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02893355, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.027480995, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1600, training loss= 0.026101837, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.024767121, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1800, training loss= 0.023486175, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.022230202, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.02100519, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.020888934, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.020758549, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.02062306, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.02048103, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.020331586, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.020174915, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.02001278, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.019844256, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2900, training loss= 0.019672995, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01949502, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.0193164, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.019132908, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018946895, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018758306, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.018572798, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.018390158, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01820136, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.018007394, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.017812917, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 86.70694732666016 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5757597, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07107416, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.059885938, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.054182813, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.04961631, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.046105552, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04320205, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.040630687, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 800, training loss= 0.03813635, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.035988092, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.033978753, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.032006968, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030133775, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1300, training loss= 0.028341468, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026580658, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024895849, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023313127, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.021843925, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.02044962, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.01910894, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.017827706, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.017710188, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.017581616, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.017446047, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.01730711, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.017161915, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017002873, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.016849024, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2800, training loss= 0.01668915, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2900, training loss= 0.016524741, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.016354855, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016180808, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016004505, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.015829483, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.01565036, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.01547027, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015287095, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015102006, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.014922758, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01473978, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.75001204, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.085250914, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.06278429, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.055207442, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.05018468, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.045893535, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.04271036, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.040396057, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.038221564, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.03617887, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.034271408, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.032460168, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030770734, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029205656, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027734963, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02636059, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025049258, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023806691, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.022577131, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.02139345, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.020252792, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2100, training loss= 0.020140363, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.020018788, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2300, training loss= 0.019891744, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.019758409, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019618733, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019472351, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.019318637, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.019159364, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018996393, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.018831484, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.018662129, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018490363, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018317044, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018138727, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017955102, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017770257, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.017584128, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.017397627, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.017211327, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 87.3111801147461 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.6358842, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.068768576, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05676273, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.051263094, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.047038663, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04352328, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04067651, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038240712, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.036144484, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034325212, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032641873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030934151, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029285608, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027736433, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02626765, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024822293, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023507114, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022244634, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021037249, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019869251, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018725684, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0186139, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01849271, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018368354, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018238477, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018101845, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017959893, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017812924, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017662557, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017509976, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017356055, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017195048, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017032344, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016865926, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016695432, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016522923, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01634941, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01617537, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01600146, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015824165, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.789166, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.08421892, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.06057107, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 300, training loss= 0.053340014, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.04773289, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.043644845, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04037608, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 700, training loss= 0.038019575, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 800, training loss= 0.036044486, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.034240317, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032538757, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030953746, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029454319, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.028112119, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026851049, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.025641775, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.024443785, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023268396, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.022131914, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021009266, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01992887, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019825304, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019712776, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.01959258, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.019463912, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019323347, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019175649, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.019026013, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018872976, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01871402, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.018546358, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.018371424, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018193563, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018006686, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3400, training loss= 0.017823756, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017637856, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017451018, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3700, training loss= 0.017259495, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3800, training loss= 0.017067466, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3900, training loss= 0.016878694, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 87.9154052734375 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.24917345, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.07568352, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.066889435, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 300, training loss= 0.060682695, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.05600395, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.051822636, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.047894057, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.044277657, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.040777277, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.03731367, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03414712, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.031213101, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.028606638, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1300, training loss= 0.026268866, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.024073325, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.022087196, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.020299012, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.018620055, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.0 ...\n",
            "\n",
            "epoch 1800, training loss= 0.017038919, training acc= 99.80915784835815%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 1900, training loss= 0.015614713, training acc= 100.0%\n",
            "Validation Accuracy valid 88.9000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.014385348, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.014271342, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.014147331, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.014020731, training acc= 100.0%\n",
            "Validation Accuracy valid 88.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.013891037, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.013759666, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.013625768, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.013487567, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.013348883, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 2900, training loss= 0.0131995315, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 3000, training loss= 0.013052328, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.012906716, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.012760933, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.012619714, training acc= 100.0%\n",
            "Validation Accuracy valid 88.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.012485092, training acc= 100.0%\n",
            "Validation Accuracy valid 88.9000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.012326297, training acc= 100.0%\n",
            "Validation Accuracy valid 88.9000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.012173998, training acc= 100.0%\n",
            "Validation Accuracy valid 89.0 ...\n",
            "\n",
            "epoch 3700, training loss= 0.012027684, training acc= 100.0%\n",
            "Validation Accuracy valid 89.0 ...\n",
            "\n",
            "epoch 3800, training loss= 0.011886925, training acc= 100.0%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "epoch 3900, training loss= 0.011745221, training acc= 100.0%\n",
            "Validation Accuracy valid 89.0999984741211 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 85.80060577392578 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.504418, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.087402806, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.07349178, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.06615071, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.060361654, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.055813592, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.052305404, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.04919141, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.046412166, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.04392541, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.041461304, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1100, training loss= 0.03919213, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03704189, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03500654, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.032974422, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.031110236, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.029355774, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.027687699, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.026039533, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1900, training loss= 0.024367457, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.02276592, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.022613639, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.02244557, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.02227202, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.022090467, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.021912565, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.021725181, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.021530744, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.021330101, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2900, training loss= 0.021125762, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.020918604, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.020703519, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.020482402, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.020261511, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.020043304, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.019824214, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.019604517, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.019383306, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.019165253, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3900, training loss= 0.018948084, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 85.19637298583984 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.36523163, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07561716, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 200, training loss= 0.06515027, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.059402976, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05493491, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.05112283, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.047600806, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.044411138, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.04139268, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.038409747, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03574101, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033170138, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.030790769, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.028562665, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026436377, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024520842, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.022728639, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.021058055, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.01948205, training acc= 99.61832165718079%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.017993778, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.016659003, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2100, training loss= 0.016530987, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2200, training loss= 0.016396595, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2300, training loss= 0.016256073, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.016113276, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.015965901, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.015813608, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.01566049, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.015503116, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.015343913, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.015184308, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.015019954, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.014857492, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.014697719, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.014539243, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.014381638, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3600, training loss= 0.014223175, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.014066911, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.013910123, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.01375649, training acc= 99.71374273300171%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.58021885, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.08485581, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.070607975, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.06262731, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.057057984, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.053207815, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.049690235, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.04686024, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.04426555, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.041970003, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.039795686, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.0378581, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03600596, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.034263887, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.03254032, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1500, training loss= 0.030838616, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.029220024, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.027674805, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.026192168, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.02471016, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.023267144, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.023126064, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.02296949, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.02280894, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.022641685, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.022469027, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.022288555, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2700, training loss= 0.022102108, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.021911602, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.021715147, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.021513795, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.021309191, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.0211038, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.02089599, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.020687165, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.020474719, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.020261569, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.02005119, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3800, training loss= 0.019839698, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.019626712, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 86.10271453857422 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.4556311, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.074582696, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.06293614, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.057584114, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05321462, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.04962176, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.046578333, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 700, training loss= 0.043769944, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.0412448, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.038835965, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.036479935, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.034269977, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.032080553, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.03005588, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.028159073, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026411103, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.024773411, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1700, training loss= 0.023194958, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021674288, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020213077, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018801836, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.018662248, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018515343, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018364336, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018202813, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018036515, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017865919, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017692238, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017512793, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01732921, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01714546, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016959682, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016774485, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016591528, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016407037, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016221495, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016037071, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015851706, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015668353, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015485169, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.64055645, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.08305522, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.067101434, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.059249263, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.054004997, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.04988301, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 600, training loss= 0.04660661, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.043897394, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.04148894, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.03928579, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.037144862, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.035187062, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03330522, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.031416453, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1400, training loss= 0.029672427, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.028105164, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1600, training loss= 0.026612062, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1700, training loss= 0.025193218, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0238436, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1900, training loss= 0.022539316, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.021238621, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.021112034, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.020975895, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.02083449, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.020685054, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.020525197, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.020361053, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.02019379, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.020019595, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2900, training loss= 0.019843232, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01966124, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3100, training loss= 0.01947409, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.019284928, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.019089451, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018895276, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.018697469, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.018498102, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3700, training loss= 0.018299049, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.018098596, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.017896308, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 86.40483856201172 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5277516, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07208062, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.061190296, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.055737764, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 400, training loss= 0.051343072, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.04785153, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.044870663, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.04210223, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.03953535, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.0371797, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1000, training loss= 0.035110723, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033155005, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1200, training loss= 0.031321067, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029557446, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027811805, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02620737, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02472034, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02324911, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021782273, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020410148, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2000, training loss= 0.01913227, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019011542, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2200, training loss= 0.018878506, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018739138, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018591372, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018437646, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018277122, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018113203, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017944794, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017772036, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017596494, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017419474, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017241213, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017063353, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016883157, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016703494, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.016520126, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01633935, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016156383, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015975889, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.4 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.6903865, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.08327955, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.06620173, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.05688604, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.05165504, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 500, training loss= 0.04755256, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.04430182, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.041454066, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.039053667, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 900, training loss= 0.03717649, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03508538, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1100, training loss= 0.033427697, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03178624, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1300, training loss= 0.030195579, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1400, training loss= 0.028712243, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.027246257, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025831535, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.024460262, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.023171553, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021914367, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2000, training loss= 0.020658432, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.020533867, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.0204001, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.020260485, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.020115977, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2500, training loss= 0.019963568, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019805945, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.019641843, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2800, training loss= 0.01947304, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 2900, training loss= 0.019298697, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3000, training loss= 0.019118823, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.018937197, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018753076, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018567074, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018380258, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.018191451, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01799966, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3700, training loss= 0.017803743, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01759735, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3900, training loss= 0.017384253, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 86.40483856201172 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5867, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.070760295, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.05940365, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.053727742, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.04929366, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 500, training loss= 0.04580903, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.042952064, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.040331498, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.03794814, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 900, training loss= 0.035802383, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03381755, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.031877726, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03003333, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1300, training loss= 0.028332334, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026718125, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.02506409, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.02353231, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1700, training loss= 0.02207078, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.020667195, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019322127, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018046701, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2100, training loss= 0.017921764, training acc= 99.42747950553894%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2200, training loss= 0.017785698, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.017644519, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.017497128, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.01734531, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.01718763, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017028602, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.016865585, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.01669558, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01652137, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3100, training loss= 0.016344313, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.016163796, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3300, training loss= 0.015982606, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.015801307, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.015622443, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.015442632, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3700, training loss= 0.015262974, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3800, training loss= 0.015085595, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3900, training loss= 0.014909347, training acc= 99.52290058135986%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.9154052734375 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7313325, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.082581095, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 200, training loss= 0.06389211, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.05581326, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.05020797, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 500, training loss= 0.045988746, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 600, training loss= 0.042381763, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 700, training loss= 0.039951686, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.4000015258789 ...\n",
            "\n",
            "epoch 800, training loss= 0.037719257, training acc= 97.5190818309784%\n",
            "Validation Accuracy valid 90.5 ...\n",
            "\n",
            "epoch 900, training loss= 0.035333022, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.033455096, training acc= 98.09160232543945%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1100, training loss= 0.031836104, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.03041949, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.029100455, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1400, training loss= 0.027833138, training acc= 98.47328066825867%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1500, training loss= 0.026617061, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1600, training loss= 0.025406063, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1700, training loss= 0.024186254, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.022973571, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1900, training loss= 0.021800337, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 2000, training loss= 0.020654123, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2100, training loss= 0.020540431, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2200, training loss= 0.020418186, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2300, training loss= 0.020289512, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.020153701, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2500, training loss= 0.020013355, training acc= 98.85495901107788%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2600, training loss= 0.019868257, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2700, training loss= 0.019716673, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2800, training loss= 0.01956069, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 2900, training loss= 0.019398678, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.019233996, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.019069929, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.018898247, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3300, training loss= 0.018721208, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 3400, training loss= 0.018541751, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3500, training loss= 0.018363494, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01818429, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3700, training loss= 0.018005095, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3800, training loss= 0.017826017, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.017644722, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "Valid acc= 90.5 %\n",
            "Validation Accuracy Test 87.00906372070312 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.6358842, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.068768576, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.05676273, training acc= 96.94656729698181%\n",
            "Validation Accuracy valid 89.30000305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.051263094, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.047038663, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 500, training loss= 0.04352328, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 90.30000305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.04067651, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 700, training loss= 0.038240712, training acc= 97.42366671562195%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 800, training loss= 0.036144484, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.034325212, training acc= 97.80534505844116%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1000, training loss= 0.032641873, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030934151, training acc= 97.99618124961853%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029285608, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027736433, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1400, training loss= 0.02626765, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.024822293, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023507114, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022244634, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021037249, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 1900, training loss= 0.019869251, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2000, training loss= 0.018725684, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0186139, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 2200, training loss= 0.01849271, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2300, training loss= 0.018368354, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018238477, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018101845, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2600, training loss= 0.017959893, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.017812924, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2800, training loss= 0.017662557, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2900, training loss= 0.017509976, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3000, training loss= 0.017356055, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017195048, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017032344, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3300, training loss= 0.016865926, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3400, training loss= 0.016695432, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3500, training loss= 0.016522923, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3600, training loss= 0.01634941, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3700, training loss= 0.01617537, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 3800, training loss= 0.01600146, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 3900, training loss= 0.015824165, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "Valid acc= 90.3 %\n",
            "Validation Accuracy Test 87.61328887939453 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7658224, training acc= 96.37404680252075%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.07967213, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.062127378, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 300, training loss= 0.05402406, training acc= 97.04198241233826%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 400, training loss= 0.04873484, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 500, training loss= 0.044697896, training acc= 97.13740348815918%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.04131275, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 700, training loss= 0.038537085, training acc= 97.32824563980103%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 800, training loss= 0.036361527, training acc= 97.2328245639801%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.03434968, training acc= 97.61450290679932%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1000, training loss= 0.03255836, training acc= 97.70992398262024%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1100, training loss= 0.030895513, training acc= 97.90076613426208%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1200, training loss= 0.029297385, training acc= 98.18702340126038%\n",
            "Validation Accuracy valid 90.20000457763672 ...\n",
            "\n",
            "epoch 1300, training loss= 0.027860902, training acc= 98.2824444770813%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1400, training loss= 0.026525274, training acc= 98.37786555290222%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1500, training loss= 0.025221335, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 1600, training loss= 0.023976186, training acc= 98.56870174407959%\n",
            "Validation Accuracy valid 90.0 ...\n",
            "\n",
            "epoch 1700, training loss= 0.022753868, training acc= 98.66412281990051%\n",
            "Validation Accuracy valid 90.10000610351562 ...\n",
            "\n",
            "epoch 1800, training loss= 0.021591393, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 1900, training loss= 0.020488355, training acc= 98.75954389572144%\n",
            "Validation Accuracy valid 89.9000015258789 ...\n",
            "\n",
            "epoch 2000, training loss= 0.019437399, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.019329676, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2200, training loss= 0.019214842, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2300, training loss= 0.019092117, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.80000305175781 ...\n",
            "\n",
            "epoch 2400, training loss= 0.018965317, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2500, training loss= 0.018832972, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.70000457763672 ...\n",
            "\n",
            "epoch 2600, training loss= 0.018693192, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2700, training loss= 0.018548274, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2800, training loss= 0.018394997, training acc= 98.9503800868988%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 2900, training loss= 0.018226312, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3000, training loss= 0.01806113, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3100, training loss= 0.017897142, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3200, training loss= 0.017730197, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3300, training loss= 0.017554646, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3400, training loss= 0.017379152, training acc= 99.04580116271973%\n",
            "Validation Accuracy valid 89.4000015258789 ...\n",
            "\n",
            "epoch 3500, training loss= 0.017201487, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3600, training loss= 0.017020423, training acc= 99.14122223854065%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3700, training loss= 0.016839093, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.5 ...\n",
            "\n",
            "epoch 3800, training loss= 0.016660348, training acc= 99.23664331436157%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "epoch 3900, training loss= 0.016481657, training acc= 99.33205842971802%\n",
            "Validation Accuracy valid 89.60000610351562 ...\n",
            "\n",
            "Valid acc= 90.200005 %\n",
            "Validation Accuracy Test 87.3111801147461 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BrkZIJvoUb6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "adf2eaf7-5184-41ee-ba64-b79639015e5e"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(ValidAccuracy_Track)\n",
        "plt.plot(ValidAccuracy_Test_track)\n",
        "\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8Y8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u3UFxL_bUb3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a190f75-b464-4410-edcd-bb989b502f12"
      },
      "cell_type": "code",
      "source": [
        "print(np.max(ValidAccuracy_Track))\n",
        "print(np.argmax(ValidAccuracy_Track))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90.5\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "54oqBMzcOEmX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Best Wt 5,2,1"
      ]
    },
    {
      "metadata": {
        "id": "SkE0jNoRMpDc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffling_indices_validation_data = np.random.permutation(validation_data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2VQdoHyMo75",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffled_validation_data = validation_data[shuffling_indices_validation_data,:]\n",
        "shuffled_validation_label = validation_label_one_hot[shuffling_indices_validation_data,:]\n",
        "train_valid_combined_shuffled = np.concatenate((train_data, shuffled_validation_data))\n",
        "train_valid_combined_shuffled_label = np.concatenate((train_label_one_hot, shuffled_validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efMdmBhLMo4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "9ab2cfb2-1e16-4009-8137-a2e32f67268e"
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([291.,   0.,  95.,   0., 163.,   0., 109.,   0., 176., 497.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qHMvoOAhMo1G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # keep aside \n",
        "# aside_examples= 300\n",
        "# aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "# aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "# combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "# combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "udUVxeCQMoxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 400\n",
        "aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1InMUDSKT0jx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "736623af-ccbd-4879-dbd5-884a391491f1"
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 150000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 20000\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 1000\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 5\n",
        "wLoss2 = 2\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<1000:\n",
        "        learn = .001\n",
        "      elif ep >=1000 and ep <= 2000:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .0001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 0.76537657, training acc total= 94.29987668991089%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 1000, training loss Total= 0.0747761, training acc total= 96.25774621963501%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2000, training loss Total= 0.052054297, training acc total= 97.24907279014587%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 3000, training loss Total= 0.049848042, training acc total= 97.37298488616943%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 4000, training loss Total= 0.047155544, training acc total= 97.62081503868103%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 5000, training loss Total= 0.044354632, training acc total= 97.74473309516907%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 6000, training loss Total= 0.04150033, training acc total= 97.91821837425232%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 7000, training loss Total= 0.03888306, training acc total= 98.0916976928711%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 8000, training loss Total= 0.03649634, training acc total= 98.26517701148987%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 9000, training loss Total= 0.034279037, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 10000, training loss Total= 0.032239888, training acc total= 98.53779673576355%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 11000, training loss Total= 0.030366449, training acc total= 98.6121416091919%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 12000, training loss Total= 0.028599026, training acc total= 98.68649244308472%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 13000, training loss Total= 0.026854211, training acc total= 98.73605966567993%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 14000, training loss Total= 0.025223225, training acc total= 98.98388981819153%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 15000, training loss Total= 0.023717625, training acc total= 99.05824065208435%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 16000, training loss Total= 0.022101259, training acc total= 99.10780787467957%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 17000, training loss Total= 0.02062425, training acc total= 99.23171997070312%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 18000, training loss Total= 0.01920116, training acc total= 99.30607080459595%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 19000, training loss Total= 0.01788414, training acc total= 99.35563802719116%\n",
            "ValidTest acc= 88.0 %\n",
            "ValidValid acc= 95.71751 %\n",
            "ValidTest acc= 87.75 %\n",
            "==================================================\n",
            "W1\n",
            "5\n",
            "W2\n",
            "2\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9-mykvMcWcHH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### fine tune for higher precision for no. epochs"
      ]
    },
    {
      "metadata": {
        "id": "zWCtawMkT0hU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3570
        },
        "outputId": "c65abbd0-654e-4a02-9b97-046fc1cf02bd"
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 150000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 10000\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 100\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 5\n",
        "wLoss2 = 2\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<1000:\n",
        "        learn = .001\n",
        "      elif ep >=1000 and ep <= 2000:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .0001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 0.76537657, training acc total= 94.29987668991089%\n",
            "ValidTest acc= 87.25 %\n",
            "epoch 100, training loss Total= 0.17020664, training acc total= 94.69640851020813%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 200, training loss Total= 0.13298558, training acc total= 94.8698878288269%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 300, training loss Total= 0.11637167, training acc total= 95.14250159263611%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 400, training loss Total= 0.10546729, training acc total= 95.31598687171936%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 500, training loss Total= 0.098181315, training acc total= 95.51424980163574%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 600, training loss Total= 0.09201851, training acc total= 95.58860063552856%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 700, training loss Total= 0.08683417, training acc total= 95.687735080719%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 800, training loss Total= 0.0823657, training acc total= 95.83643078804016%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 900, training loss Total= 0.0783966, training acc total= 96.03469371795654%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 1000, training loss Total= 0.0747761, training acc total= 96.25774621963501%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 1100, training loss Total= 0.07323474, training acc total= 96.33209705352783%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 1200, training loss Total= 0.068770446, training acc total= 96.43122553825378%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 1300, training loss Total= 0.06624627, training acc total= 96.5055763721466%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 1400, training loss Total= 0.06389571, training acc total= 96.62949442863464%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 1500, training loss Total= 0.06165565, training acc total= 96.7286229133606%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 1600, training loss Total= 0.05957232, training acc total= 96.82775735855103%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 1700, training loss Total= 0.05758182, training acc total= 96.95167541503906%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 1800, training loss Total= 0.055662595, training acc total= 97.10037112236023%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 1900, training loss Total= 0.053826258, training acc total= 97.17472195625305%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 2000, training loss Total= 0.052054297, training acc total= 97.24907279014587%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 2100, training loss Total= 0.051870026, training acc total= 97.24907279014587%\n",
            "ValidTest acc= 87.75 %\n",
            "epoch 2200, training loss Total= 0.051679824, training acc total= 97.27385640144348%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2300, training loss Total= 0.051479433, training acc total= 97.27385640144348%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2400, training loss Total= 0.051269673, training acc total= 97.27385640144348%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2500, training loss Total= 0.051051237, training acc total= 97.29863405227661%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2600, training loss Total= 0.050825063, training acc total= 97.32341766357422%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2700, training loss Total= 0.050591294, training acc total= 97.32341766357422%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2800, training loss Total= 0.05034918, training acc total= 97.34820127487183%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 2900, training loss Total= 0.05010041, training acc total= 97.37298488616943%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 3000, training loss Total= 0.049848042, training acc total= 97.37298488616943%\n",
            "ValidTest acc= 88.0 %\n",
            "epoch 3100, training loss Total= 0.049591552, training acc total= 97.39776849746704%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3200, training loss Total= 0.049332082, training acc total= 97.47211933135986%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3300, training loss Total= 0.04907037, training acc total= 97.47211933135986%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3400, training loss Total= 0.04880447, training acc total= 97.47211933135986%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3500, training loss Total= 0.04853209, training acc total= 97.49690294265747%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3600, training loss Total= 0.048255924, training acc total= 97.52168655395508%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3700, training loss Total= 0.04797944, training acc total= 97.52168655395508%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3800, training loss Total= 0.047704432, training acc total= 97.54647016525269%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 3900, training loss Total= 0.04742913, training acc total= 97.62081503868103%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4000, training loss Total= 0.047155544, training acc total= 97.62081503868103%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4100, training loss Total= 0.046881486, training acc total= 97.62081503868103%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4200, training loss Total= 0.046606887, training acc total= 97.5960373878479%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4300, training loss Total= 0.04633155, training acc total= 97.67038226127625%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4400, training loss Total= 0.04605273, training acc total= 97.71994948387146%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4500, training loss Total= 0.045773398, training acc total= 97.71994948387146%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4600, training loss Total= 0.045494217, training acc total= 97.74473309516907%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 4700, training loss Total= 0.045208286, training acc total= 97.74473309516907%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 4800, training loss Total= 0.04492601, training acc total= 97.74473309516907%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 4900, training loss Total= 0.04464038, training acc total= 97.74473309516907%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 5000, training loss Total= 0.044354632, training acc total= 97.74473309516907%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 5100, training loss Total= 0.044062782, training acc total= 97.76951670646667%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5200, training loss Total= 0.043774117, training acc total= 97.79430031776428%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5300, training loss Total= 0.04348045, training acc total= 97.8438675403595%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5400, training loss Total= 0.043183424, training acc total= 97.8438675403595%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5500, training loss Total= 0.042894155, training acc total= 97.8686511516571%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5600, training loss Total= 0.04261159, training acc total= 97.8686511516571%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5700, training loss Total= 0.042327907, training acc total= 97.8686511516571%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5800, training loss Total= 0.042049736, training acc total= 97.8686511516571%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 5900, training loss Total= 0.04177122, training acc total= 97.91821837425232%\n",
            "ValidTest acc= 88.25 %\n",
            "epoch 6000, training loss Total= 0.04150033, training acc total= 97.91821837425232%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 6100, training loss Total= 0.04123328, training acc total= 97.96777963638306%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6200, training loss Total= 0.040968712, training acc total= 97.99256324768066%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6300, training loss Total= 0.04070255, training acc total= 97.96777963638306%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6400, training loss Total= 0.040438533, training acc total= 98.01734685897827%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6500, training loss Total= 0.040174786, training acc total= 98.01734685897827%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6600, training loss Total= 0.03991116, training acc total= 98.01734685897827%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6700, training loss Total= 0.03964763, training acc total= 97.99256324768066%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6800, training loss Total= 0.039387632, training acc total= 98.06691408157349%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 6900, training loss Total= 0.039132737, training acc total= 98.0916976928711%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7000, training loss Total= 0.03888306, training acc total= 98.0916976928711%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7100, training loss Total= 0.038635988, training acc total= 98.0916976928711%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7200, training loss Total= 0.038389534, training acc total= 98.0916976928711%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7300, training loss Total= 0.038145248, training acc total= 98.0916976928711%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7400, training loss Total= 0.037902623, training acc total= 98.1164813041687%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7500, training loss Total= 0.037660748, training acc total= 98.1164813041687%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7600, training loss Total= 0.037423983, training acc total= 98.16604852676392%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7700, training loss Total= 0.0371899, training acc total= 98.19083213806152%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 7800, training loss Total= 0.036958247, training acc total= 98.21561574935913%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 7900, training loss Total= 0.03672638, training acc total= 98.26517701148987%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8000, training loss Total= 0.03649634, training acc total= 98.26517701148987%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 8100, training loss Total= 0.036268327, training acc total= 98.28996062278748%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 8200, training loss Total= 0.03603803, training acc total= 98.31474423408508%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8300, training loss Total= 0.035812832, training acc total= 98.33952784538269%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8400, training loss Total= 0.035589986, training acc total= 98.3890950679779%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8500, training loss Total= 0.035367463, training acc total= 98.3890950679779%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8600, training loss Total= 0.035146188, training acc total= 98.41387867927551%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8700, training loss Total= 0.034927536, training acc total= 98.41387867927551%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8800, training loss Total= 0.03471052, training acc total= 98.41387867927551%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 8900, training loss Total= 0.034495566, training acc total= 98.41387867927551%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 9000, training loss Total= 0.034279037, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 9100, training loss Total= 0.034064848, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 9200, training loss Total= 0.033853132, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 9300, training loss Total= 0.03364511, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 9400, training loss Total= 0.03344024, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 89.0 %\n",
            "epoch 9500, training loss Total= 0.033237357, training acc total= 98.43866229057312%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 9600, training loss Total= 0.033035707, training acc total= 98.46344590187073%\n",
            "ValidTest acc= 88.75 %\n",
            "epoch 9700, training loss Total= 0.032835975, training acc total= 98.46344590187073%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 9800, training loss Total= 0.03263575, training acc total= 98.48822951316833%\n",
            "ValidTest acc= 88.5 %\n",
            "epoch 9900, training loss Total= 0.0324363, training acc total= 98.51301312446594%\n",
            "ValidTest acc= 88.5 %\n",
            "ValidValid acc= 95.116455 %\n",
            "ValidTest acc= 88.5 %\n",
            "==================================================\n",
            "W1\n",
            "5\n",
            "W2\n",
            "2\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bZXwHMWIbQ4j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### train till 9000 epochs combined data"
      ]
    },
    {
      "metadata": {
        "id": "tQg6Phm6T0e_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 1\n",
        "aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLJITTcvT0ch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "fe0ad35d-ed16-4d05-da32-bc5f9110011d"
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 150000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 9000\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 500\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "        layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "        out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 5\n",
        "wLoss2 = 2\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<1000:\n",
        "        learn = .001\n",
        "      elif ep >=1000 and ep <= 2000:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .0001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "#           validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#           print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "#     validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "#     print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "#     validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#     print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    saver.save(sess, './statlog_letterReducedAdamFinal')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 0.705518, training acc total= 93.54984164237976%\n",
            "epoch 500, training loss Total= 0.119696625, training acc total= 94.88046765327454%\n",
            "epoch 1000, training loss Total= 0.08970777, training acc total= 95.66982388496399%\n",
            "epoch 1500, training loss Total= 0.07214738, training acc total= 96.27875685691833%\n",
            "epoch 2000, training loss Total= 0.059417076, training acc total= 97.04555869102478%\n",
            "epoch 2500, training loss Total= 0.057984717, training acc total= 97.04555869102478%\n",
            "epoch 3000, training loss Total= 0.056567885, training acc total= 97.06810712814331%\n",
            "epoch 3500, training loss Total= 0.05493204, training acc total= 97.18087315559387%\n",
            "epoch 4000, training loss Total= 0.053202674, training acc total= 97.2485363483429%\n",
            "epoch 4500, training loss Total= 0.05141764, training acc total= 97.2485363483429%\n",
            "epoch 5000, training loss Total= 0.04971552, training acc total= 97.36129641532898%\n",
            "epoch 5500, training loss Total= 0.048168533, training acc total= 97.406405210495%\n",
            "epoch 6000, training loss Total= 0.046673853, training acc total= 97.47406244277954%\n",
            "epoch 6500, training loss Total= 0.04525219, training acc total= 97.51917123794556%\n",
            "epoch 7000, training loss Total= 0.04391202, training acc total= 97.51917123794556%\n",
            "epoch 7500, training loss Total= 0.042611167, training acc total= 97.60938286781311%\n",
            "epoch 8000, training loss Total= 0.041488845, training acc total= 97.69959449768066%\n",
            "epoch 8500, training loss Total= 0.040406205, training acc total= 97.76725172996521%\n",
            "Train acc= 0.9781236 %\n",
            "==================================================\n",
            "W1\n",
            "5\n",
            "W2\n",
            "2\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-brfh_zMeNtS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check on test data"
      ]
    },
    {
      "metadata": {
        "id": "rkymr0dpT0aC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ffa128da-d9b6-43b0-fcb3-8d4c868aff1b"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, './statlog_letterReducedAdamFinal')\n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_accuracy), \"%\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedAdamFinal\n",
            "Train acc= 0.9781236 %\n",
            "Test acc= 90.3 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lirrvwQTT0Wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}