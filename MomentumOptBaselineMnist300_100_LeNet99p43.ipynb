{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MomentumOptBaselineMnist300_100_LeNet99p43.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "tMF1ajQrJWRM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/MomentumOptBaselineMnist300_100_LeNet99p43.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0SINGreLFCRz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import packages"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "My4EmvydE3bW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_9T8Y6Y2J4g",
        "colab_type": "code",
        "outputId": "6688ea12-3251-4095-ec95-44cdaa053de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
        "\n",
        "assert(len(X_train) == len(y_train))\n",
        "assert(len(X_validation) == len(y_validation))\n",
        "assert(len(X_test) == len(y_test))\n",
        "\n",
        "print()\n",
        "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "print()\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-566519a95339>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "\n",
            "Image Shape: (28, 28, 1)\n",
            "\n",
            "Training Set:   55000 samples\n",
            "Validation Set: 5000 samples\n",
            "Test Set:       10000 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UU12uT1x2kNS",
        "colab_type": "code",
        "outputId": "49d71ca0-fa9e-44b7-9922-9324e9368547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_validation.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ahO5eMXW2ODm",
        "colab_type": "code",
        "outputId": "c7f925a0-b733-45b2-f4f3-8d1ce904d074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "yy = X_train.reshape(X_train.shape[0],-1)\n",
        "yy.shape\n",
        "# yy = np.reshape(X_train. newshape=(X_train.shape[0],-1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "3goNaizqshmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = X_train.reshape(X_train.shape[0],-1)\n",
        "train_label = y_train\n",
        "validation_data = X_validation.reshape(X_validation.shape[0],-1)\n",
        "validation_label = y_validation\n",
        "test_data = X_test.reshape(X_test.shape[0],-1)\n",
        "test_label = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XdbYPAMNxDZg",
        "colab_type": "code",
        "outputId": "60006330-d10f-4ecb-b7b7-fb8d7c6322b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(train_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5444., 6179., 5470., 5638., 5307., 4987., 5417., 5715., 5389.,\n",
              "        5454.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xSuKAIntmzfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dTAGPqvlFEuQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# np.max(train_data[1,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ovvpmlXeFH1x",
        "outputId": "cbe3abc6-7268-42e8-836a-c44307164565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "train_data_pandas = pd.DataFrame(train_data)\n",
        "train_data_labels = pd.DataFrame(train_label)\n",
        "train_data_pandas.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
              "\n",
              "   778  779  780  781  782  783  \n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "epqBn1YjFlII",
        "outputId": "01df0248-3633-4b9f-d094-cc910cca8cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "train_data_labels.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  7\n",
              "1  3\n",
              "2  4\n",
              "3  6\n",
              "4  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ewLyg3iuFqkO",
        "outputId": "8c3e82eb-62ef-4978-e1ce-037fe681afec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w5wMHmhIFthO",
        "outputId": "de23565b-d7b0-420a-ced7-8f002a11db3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5Jg0BONTGBA1"
      },
      "cell_type": "markdown",
      "source": [
        "#### Combine Validation and train data for MLP classifier - and set validation fraction to 4500/15000 = 0.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8krXltl9GPfv",
        "outputId": "323218a3-3bf8-4f83-9445-739c4e794081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_combined.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NUWNzsz4v04T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bWN_sAWEFNtb"
      },
      "cell_type": "markdown",
      "source": [
        "#### Fit MLP Classifier"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QSdXJQLnFKa2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clf = MLPClassifier(hidden_layer_sizes=(104),validation_fraction=0.3)\n",
        "# clf.fit(train_data, train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnBnT6NdTqyO",
        "colab_type": "code",
        "outputId": "0a2cf50a-130f-4546-e363-41ff3d8e8e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "20*90/36"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xfKh_nDUvj5G",
        "outputId": "8825af72-9c7a-456a-c004-825b24260c45",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf = MLPClassifier(hidden_layer_sizes=(300,100,), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "# clf.fit(train_valid_combined, train_valid_label)\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.27088284\n",
            "Iteration 2, loss = 0.09284897\n",
            "Iteration 3, loss = 0.06362837\n",
            "Iteration 4, loss = 0.04513577\n",
            "Iteration 5, loss = 0.03265040\n",
            "Iteration 6, loss = 0.02556718\n",
            "Iteration 7, loss = 0.01983507\n",
            "Iteration 8, loss = 0.01265771\n",
            "Iteration 9, loss = 0.00954815\n",
            "Iteration 10, loss = 0.00746641\n",
            "Iteration 11, loss = 0.00487287\n",
            "Iteration 12, loss = 0.00280570\n",
            "Iteration 13, loss = 0.00162220\n",
            "Iteration 14, loss = 0.00092841\n",
            "Iteration 15, loss = 0.00073810\n",
            "Iteration 16, loss = 0.00063660\n",
            "Iteration 17, loss = 0.00065059\n",
            "Iteration 18, loss = 0.00056893\n",
            "Iteration 19, loss = 0.00052885\n",
            "Iteration 20, loss = 0.00050482\n",
            "Iteration 21, loss = 0.00049094\n",
            "Iteration 22, loss = 0.00047780\n",
            "Iteration 23, loss = 0.00046712\n",
            "Iteration 24, loss = 0.00045812\n",
            "Iteration 25, loss = 0.00044891\n",
            "Iteration 26, loss = 0.00044162\n",
            "Iteration 27, loss = 0.00043538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(300, 100), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lLNA4D0qGxJi"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "02O8VTAoGqnG",
        "outputId": "b6838ea2-b371-4dde-b8b3-9ea5c5d7b363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "whn8u2m5iY7M"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pii8wXXSG1r7"
      },
      "cell_type": "markdown",
      "source": [
        "#### Validation Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SakclBGkGvI0",
        "outputId": "dce9a21e-2c27-4cb7-8235-f5082ac8fc30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VbIkGX5gG5ZG"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QLo_AzFVG3ca",
        "outputId": "ace5357d-855f-435b-a478-218dd184281e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "InLIF676HEES"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow model using weights initialized from numpy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tcBNfKZNG9Pm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ortxRVBMH7W7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z7mWVCDVEgLm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hid_neuron = [90]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LR62GfKJv_6E",
        "outputId": "1f1235ee-6fbb-4446-933c-a23b6c3e961b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EylNp0IJONbz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Base NN model in tensor flow"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VzJnI_o2xD5C"
      },
      "cell_type": "markdown",
      "source": [
        "#### 36 -> 90 -> 6"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "73Way2v2Pbys"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "`[# This is formatted as code](https://)`\n",
        "```\n",
        "\n",
        "## Train baseline model in tensorflow"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L-hUDOm5xClH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IMHh0nROw5O-",
        "outputId": "f4014bf6-0ec5-4734-f4f7-5ffb0e6c04ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yV4xtxJLvyNj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wj_W9eCBvyKy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TKQ6nMqMvyJD",
        "outputId": "a10ef4b4-478e-437d-9ee5-1d194fe465ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jy2mQcHAEn20",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf.train.GradientDescentOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eHe01FffvyEJ",
        "outputId": "7f9dea8e-7480-47bf-f424-377ca60c3a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2342
        }
      },
      "cell_type": "code",
      "source": [
        "# ## Building the graph - Best!\n",
        "# saver = tf.train.Saver()\n",
        "# learning_rate = 0.001\n",
        "# hid_neuron = [374]\n",
        "# num_steps = 20000\n",
        "# batch_size = 200\n",
        "# train_losses = []\n",
        "# test_acc = []\n",
        "# X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "# Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# def neural_net(x,train = True):\n",
        "#     layer_outputs = []\n",
        "#     layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "#     layer_1 = tf.nn.relu(layer_1)\n",
        "# #     layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "# #     layer_2 = tf.nn.relu(layer_2)\n",
        "#     out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_outputs.append(out_layer)\n",
        "#     return out_layer\n",
        "\n",
        "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X), labels=Y))\n",
        "\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "# train_op = optimizer.minimize(loss)\n",
        "# correct_pred = tf.equal(tf.argmax(neural_net(X), 1), tf.argmax(Y, 1))\n",
        "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#   ### Initialization and running the model\n",
        "# with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "#     best_accuracy_valid = 0\n",
        "#     for step in range(0, num_steps):\n",
        "#         batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "#         sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "#         if step % 1000 == 0:\n",
        "#             train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "#             train_losses.append(train_loss)\n",
        "#             validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "#             if step%1000 == 0:\n",
        "#               print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "#               print()\n",
        "#               if (validation_accuracy >= best_accuracy_valid):\n",
        "#                 best_accuracy_valid = validation_accuracy\n",
        "#                 saver.save(sess, './statlog_letter')\n",
        "#                 test_Accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "#     print(\"Test acc=\",str(test_Accuracy), \"%\")\n",
        "#     print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "\n",
        "#     print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-25-4106bc23d37b>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[200,100] labels_size=[200,10]\n\t [[{{node softmax_cross_entropy_with_logits_sg}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4106bc23d37b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[200,100] labels_size=[200,10]\n\t [[node softmax_cross_entropy_with_logits_sg (defined at <ipython-input-25-4106bc23d37b>:20) ]]\n\nCaused by op 'softmax_cross_entropy_with_logits_sg', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-25-4106bc23d37b>\", line 20, in <module>\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X), labels=Y))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2561, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, axis=dim, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2370, in softmax_cross_entropy_with_logits_v2\n    labels=labels, logits=logits, axis=axis, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2471, in softmax_cross_entropy_with_logits_v2_helper\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7862, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[200,100] labels_size=[200,10]\n\t [[node softmax_cross_entropy_with_logits_sg (defined at <ipython-input-25-4106bc23d37b>:20) ]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5A_PHV3bS7ui"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8RFK2bW4JZ7w"
      },
      "cell_type": "markdown",
      "source": [
        "#### My model with feedback"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "G5BxkTLzUAok"
      },
      "cell_type": "markdown",
      "source": [
        "## Divide valid in two parts for validation and validation-testÂ¶"
      ]
    },
    {
      "metadata": {
        "id": "mejHTwMYhEzu",
        "colab_type": "code",
        "outputId": "80dad2b1-47bc-41f8-bcea-df321e64dbb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(validation_data.shape)\n",
        "print(train_data.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 784)\n",
            "(55000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jVm6nWpSJn1l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data = validation_data[0:4000,:]\n",
        "valid_validation_data_label = validation_label_one_hot[0:4000,:]\n",
        "valid_test_data = validation_data[4000:,:]\n",
        "valid_test_data_label = validation_label_one_hot[4000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wcT7Xaz1KNcU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "awQpf94YHESr",
        "colab_type": "code",
        "outputId": "06fc4421-ca5b-4f73-c903-c36e451d9841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.coefs_[1].shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "UvE74mAlHf7Y",
        "colab_type": "code",
        "outputId": "905bab41-2ae6-40a9-9ed9-fab200b1b209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.coefs_[0].shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ydDcWHWsJcJ-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "G_w_out_h1 = tf.Variable(xavier_init([10,300]))\n",
        "G_b_out_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "G_w_h2_h1 = tf.Variable(xavier_init([100,300]))\n",
        "G_b_h2_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "G_W1f = tf.Variable(xavier_init(clf.coefs_[0].shape))\n",
        "G_b1f = tf.Variable(xavier_init(clf.intercepts_ [0].shape))\n",
        "\n",
        "\n",
        "G_W2f = tf.Variable(xavier_init([clf.coefs_[0].shape[0], clf.coefs_[1].shape[1]]))\n",
        "G_b2f = tf.Variable(xavier_init(clf.intercepts_ [1].shape))\n",
        "\n",
        "\n",
        "# G_w_h1_input = tf.Variable(xavier_init([90,180]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([180]))\n",
        "\n",
        "\n",
        "# G_w_input_h1_h2 = tf.Variable(xavier_init([180,90]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([90]))\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xguK-SPLUrkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# valid_validation_data = validation_data[0:1000,:]\n",
        "# valid_validation_data_label = validation_label_one_hot[0:1000,:]\n",
        "# valid_test_data = validation_data[1000:,:]\n",
        "# valid_test_data_label = validation_label_one_hot[1000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgzAMkJCXVq2",
        "colab_type": "code",
        "outputId": "a28ed364-dab7-46b6-85c4-23435736bf70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data_label"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "qT_XdektXjmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plt.scatter(np.argmax(valid_validation_data_label,axis = 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RS8J8lVeXGoK",
        "colab_type": "code",
        "outputId": "f4c447a0-e5df-40c9-aa7d-de5714776b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(valid_validation_data_label,axis = 1))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([379., 444., 397., 392., 430., 350., 405., 433., 373., 397.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AJtaOCHUc8Us",
        "colab_type": "code",
        "outputId": "c40195bf-bad4-44a7-e398-25795e168aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "KrEu6ndlUZh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "metadata": {
        "id": "Q5TyGgw4Ub9n",
        "colab_type": "code",
        "outputId": "6ed22ddd-5bdd-49ee-9fdb-f140ab2b7883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5691
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "# hid_neuron = [90]\n",
        "num_steps = 30000\n",
        "batch_size = 4112\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "plot_every = 300\n",
        "number_of_epoch = 3000\n",
        "# learning_rate = 0.001\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "number_of_ex = train_data.shape[0]\n",
        "\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "weights = {\n",
        "    'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "    'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "    'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "}\n",
        "saver = tf.train.Saver()\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "ValidAccuracy_Test_track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "    \n",
        "for wL1 in range(1,6):\n",
        "  for WL2 in range(1,wL1+1):\n",
        "    for WL3 in range(0,2):\n",
        "\n",
        "        wLoss1 = wL1\n",
        "        wLoss2 = WL2\n",
        "        wLoss3 = WL3\n",
        "        loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "        loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "        loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "        loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        ### Initialization and running the model\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            best_accuracy_valid = 0\n",
        "            for ep in range(0,number_of_epoch):\n",
        "              if ep<10:\n",
        "                learn = .1\n",
        "              elif ep >=50 and ep <= 500:\n",
        "                learn = .1\n",
        "              else:\n",
        "                learn = .1\n",
        "#               learn = .01/(10+ep)\n",
        "              for step in range(0, total_steps_for_one_pass):\n",
        "\n",
        "                if step>=number_of_ex//batch_size:\n",
        "                  batch_x, batch_y = train_data[step*batch_size:,:],train_label_one_hot[step*batch_size:,:]\n",
        "#                   print(step,'Finishing',step*batch_size )\n",
        "                  step = 0\n",
        "\n",
        "                else:\n",
        "\n",
        "                  start = step*batch_size\n",
        "                  finish = (step+1)*batch_size\n",
        "#                   print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "                  batch_x, batch_y = train_data[step:finish,:],train_label_one_hot[step:finish,:]\n",
        "        #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})              \n",
        "\n",
        "\n",
        "\n",
        "  #                 batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "  #                 sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "              if ep % plot_every == 0:\n",
        "                  train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                  print(\"epoch \" + str(ep) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                  train_losses.append(train_loss)\n",
        "                  validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                  if ep%plot_every == 0:\n",
        "                    print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                    print()\n",
        "                    if (validation_accuracy >= best_accuracy_valid):\n",
        "                      best_accuracy_valid = validation_accuracy\n",
        "                      saver.save(sess, './statimgTrack')\n",
        "                      G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "            print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "            ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "            this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "            W_track.append(this_params)\n",
        "            # code for checking accuracy of valid_test\n",
        "            validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "            ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "            print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "            print(\"=\"*50)\n",
        "            print(\"W1 = {} ...\".format(wLoss1))\n",
        "            print(\"W2 = {} ...\".format(wLoss2))\n",
        "            print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "            print(\"*\"*50)\n",
        "            print(\"=\"*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss= 0.7109242, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 300, training loss= 0.007783676, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.0027891845, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 900, training loss= 0.0016589684, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.0012297778, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0010037312, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0008669663, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2100, training loss= 0.00076738984, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.00068860856, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.00062529475, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.29999542236328 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.275104, training acc= 100.0%\n",
            "Validation Accuracy valid 98.67500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.01557137, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 600, training loss= 0.005725173, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 900, training loss= 0.0029603648, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 1200, training loss= 0.0018555622, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0013853711, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 1800, training loss= 0.00110805, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 2100, training loss= 0.00091841206, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 2400, training loss= 0.00076548255, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0006465852, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.675 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5889836, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.008924034, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 600, training loss= 0.0033834702, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.0019056643, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 1200, training loss= 0.0013613923, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0010895128, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 1800, training loss= 0.00092667516, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0008170423, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.00073550036, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.00067398034, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.2114537, training acc= 100.0%\n",
            "Validation Accuracy valid 98.6500015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.016077153, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 600, training loss= 0.006875961, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 900, training loss= 0.0035939226, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 1200, training loss= 0.002299598, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0016859934, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0013091039, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0010756656, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 2400, training loss= 0.00092388893, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0008109592, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.65 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7109242, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 300, training loss= 0.007783676, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 600, training loss= 0.0027891845, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 900, training loss= 0.0016589684, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 1200, training loss= 0.0012297778, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0010037312, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0008669663, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2100, training loss= 0.00076738984, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2400, training loss= 0.00068860856, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.00062529475, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.29999542236328 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.1699575, training acc= 100.0%\n",
            "Validation Accuracy valid 98.67500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.016237736, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 600, training loss= 0.006130381, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 900, training loss= 0.0035505106, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 1200, training loss= 0.001962721, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0012922718, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0009887347, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 2100, training loss= 0.00081491034, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.00069979735, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0006146639, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.675 %\n",
            "Validation Accuracy Test 98.5999984741211 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.4977525, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 300, training loss= 0.00927194, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 600, training loss= 0.0039037215, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 900, training loss= 0.0021840362, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 1200, training loss= 0.0015270943, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 1500, training loss= 0.0011925066, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 1800, training loss= 0.0009894959, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 2100, training loss= 0.0008562515, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 2400, training loss= 0.0007647453, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 2700, training loss= 0.0006933812, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.1583214, training acc= 100.0%\n",
            "Validation Accuracy valid 98.6500015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.017377121, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 600, training loss= 0.0074922363, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 900, training loss= 0.004185082, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-c376d1a5e182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                   \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ScLSLeoIlVL8",
        "colab_type": "code",
        "outputId": "3caa63b6-c911-4163-845b-5b6cd1c1b22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1458
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "# hid_neuron = [90]\n",
        "num_steps = 30000\n",
        "batch_size = 4112\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "plot_every = 100\n",
        "number_of_epoch = 500\n",
        "# learning_rate = 0.001\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "number_of_ex = train_data.shape[0]\n",
        "\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "weights = {\n",
        "    'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "    'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "    'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "}\n",
        "saver = tf.train.Saver()\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "ValidAccuracy_Test_track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.relu(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "    \n",
        "for wL1 in range(1,6):\n",
        "  for WL2 in range(1,wL1+1):\n",
        "    for WL3 in range(0,2):\n",
        "\n",
        "        wLoss1 = wL1\n",
        "        wLoss2 = WL2\n",
        "        wLoss3 = WL3\n",
        "        loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "        loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "        loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "        loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        ### Initialization and running the model\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            best_accuracy_valid = 0\n",
        "            for ep in range(0,number_of_epoch):\n",
        "              if ep<10:\n",
        "                learn = .01\n",
        "              elif ep >=50 and ep <= 500:\n",
        "                learn = .01\n",
        "              else:\n",
        "                learn = .01\n",
        "#               learn = .01/(10+ep)\n",
        "              for step in range(0, total_steps_for_one_pass):\n",
        "\n",
        "                if step>=number_of_ex//batch_size:\n",
        "                  batch_x, batch_y = train_data[step*batch_size:,:],train_label_one_hot[step*batch_size:,:]\n",
        "#                   print(step,'Finishing',step*batch_size )\n",
        "                  step = 0\n",
        "\n",
        "                else:\n",
        "\n",
        "                  start = step*batch_size\n",
        "                  finish = (step+1)*batch_size\n",
        "#                   print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "                  batch_x, batch_y = train_data[step:finish,:],train_label_one_hot[step:finish,:]\n",
        "        #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})              \n",
        "\n",
        "\n",
        "\n",
        "  #                 batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "  #                 sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "              if ep % plot_every == 0:\n",
        "                  train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                  print(\"epoch \" + str(ep) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                  train_losses.append(train_loss)\n",
        "                  validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                  if ep%plot_every == 0:\n",
        "                    print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                    print()\n",
        "                    if (validation_accuracy >= best_accuracy_valid):\n",
        "                      best_accuracy_valid = validation_accuracy\n",
        "                      saver.save(sess, './statimgTrack')\n",
        "                      G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "            print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "            ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "            this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "            W_track.append(this_params)\n",
        "            # code for checking accuracy of valid_test\n",
        "            validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "            ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "            print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "            print(\"=\"*50)\n",
        "            print(\"W1 = {} ...\".format(wLoss1))\n",
        "            print(\"W2 = {} ...\".format(wLoss2))\n",
        "            print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "            print(\"*\"*50)\n",
        "            print(\"=\"*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss= 0.16770445, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.19999694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.0005826909, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00041783138, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.00035049, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.00030960285, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.5999984741211 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= nan, training acc= 9.84455943107605%\n",
            "Validation Accuracy valid 9.475000381469727 ...\n",
            "\n",
            "epoch 100, training loss= nan, training acc= 9.84455943107605%\n",
            "Validation Accuracy valid 9.475000381469727 ...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-11be30cf328f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                   \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2gSElnozvzRI",
        "colab_type": "code",
        "outputId": "f9511061-df69-4b0f-c11b-8e7fc7bcd1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11747
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "# hid_neuron = [90]\n",
        "num_steps = 30000\n",
        "batch_size = 4112\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "plot_every = 100\n",
        "number_of_epoch = 500\n",
        "# learning_rate = 0.001\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "number_of_ex = train_data.shape[0]\n",
        "\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "weights = {\n",
        "    'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "    'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "    'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "}\n",
        "saver = tf.train.Saver()\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "ValidAccuracy_Test_track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "    \n",
        "for wL1 in range(1,6):\n",
        "  for WL2 in range(1,wL1+1):\n",
        "    for WL3 in range(0,2):\n",
        "\n",
        "        wLoss1 = wL1\n",
        "        wLoss2 = WL2\n",
        "        wLoss3 = WL3\n",
        "        loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "        loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "        loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "        loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        ### Initialization and running the model\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            best_accuracy_valid = 0\n",
        "            for ep in range(0,number_of_epoch):\n",
        "              if ep<10:\n",
        "                learn = .1\n",
        "              elif ep >=50 and ep <= 500:\n",
        "                learn = .1\n",
        "              else:\n",
        "                learn = .1\n",
        "#               learn = .01/(10+ep)\n",
        "              for step in range(0, total_steps_for_one_pass):\n",
        "\n",
        "                if step>=number_of_ex//batch_size:\n",
        "                  batch_x, batch_y = train_data[step*batch_size:,:],train_label_one_hot[step*batch_size:,:]\n",
        "#                   print(step,'Finishing',step*batch_size )\n",
        "                  step = 0\n",
        "\n",
        "                else:\n",
        "\n",
        "                  start = step*batch_size\n",
        "                  finish = (step+1)*batch_size\n",
        "#                   print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "                  batch_x, batch_y = train_data[step:finish,:],train_label_one_hot[step:finish,:]\n",
        "        #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})              \n",
        "\n",
        "\n",
        "\n",
        "  #                 batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "  #                 sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "              if ep % plot_every == 0:\n",
        "                  train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                  print(\"epoch \" + str(ep) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                  train_losses.append(train_loss)\n",
        "                  validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                  if ep%plot_every == 0:\n",
        "                    print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                    print()\n",
        "                    if (validation_accuracy >= best_accuracy_valid):\n",
        "                      best_accuracy_valid = validation_accuracy\n",
        "                      saver.save(sess, './statimgTrack')\n",
        "                      G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "            print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "            ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "            this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "            W_track.append(this_params)\n",
        "            # code for checking accuracy of valid_test\n",
        "            validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "            ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "            print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "            print(\"=\"*50)\n",
        "            print(\"W1 = {} ...\".format(wLoss1))\n",
        "            print(\"W2 = {} ...\".format(wLoss2))\n",
        "            print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "            print(\"*\"*50)\n",
        "            print(\"=\"*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss= 0.021946924, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.125 ...\n",
            "\n",
            "epoch 100, training loss= 0.0002747263, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017923674, training acc= 100.0%\n",
            "Validation Accuracy valid 98.29999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014110695, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011885325, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.375 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.52855253, training acc= 89.0544056892395%\n",
            "Validation Accuracy valid 87.44999694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.004530017, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.0999984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.0018350243, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.19999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.0011931362, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.19999694824219 ...\n",
            "\n",
            "epoch 400, training loss= 0.00089056086, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.2750015258789 ...\n",
            "\n",
            "Valid acc= 98.275 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.020294197, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.00028132772, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017958932, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.0001376505, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011468847, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.575005 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.3312016, training acc= 97.73315787315369%\n",
            "Validation Accuracy valid 94.9000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.003626851, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.1500015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.0013101697, training acc= 100.0%\n",
            "Validation Accuracy valid 98.25 ...\n",
            "\n",
            "epoch 300, training loss= 0.0008410426, training acc= 100.0%\n",
            "Validation Accuracy valid 98.25 ...\n",
            "\n",
            "epoch 400, training loss= 0.00062778615, training acc= 100.0%\n",
            "Validation Accuracy valid 98.25 ...\n",
            "\n",
            "Valid acc= 98.25 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.021946924, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.125 ...\n",
            "\n",
            "epoch 100, training loss= 0.0002747263, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017923674, training acc= 100.0%\n",
            "Validation Accuracy valid 98.29999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014110695, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011885325, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.375 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.29359356, training acc= 97.66839146614075%\n",
            "Validation Accuracy valid 94.92499542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.0029027087, training acc= 100.0%\n",
            "Validation Accuracy valid 98.2750015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.0012406582, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.000817127, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 400, training loss= 0.00061338994, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.375 %\n",
            "Validation Accuracy Test 98.79999542236328 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.020069176, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.00028909778, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.00018555738, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014172871, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011838738, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.089660816, training acc= 99.22279715538025%\n",
            "Validation Accuracy valid 97.5999984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.001024481, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 200, training loss= 0.0005374611, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00038012184, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00029852774, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "Valid acc= 98.425 %\n",
            "Validation Accuracy Test 98.79999542236328 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.021461353, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.00027847136, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.00018052245, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014043627, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011668905, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.14891395, training acc= 98.4455943107605%\n",
            "Validation Accuracy valid 96.32499694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.0014636135, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00075413764, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.0005178142, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00039477923, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.575005 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.021946924, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.125 ...\n",
            "\n",
            "epoch 100, training loss= 0.0002747263, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017923674, training acc= 100.0%\n",
            "Validation Accuracy valid 98.29999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014110695, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011885325, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.375 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.22461706, training acc= 97.40932583808899%\n",
            "Validation Accuracy valid 95.05000305175781 ...\n",
            "\n",
            "epoch 100, training loss= 0.0017192464, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00086187164, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.00058671145, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.0004474862, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.475 %\n",
            "Validation Accuracy Test 98.5999984741211 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.020182265, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.0002942864, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.00018824756, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014397416, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.000119826014, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.060114633, training acc= 99.61140155792236%\n",
            "Validation Accuracy valid 97.79999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.000687844, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 200, training loss= 0.00039446037, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 300, training loss= 0.00028779756, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00022947582, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.4 %\n",
            "Validation Accuracy Test 99.0999984741211 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.020294197, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.00028132772, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017958932, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.0001376505, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011468847, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.575005 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.09033181, training acc= 99.15803074836731%\n",
            "Validation Accuracy valid 97.1500015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.0008358276, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00048686762, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.00035402642, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.00028053988, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 98.79999542236328 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.020243868, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 100, training loss= 0.00024600676, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00016510666, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.0001293852, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00010967862, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "Valid acc= 98.425 %\n",
            "Validation Accuracy Test 99.0 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.14567834, training acc= 98.57512712478638%\n",
            "Validation Accuracy valid 96.35000610351562 ...\n",
            "\n",
            "epoch 100, training loss= 0.0010785146, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00059309055, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.00042271, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.0003290108, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.475 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.021946924, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.125 ...\n",
            "\n",
            "epoch 100, training loss= 0.0002747263, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017923674, training acc= 100.0%\n",
            "Validation Accuracy valid 98.29999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014110695, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011885325, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.375 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.20865396, training acc= 97.21502661705017%\n",
            "Validation Accuracy valid 95.25 ...\n",
            "\n",
            "epoch 100, training loss= 0.0012999487, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.0006973849, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.0004790201, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00036797972, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.45 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.02014227, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.00029267353, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00018704307, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014369929, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 400, training loss= 0.0001212144, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.031903025, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.125 ...\n",
            "\n",
            "epoch 100, training loss= 0.00045382473, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 200, training loss= 0.00028480147, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00021985221, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.00018097594, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "Valid acc= 98.425 %\n",
            "Validation Accuracy Test 98.79999542236328 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.019986643, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.0002832792, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.00018162526, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 300, training loss= 0.00013889822, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011576479, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.525 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.04536052, training acc= 99.54662919044495%\n",
            "Validation Accuracy valid 97.8499984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.00059406564, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 200, training loss= 0.00034374822, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00025494542, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.00020676407, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.475 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.020988084, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.00027606764, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017895215, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 300, training loss= 0.00013895384, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 400, training loss= 0.000115401366, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.06800965, training acc= 99.28756356239319%\n",
            "Validation Accuracy valid 97.375 ...\n",
            "\n",
            "epoch 100, training loss= 0.0007938519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00045157468, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 300, training loss= 0.00032798346, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00026335873, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.45 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.01981232, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.00024268107, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 200, training loss= 0.00015820321, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.000126626, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.000107562264, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.45 %\n",
            "Validation Accuracy Test 99.0 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.11547413, training acc= 98.76943230628967%\n",
            "Validation Accuracy valid 96.5999984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.0010122193, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00054324575, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.0003889837, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.0003023244, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "Valid acc= 98.475 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.021946924, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.125 ...\n",
            "\n",
            "epoch 100, training loss= 0.0002747263, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.00017923674, training acc= 100.0%\n",
            "Validation Accuracy valid 98.29999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014110695, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00011885325, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.375 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.1694968, training acc= 97.86269664764404%\n",
            "Validation Accuracy valid 95.95000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.0014477465, training acc= 100.0%\n",
            "Validation Accuracy valid 98.32500457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.0006772006, training acc= 100.0%\n",
            "Validation Accuracy valid 98.29999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00045582926, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00034536616, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.4 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_d9m5Qxk3nFq",
        "colab_type": "code",
        "outputId": "f0acb707-3c94-44f3-afd8-e4f8bee73e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(ValidAccuracy_Track)\n",
        "plt.plot(ValidAccuracy_Test_track)\n",
        "\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKh/geEbyQzty7+EQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "my-rDMMN3nvN",
        "colab_type": "code",
        "outputId": "89c0c682-2912-4da6-dfdc-f25779f5f51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.max(ValidAccuracy_Track))\n",
        "print(np.argmax(ValidAccuracy_Track))\n",
        "print(np.max(ValidAccuracy_Test_track))\n",
        "print(np.argmax(ValidAccuracy_Test_track))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.575005\n",
            "2\n",
            "99.1\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70fhLDvz7i8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## More exploration of hypter parameter"
      ]
    },
    {
      "metadata": {
        "id": "HqYMID1aA7O6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Az5yDREBBLvi",
        "colab_type": "code",
        "outputId": "9d924123-e629-4727-f66d-453326140cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "8feotEbb7mwC",
        "colab_type": "code",
        "outputId": "8e81d32f-e774-400f-cc56-5ba61e9ffd9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10183
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "# hid_neuron = [90]\n",
        "num_steps = 30000\n",
        "batch_size = 4112\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "plot_every = 100\n",
        "number_of_epoch = 500\n",
        "BATCH_SIZE = batch_size\n",
        "# learning_rate = 0.001\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "number_of_ex = train_data.shape[0]\n",
        "num_examples = len(X_train)\n",
        "\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "weights = {\n",
        "    'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "    'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "    'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "}\n",
        "saver = tf.train.Saver()\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "ValidAccuracy_Test_track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "    \n",
        "for wL1 in range(6,8):\n",
        "  for WL2 in range(1,wL1+1):\n",
        "    for WL3 in range(0,2):\n",
        "\n",
        "        wLoss1 = wL1\n",
        "        wLoss2 = WL2\n",
        "        wLoss3 = WL3\n",
        "        loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "        loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "        loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "        loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        ### Initialization and running the model\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            best_accuracy_valid = 0\n",
        "            for ep in range(0,number_of_epoch):\n",
        "              train_data, train_label_one_hot = shuffle(train_data, train_label_one_hot)\n",
        "              if ep<10:\n",
        "                learn = .1\n",
        "              elif ep >=50 and ep <= 500:\n",
        "                learn = .1\n",
        "              else:\n",
        "                learn = .1\n",
        "#               learn = .01/(10+ep)\n",
        "\n",
        "              for offset in range(0, num_examples, BATCH_SIZE):\n",
        "                end = offset + BATCH_SIZE\n",
        "                batch_x, batch_y = train_data[offset:end], train_label_one_hot[offset:end]\n",
        "#               for step in range(0, total_steps_for_one_pass):\n",
        "\n",
        "#                 if step>=number_of_ex//batch_size:\n",
        "#                   batch_x, batch_y = train_data[step*batch_size:,:],train_label_one_hot[step*batch_size:,:]\n",
        "# #                   print(step,'Finishing',step*batch_size )\n",
        "#                   step = 0\n",
        "\n",
        "#                 else:\n",
        "\n",
        "#                   start = step*batch_size\n",
        "#                   finish = (step+1)*batch_size\n",
        "# #                   print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "#                   batch_x, batch_y = train_data[step:finish,:],train_label_one_hot[step:finish,:]\n",
        "        #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})              \n",
        "\n",
        "\n",
        "\n",
        "  #                 batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "  #                 sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "              if ep % plot_every == 0:\n",
        "                  train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                  print(\"epoch \" + str(ep) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                  train_losses.append(train_loss)\n",
        "                  validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                  if ep%plot_every == 0:\n",
        "                    print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                    print()\n",
        "                    if (validation_accuracy >= best_accuracy_valid):\n",
        "                      best_accuracy_valid = validation_accuracy\n",
        "                      saver.save(sess, './statimgTrack')\n",
        "                      G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "            print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "            ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "            this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "            W_track.append(this_params)\n",
        "            # code for checking accuracy of valid_test\n",
        "            validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "            ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "            print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "            print(\"=\"*50)\n",
        "            print(\"W1 = {} ...\".format(wLoss1))\n",
        "            print(\"W2 = {} ...\".format(wLoss2))\n",
        "            print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "            print(\"*\"*50)\n",
        "            print(\"=\"*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss= 0.033707075, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.00032289635, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 200, training loss= 0.0002904592, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00023303737, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.0001869944, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.575005 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.033049934, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.0009904362, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.00031670099, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.00015583908, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.0003386044, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.475 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.033978827, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.00037186258, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00032897206, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00019014481, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00019959293, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 99.0 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.04051337, training acc= 99.80570077896118%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.00064868777, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00030681136, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.000311853, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.00018246438, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 99.0 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.04140885, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 100, training loss= 0.00032679914, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.00021025554, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 300, training loss= 0.00018267515, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.00017295225, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.06437242, training acc= 99.41709637641907%\n",
            "Validation Accuracy valid 97.89999389648438 ...\n",
            "\n",
            "epoch 100, training loss= 0.0007152708, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 200, training loss= 0.0004043932, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 300, training loss= 0.000313153, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.0003301058, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.4 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.015992982, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.00045513958, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00026683856, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.0002557656, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.00018758482, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.525 %\n",
            "Validation Accuracy Test 99.0 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.08723603, training acc= 99.35232996940613%\n",
            "Validation Accuracy valid 97.42500305175781 ...\n",
            "\n",
            "epoch 100, training loss= 0.0011374552, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.0006501743, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00050228724, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.0003422109, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.45 %\n",
            "Validation Accuracy Test 99.0999984741211 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.030932127, training acc= 99.6761679649353%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 100, training loss= 0.00045397997, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.00019491794, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 300, training loss= 0.00019021405, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.00018144128, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 99.0999984741211 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.18198709, training acc= 98.4455943107605%\n",
            "Validation Accuracy valid 96.82499694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.0017287303, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.0008311162, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.00054238917, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.0005144263, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.475 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.03575018, training acc= 99.80570077896118%\n",
            "Validation Accuracy valid 98.1500015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.00055763545, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00023921172, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00023538721, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00016579323, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.45 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.1763041, training acc= 98.4455943107605%\n",
            "Validation Accuracy valid 96.45000457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.0012423196, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00093930727, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 300, training loss= 0.0007905663, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.00063606736, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 98.79999542236328 ...\n",
            "==================================================\n",
            "W1 = 6 ...\n",
            "W2 = 6 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.029318549, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.00047419284, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.0002598602, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00021911843, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00018553514, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.79999542236328 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.03371361, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.00043258225, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.00023584433, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00014057603, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.00018918025, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.575005 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.03627171, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.00040079968, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00021800319, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00020308552, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00015595538, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.525 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.036306836, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.00047338993, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 200, training loss= 0.00028385274, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.00023929394, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00020918064, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.036995843, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.000595862, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.00028169758, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.0002163801, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.0001454828, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "Valid acc= 98.525 %\n",
            "Validation Accuracy Test 99.0 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.039272357, training acc= 99.74093437194824%\n",
            "Validation Accuracy valid 98.25 ...\n",
            "\n",
            "epoch 100, training loss= 0.00055064005, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00030588565, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.00024027735, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.0002901692, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.042039856, training acc= 99.93523359298706%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.00030224706, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00024773597, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00022078598, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00013922882, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.051399063, training acc= 99.80570077896118%\n",
            "Validation Accuracy valid 97.9749984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.0011838762, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.0005270537, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.00029338448, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 400, training loss= 0.00023249732, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.45 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.050490618, training acc= 99.80570077896118%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.00047844608, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.00021775265, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.00015013653, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.00018068135, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.525 %\n",
            "Validation Accuracy Test 99.0999984741211 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 5 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.11001609, training acc= 98.57512712478638%\n",
            "Validation Accuracy valid 97.07499694824219 ...\n",
            "\n",
            "epoch 100, training loss= 0.001317226, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 200, training loss= 0.00069232343, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.0005268219, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.0004292436, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.549995 %\n",
            "Validation Accuracy Test 99.0 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 5 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.036265355, training acc= 99.87046718597412%\n",
            "Validation Accuracy valid 98.2249984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.0005452569, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.0002901206, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.00026201486, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.00019422328, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 99.0999984741211 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 6 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.12647043, training acc= 99.02849793434143%\n",
            "Validation Accuracy valid 97.29999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.001491224, training acc= 100.0%\n",
            "Validation Accuracy valid 98.29999542236328 ...\n",
            "\n",
            "epoch 200, training loss= 0.000700998, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.00039376752, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.0003899487, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "Valid acc= 98.35 %\n",
            "Validation Accuracy Test 98.79999542236328 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 6 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.038643233, training acc= 99.481862783432%\n",
            "Validation Accuracy valid 98.07500457763672 ...\n",
            "\n",
            "epoch 100, training loss= 0.00033723045, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.0003346501, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.00019935383, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.00020429952, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.4 %\n",
            "Validation Accuracy Test 98.9000015258789 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 7 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.18188585, training acc= 97.66839146614075%\n",
            "Validation Accuracy valid 95.9749984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.0014079412, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.00082752627, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.0005379102, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 400, training loss= 0.00056635734, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.5 %\n",
            "Validation Accuracy Test 98.69999694824219 ...\n",
            "==================================================\n",
            "W1 = 7 ...\n",
            "W2 = 7 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tFZuM6nyJCTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Best wt 210 OR 420\n",
        "---- Try with 210 first to check performance on validation data"
      ]
    },
    {
      "metadata": {
        "id": "yFe9-HrrMO-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Copy correct NN model below"
      ]
    },
    {
      "metadata": {
        "id": "LG0RQz3FI1vV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qw9Rb1WxH91p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### weights are 4,2,0"
      ]
    },
    {
      "metadata": {
        "id": "SNa5wb-fNgdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffling_indices_validation_data = np.random.permutation(validation_data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wo0Ckop2O-zr",
        "colab_type": "code",
        "outputId": "78ce01f4-ddb1-4ac2-afe0-36cd2a2ad539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "shuffling_indices_validation_data.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "ffB0tBLCLyxQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffled_validation_data = validation_data[shuffling_indices_validation_data,:]\n",
        "shuffled_validation_label = validation_label_one_hot[shuffling_indices_validation_data,:]\n",
        "train_valid_combined_shuffled = np.concatenate((train_data, shuffled_validation_data))\n",
        "train_valid_combined_shuffled_label = np.concatenate((train_label_one_hot, shuffled_validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWTB1ulkVKth",
        "colab_type": "code",
        "outputId": "e8dd9325-4b01-4c11-ed97-7b6f24bc5834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "v5ENx3_HSDoW",
        "colab_type": "code",
        "outputId": "1a610ecf-9160-4eec-a8a4-8c3de50ab596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([479., 563., 488., 493., 535., 434., 501., 550., 462., 495.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Lun1902_I2aP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # keep aside \n",
        "# aside_examples= 300\n",
        "# aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "# aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "# combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "# combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFcQiksEQNKy",
        "colab_type": "code",
        "outputId": "4d75eb06-c3c1-482b-9cc6-c1d6d4f97b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined_shuffled.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "H4ELGdrQQD6G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # keep aside \n",
        "# aside_examples= 400\n",
        "# aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "# aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "# combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "# combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ag4ONRjCL88-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 5000\n",
        "aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJReYXGJVQlk",
        "colab_type": "code",
        "outputId": "e0d2816e-7788-4339-b7ec-4a3cd3d58e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "combined_train_valid.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "0QFZESfMJErm",
        "colab_type": "code",
        "outputId": "13ff87d4-4818-4fdf-8ca7-ebbf3831effd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(aside_valid_test_label,axis = 1))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([479., 563., 488., 493., 535., 434., 501., 550., 462., 495.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ov9vN2217iir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Also check if lasso helps"
      ]
    },
    {
      "metadata": {
        "id": "slO3Bvd97lWW",
        "colab_type": "code",
        "outputId": "c79c3c74-bfdb-46c7-b26d-eb1a1212143d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1149
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 5000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 200\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    return out_layer\n",
        "#     layer_outputs.append(out_layer)\n",
        "#     for loop in range(0,2):        \n",
        "#         layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "#         layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "#         layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "#         layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "#         layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "#         layer_1 = tf.nn.relu(layer_1)\n",
        "#         layer_2 = tf.matmul(layer_1, G_W2) + G_b2 #+ tf.add(tf.matmul(x, G_W2f), G_b2f)\n",
        "#         layer_2 = tf.nn.relu(layer_2)\n",
        "#         out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "#         layer_outputs.append(out_layer)\n",
        "#     if train == True:\n",
        "#         return layer_outputs\n",
        "#     else:\n",
        "#         return layer_outputs[0]\n",
        "\n",
        "\n",
        "# wLoss1 = 7\n",
        "# wLoss2 = 1\n",
        "# wLoss3 = 1\n",
        "# loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "# loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "# loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "# loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X), labels=Y))\n",
        "\n",
        "# loss_reg = (tf.nn.l2_loss(G_W1) + tf.nn.l2_loss(G_b1) + \n",
        "#             tf.nn.l2_loss(G_W2) + tf.nn.l2_loss(G_b2) + \n",
        "#             tf.nn.l2_loss(G_W3) + tf.nn.l2_loss(G_b3) + \n",
        "#             tf.nn.l2_loss(G_w_out_h1) + tf.nn.l2_loss(G_b_out_h1) +\n",
        "#             tf.nn.l2_loss(G_w_h2_h1) + tf.nn.l2_loss(G_b_out_h1) + \n",
        "#             tf.nn.l2_loss(G_W1f) + tf.nn.l2_loss(G_b1f))\n",
        "\n",
        "loss_total = loss #+ 0*loss_reg\n",
        "\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate=lr, use_nesterov=True, momentum=.9)\n",
        "# optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "train_op = optimizer.minimize(loss_total)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X), 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<300:\n",
        "        learn = .001\n",
        "      elif ep >=300 and ep <= 500:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDTemp')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "    saver.save(sess, './statlog_letterMnist1stIter')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 0.00013054845, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 200, training loss Total= 0.00012959605, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 400, training loss Total= 0.00012905168, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 600, training loss Total= 0.00012857722, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 800, training loss Total= 0.00012812915, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 1000, training loss Total= 0.00012770137, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 1200, training loss Total= 0.00012728921, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 1400, training loss Total= 0.00012688768, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 1600, training loss Total= 0.00012649677, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 1800, training loss Total= 0.00012611394, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 2000, training loss Total= 0.00012574153, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 2200, training loss Total= 0.0001253754, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 2400, training loss Total= 0.00012501368, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 2600, training loss Total= 0.00012465678, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 2800, training loss Total= 0.00012430326, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 3000, training loss Total= 0.00012395225, training acc total= 100.0%\n",
            "ValidTest acc= 98.7 %\n",
            "epoch 3200, training loss Total= 0.00012360435, training acc total= 100.0%\n",
            "ValidTest acc= 98.7 %\n",
            "epoch 3400, training loss Total= 0.00012325853, training acc total= 100.0%\n",
            "ValidTest acc= 98.7 %\n",
            "epoch 3600, training loss Total= 0.00012291536, training acc total= 100.0%\n",
            "ValidTest acc= 98.7 %\n",
            "epoch 3800, training loss Total= 0.00012257611, training acc total= 100.0%\n",
            "ValidTest acc= 98.7 %\n",
            "epoch 4000, training loss Total= 0.00012223894, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 4200, training loss Total= 0.00012190434, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 4400, training loss Total= 0.00012157204, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 4600, training loss Total= 0.00012124269, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 4800, training loss Total= 0.00012091559, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "ValidValid acc= 98.68 %\n",
            "ValidTest acc= 98.68 %\n",
            "==================================================\n",
            "W1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d348d6678f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwLoss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wLoss1' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Mn-7BVjWVeML",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tPS1muiM3yay",
        "colab_type": "code",
        "outputId": "8de9e2be-b030-4960-e237-5dab2579d2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1618
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 2000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2 #+ tf.add(tf.matmul(x, G_W2f), G_b2f)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 7\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "loss_reg = (tf.nn.l2_loss(G_W1) + tf.nn.l2_loss(G_b1) + \n",
        "            tf.nn.l2_loss(G_W2) + tf.nn.l2_loss(G_b2) + \n",
        "            tf.nn.l2_loss(G_W3) + tf.nn.l2_loss(G_b3) + \n",
        "            tf.nn.l2_loss(G_w_out_h1) + tf.nn.l2_loss(G_b_out_h1) +\n",
        "            tf.nn.l2_loss(G_w_h2_h1) + tf.nn.l2_loss(G_b_out_h1) + \n",
        "            tf.nn.l2_loss(G_W1f) + tf.nn.l2_loss(G_b1f))\n",
        "\n",
        "loss_total = loss + 0*loss_reg\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate=lr, use_nesterov=True, momentum=.9)\n",
        "# optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "train_op = optimizer.minimize(loss_total)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, './statlog_letterReducedSGDTemp')\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<300:\n",
        "        learn = .001\n",
        "      elif ep >=300 and ep <= 500:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDTempv2')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "    saver.save(sess, './statlog_letterMnist1stIterv2')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedSGDTemp\n",
            "epoch 0, training loss Total= 0.0013409373, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 50, training loss Total= 0.0013255327, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 100, training loss Total= 0.0013103229, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 150, training loss Total= 0.0012952089, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 200, training loss Total= 0.0012805733, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 250, training loss Total= 0.0012661862, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 300, training loss Total= 0.0012519024, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 350, training loss Total= 0.0012377505, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 400, training loss Total= 0.0012236275, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 450, training loss Total= 0.0012094644, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 500, training loss Total= 0.0011954667, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 550, training loss Total= 0.001181618, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 600, training loss Total= 0.0011678359, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 650, training loss Total= 0.0011542642, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 700, training loss Total= 0.0011408911, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 750, training loss Total= 0.0011264576, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 800, training loss Total= 0.0011117283, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 850, training loss Total= 0.0010969813, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 900, training loss Total= 0.0010823092, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 950, training loss Total= 0.0010678307, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1000, training loss Total= 0.0010532999, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1050, training loss Total= 0.0010383031, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1100, training loss Total= 0.001023307, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1150, training loss Total= 0.0010073103, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1200, training loss Total= 0.0009887961, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1250, training loss Total= 0.0009698518, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1300, training loss Total= 0.0009503507, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1350, training loss Total= 0.00093149376, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1400, training loss Total= 0.00091275823, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1450, training loss Total= 0.0008975344, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1500, training loss Total= 0.0008887079, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1550, training loss Total= 0.00088065077, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1600, training loss Total= 0.0008725488, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1650, training loss Total= 0.0008642755, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1700, training loss Total= 0.0008560798, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1750, training loss Total= 0.00084831426, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1800, training loss Total= 0.00084156264, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1850, training loss Total= 0.000835961, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1900, training loss Total= 0.00083099754, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 1950, training loss Total= 0.00082631357, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "ValidValid acc= 98.64 %\n",
            "ValidTest acc= 98.64 %\n",
            "==================================================\n",
            "W1\n",
            "7\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DI_IMo-3rpbG",
        "colab_type": "code",
        "outputId": "ec70cd7e-fd41-4314-d02b-411145418794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 2000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2 #+ tf.add(tf.matmul(x, G_W2f), G_b2f)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 7\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "loss_reg = (tf.nn.l2_loss(G_W1) + tf.nn.l2_loss(G_b1) + \n",
        "            tf.nn.l2_loss(G_W2) + tf.nn.l2_loss(G_b2) + \n",
        "            tf.nn.l2_loss(G_W3) + tf.nn.l2_loss(G_b3) + \n",
        "            tf.nn.l2_loss(G_w_out_h1) + tf.nn.l2_loss(G_b_out_h1) +\n",
        "            tf.nn.l2_loss(G_w_h2_h1) + tf.nn.l2_loss(G_b_out_h1) + \n",
        "            tf.nn.l2_loss(G_W1f) + tf.nn.l2_loss(G_b1f))\n",
        "\n",
        "loss_total = loss + 0*loss_reg\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate=lr, use_nesterov=True, momentum=.9)\n",
        "# optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "train_op = optimizer.minimize(loss_total)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "#     saver.restore(sess, './statlog_letterMnist1stIterv2')\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<300:\n",
        "        learn = .001\n",
        "      elif ep >=300 and ep <= 500:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDTempv3')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "    saver.save(sess, './statlog_letterMnist1stIterv3')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 5.2104588, training acc total= 99.98727440834045%\n",
            "ValidTest acc= 98.439995 %\n",
            "epoch 50, training loss Total= 0.121449165, training acc total= 99.88545179367065%\n",
            "ValidTest acc= 98.2 %\n",
            "epoch 100, training loss Total= 0.06334083, training acc total= 99.92727041244507%\n",
            "ValidTest acc= 98.36 %\n",
            "epoch 150, training loss Total= 0.043339957, training acc total= 99.94545578956604%\n",
            "ValidTest acc= 98.42 %\n",
            "epoch 200, training loss Total= 0.031926785, training acc total= 99.95454549789429%\n",
            "ValidTest acc= 98.479996 %\n",
            "epoch 250, training loss Total= 0.024888977, training acc total= 99.96181726455688%\n",
            "ValidTest acc= 98.46 %\n",
            "epoch 300, training loss Total= 0.020605508, training acc total= 99.96363520622253%\n",
            "ValidTest acc= 98.479996 %\n",
            "epoch 350, training loss Total= 0.017498054, training acc total= 99.96727108955383%\n",
            "ValidTest acc= 98.479996 %\n",
            "epoch 400, training loss Total= 0.015185614, training acc total= 99.96908903121948%\n",
            "ValidTest acc= 98.439995 %\n",
            "epoch 450, training loss Total= 0.013337182, training acc total= 99.97272491455078%\n",
            "ValidTest acc= 98.439995 %\n",
            "epoch 500, training loss Total= 0.011755137, training acc total= 99.97454285621643%\n",
            "ValidTest acc= 98.46 %\n",
            "epoch 550, training loss Total= 0.010388476, training acc total= 99.9781847000122%\n",
            "ValidTest acc= 98.479996 %\n",
            "epoch 600, training loss Total= 0.009290573, training acc total= 99.98000264167786%\n",
            "ValidTest acc= 98.479996 %\n",
            "epoch 650, training loss Total= 0.0084776, training acc total= 99.98363852500916%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 700, training loss Total= 0.007783565, training acc total= 99.9854564666748%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 750, training loss Total= 0.007225833, training acc total= 99.9854564666748%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 800, training loss Total= 0.0068022883, training acc total= 99.98727440834045%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 850, training loss Total= 0.006425904, training acc total= 99.9927282333374%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 900, training loss Total= 0.0061329687, training acc total= 99.9927282333374%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 950, training loss Total= 0.005867427, training acc total= 99.9927282333374%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 1000, training loss Total= 0.0056186635, training acc total= 99.9927282333374%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 1050, training loss Total= 0.005392347, training acc total= 99.9927282333374%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 1100, training loss Total= 0.0051906994, training acc total= 99.9927282333374%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 1150, training loss Total= 0.0049981275, training acc total= 99.9963641166687%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 1200, training loss Total= 0.004814003, training acc total= 99.99818205833435%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 1250, training loss Total= 0.0046342355, training acc total= 99.99818205833435%\n",
            "ValidTest acc= 98.54 %\n",
            "epoch 1300, training loss Total= 0.0044470937, training acc total= 99.99818205833435%\n",
            "ValidTest acc= 98.54 %\n",
            "epoch 1350, training loss Total= 0.0042676157, training acc total= 99.99818205833435%\n",
            "ValidTest acc= 98.54 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xfXF-9jRJfKa",
        "colab_type": "code",
        "outputId": "08e2df79-8441-495c-faab-f9605e7d7460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "combined_train_valid.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-968481c5c693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_train_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'combined_train_valid' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Lsmky8iWr2_f",
        "colab_type": "code",
        "outputId": "c710734c-195d-4e0a-9db5-0e0fa5dcb266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 2000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2 #+ tf.add(tf.matmul(x, G_W2f), G_b2f)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 7\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "loss_reg = (tf.nn.l2_loss(G_W1) + tf.nn.l2_loss(G_b1) + \n",
        "            tf.nn.l2_loss(G_W2) + tf.nn.l2_loss(G_b2) + \n",
        "            tf.nn.l2_loss(G_W3) + tf.nn.l2_loss(G_b3) + \n",
        "            tf.nn.l2_loss(G_w_out_h1) + tf.nn.l2_loss(G_b_out_h1) +\n",
        "            tf.nn.l2_loss(G_w_h2_h1) + tf.nn.l2_loss(G_b_out_h1) + \n",
        "            tf.nn.l2_loss(G_W1f) + tf.nn.l2_loss(G_b1f))\n",
        "\n",
        "loss_total = loss + 0*loss_reg\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate=lr, use_nesterov=True, momentum=.9)\n",
        "# optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "train_op = optimizer.minimize(loss_total)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, './statlog_letterMnist1stIterv3')\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<300:\n",
        "        learn = .001\n",
        "      elif ep >=300 and ep <= 500:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDTempv4')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "    saver.save(sess, './statlog_letterMnist1stIterv4')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterMnist1stIterv3\n",
            "epoch 0, training loss Total= 0.00067594874, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 50, training loss Total= 0.00067278097, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 100, training loss Total= 0.0006696326, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 150, training loss Total= 0.0006664976, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 200, training loss Total= 0.00066337537, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 250, training loss Total= 0.00066027936, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 300, training loss Total= 0.00065721857, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 350, training loss Total= 0.00065415737, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 400, training loss Total= 0.0006511199, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 450, training loss Total= 0.0006480967, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 500, training loss Total= 0.0006450848, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 550, training loss Total= 0.00064209005, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 600, training loss Total= 0.0006391099, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 650, training loss Total= 0.0006361432, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 700, training loss Total= 0.0006331907, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 750, training loss Total= 0.00063025224, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 800, training loss Total= 0.00062732655, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 850, training loss Total= 0.0006244144, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 900, training loss Total= 0.0006215142, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 950, training loss Total= 0.0006186288, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1000, training loss Total= 0.00061575224, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1050, training loss Total= 0.0006128882, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1100, training loss Total= 0.00061003875, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1150, training loss Total= 0.0006072054, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1200, training loss Total= 0.0006043876, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1250, training loss Total= 0.00060158345, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1300, training loss Total= 0.00059879274, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1350, training loss Total= 0.0005960185, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1400, training loss Total= 0.0005932595, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1450, training loss Total= 0.0005905153, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1500, training loss Total= 0.0005877975, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1550, training loss Total= 0.0005851858, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1600, training loss Total= 0.00058257085, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1650, training loss Total= 0.00057996606, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1700, training loss Total= 0.000577377, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1750, training loss Total= 0.0005748012, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1800, training loss Total= 0.0005722385, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1850, training loss Total= 0.00056968996, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1900, training loss Total= 0.00056715857, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 1950, training loss Total= 0.00056464056, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "ValidValid acc= 98.659996 %\n",
            "ValidTest acc= 98.659996 %\n",
            "==================================================\n",
            "W1\n",
            "7\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4eroWboor2ze",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gyWm9EC2COvP",
        "colab_type": "code",
        "outputId": "0e30c8ca-4252-42de-a0e9-16c62a20e2a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "# ## 123 Building the graph - Best!\n",
        "# saver = tf.train.Saver()\n",
        "# number_of_ex = combined_train_valid.shape[0]\n",
        "# hid_neuron = [104]\n",
        "# num_steps = 20000\n",
        "# # num_steps = 20000\n",
        "# number_of_epoch = 2000\n",
        "# batch_size = 4112\n",
        "# # batch_size = train_data.shape[0]\n",
        "\n",
        "# train_losses = []\n",
        "# test_acc = []\n",
        "# train_accuracy = []\n",
        "# val_accuracy = []\n",
        "# plot_every = 50\n",
        "# best_accuracy_valid = 0\n",
        "# learning_rate = 0.001\n",
        "# track_step = []\n",
        "# tracked_valid_accuracy = []\n",
        "# total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "# step = 0\n",
        "# X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "# Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# lr = tf.placeholder(tf.float32, shape = [])\n",
        "# W_track = []\n",
        "# ValidAccuracy_Track = []\n",
        "# def neural_net(x,train = True):\n",
        "#     layer_outputs = []\n",
        "#     layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "#     layer_1 = tf.nn.relu(layer_1)\n",
        "#     layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_2 = tf.nn.relu(layer_2)\n",
        "#     out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "#     layer_outputs.append(out_layer)\n",
        "#     for loop in range(0,2):        \n",
        "#         layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "#         layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "#         layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "#         layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "#         layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "#         layer_1 = tf.nn.relu(layer_1)\n",
        "#         layer_2 = tf.matmul(layer_1, G_W2) + G_b2 #+ tf.add(tf.matmul(x, G_W2f), G_b2f)\n",
        "#         layer_2 = tf.nn.relu(layer_2)\n",
        "#         out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "#         layer_outputs.append(out_layer)\n",
        "#     if train == True:\n",
        "#         return layer_outputs\n",
        "#     else:\n",
        "#         return layer_outputs[0]\n",
        "\n",
        "\n",
        "# wLoss1 = 7\n",
        "# wLoss2 = 1\n",
        "# wLoss3 = 1\n",
        "# loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "# loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "# loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "# loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "# loss_reg = (tf.nn.l2_loss(G_W1) + tf.nn.l2_loss(G_b1) + \n",
        "#             tf.nn.l2_loss(G_W2) + tf.nn.l2_loss(G_b2) + \n",
        "#             tf.nn.l2_loss(G_W3) + tf.nn.l2_loss(G_b3) + \n",
        "#             tf.nn.l2_loss(G_w_out_h1) + tf.nn.l2_loss(G_b_out_h1) +\n",
        "#             tf.nn.l2_loss(G_w_h2_h1) + tf.nn.l2_loss(G_b_out_h1) + \n",
        "#             tf.nn.l2_loss(G_W1f) + tf.nn.l2_loss(G_b1f))\n",
        "\n",
        "# loss_total = loss + 0*loss_reg\n",
        "\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "# # optimizer = tf.train.MomentumOptimizer(learning_rate=lr, use_nesterov=True, momentum=.9)\n",
        "# # optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# train_op = optimizer.minimize(loss_total)\n",
        "# correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# ### Initialization and running the model\n",
        "# with tf.Session() as sess:\n",
        "# #     sess.run(tf.global_variables_initializer())\n",
        "#     saver.restore(sess, './statlog_letterMnist1stIterv2')\n",
        "#     best_accuracy_valid = 0\n",
        "#     for ep in range(0,number_of_epoch):\n",
        "#       if ep<300:\n",
        "#         learn = .001\n",
        "#       elif ep >=300 and ep <= 500:\n",
        "#         learn = .001\n",
        "#       else:\n",
        "#         learn = .001\n",
        "#       for step in range(0, total_steps_for_one_pass):\n",
        "# #         print(step)\n",
        "# #         if (step>5000):\n",
        "# #           plot_every = 10\n",
        "        \n",
        "#         if step>=number_of_ex//batch_size:\n",
        "#           batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "# #           print(step,'Finishing',step*batch_size )\n",
        "#           step = 0\n",
        "          \n",
        "#         else:\n",
        "          \n",
        "#           start = step*batch_size\n",
        "#           finish = (step+1)*batch_size\n",
        "# #           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "#           batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "# #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "#         sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "#       if ep % plot_every == 0:\n",
        "#           train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "# #             train_accuracy.append(train_acc)\n",
        "# #             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "#           track_step.append(step)\n",
        "#           train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "#           train_accuracy.append(train_acc_total)\n",
        "#           train_losses.append(train_loss_total)\n",
        "#           print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "#           validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#           print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#           tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "#           if ep%plot_every == 0:\n",
        "#             if (validationTest_accuracy >= best_accuracy_valid):\n",
        "#               best_accuracy_valid = validationTest_accuracy\n",
        "#               saver.save(sess, './statlog_letterReducedSGDTempv3')\n",
        "#               G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "#   #         if(train_loss_total<0.033881765):\n",
        "#   #           break\n",
        "                                         \n",
        "#     validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "#     print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "#     validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#     print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#     this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "#     W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterMnist1stIterv3')\n",
        "#     print(\"=\"*50)\n",
        "#     print(\"W1\")\n",
        "#     print(wLoss1)\n",
        "\n",
        "#     print(\"W2\")\n",
        "#     print(wLoss2)\n",
        "#     print(\"*\"*50)\n",
        "    \n",
        "#     print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterMnist1stIterv2\n",
            "epoch 0, training loss Total= 0.001631551, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 50, training loss Total= 0.0016124333, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 100, training loss Total= 0.001592647, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 150, training loss Total= 0.0015743133, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 200, training loss Total= 0.0015567641, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 250, training loss Total= 0.0015393688, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 300, training loss Total= 0.0015221691, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 350, training loss Total= 0.0015050551, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 400, training loss Total= 0.0014879111, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 450, training loss Total= 0.0014709034, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 500, training loss Total= 0.0014540771, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 550, training loss Total= 0.0014377121, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 600, training loss Total= 0.0014214625, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 650, training loss Total= 0.0014050865, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 700, training loss Total= 0.0013887123, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 750, training loss Total= 0.0013726427, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 800, training loss Total= 0.0013566993, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 850, training loss Total= 0.0013409373, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 900, training loss Total= 0.0013255327, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 950, training loss Total= 0.0013103229, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 1000, training loss Total= 0.0012952089, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 1050, training loss Total= 0.0012805733, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 1100, training loss Total= 0.0012661862, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 1150, training loss Total= 0.0012519024, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 1200, training loss Total= 0.0012377505, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 1250, training loss Total= 0.0012236275, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 1300, training loss Total= 0.0012094644, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 1350, training loss Total= 0.0011954667, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 1400, training loss Total= 0.001181618, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMF1ajQrJWRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Determine how many epochs are required"
      ]
    },
    {
      "metadata": {
        "id": "PiTBKrflH9K7",
        "colab_type": "code",
        "outputId": "a002c9b2-9427-43c0-ea6a-523064363e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 1000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 2\n",
        "wLoss2 = 1\n",
        "wLoss3 = 0\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "# G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "# G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "# G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "# G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "# G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "# G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "# G_w_out_h1 = tf.Variable(xavier_init([10,300]))\n",
        "# G_b_out_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "# G_w_h2_h1 = tf.Variable(xavier_init([100,300]))\n",
        "# G_b_h2_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "# G_W1f = tf.Variable(xavier_init(clf.coefs_[0].shape))\n",
        "# G_b1f = tf.Variable(xavier_init(clf.intercepts_ [0].shape))\n",
        "\n",
        "\n",
        "\n",
        "loss_reg = (tf.nn.l2_loss(G_W1) + tf.nn.l2_loss(G_b1) + \n",
        "            tf.nn.l2_loss(G_W2) + tf.nn.l2_loss(G_b2) + \n",
        "            tf.nn.l2_loss(G_W3) + tf.nn.l2_loss(G_b3) + \n",
        "            tf.nn.l2_loss(G_w_out_h1) + tf.nn.l2_loss(G_b_out_h1) +\n",
        "            tf.nn.l2_loss(G_w_h2_h1) + tf.nn.l2_loss(G_b_out_h1) + \n",
        "            tf.nn.l2_loss(G_W1f) + tf.nn.l2_loss(G_b1f))\n",
        "\n",
        "loss_total = loss + .000*loss_reg\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss_total)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, './statlog_letterReducedSGDTemp')\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<50:\n",
        "        learn = .001\n",
        "      elif ep >=300 and ep <= 500:\n",
        "        learn = .001\n",
        "      else:\n",
        "        learn = .001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDTempv2')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    saver.save(sess, './statlog_letterReducedSGDEndv2')\n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedSGDTemp\n",
            "epoch 0, training loss Total= 0.0005712064, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 50, training loss Total= 0.0005695877, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 100, training loss Total= 0.0005679791, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 150, training loss Total= 0.00056638516, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 200, training loss Total= 0.0005648037, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 250, training loss Total= 0.00056323444, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 300, training loss Total= 0.0005616752, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 350, training loss Total= 0.0005601236, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 400, training loss Total= 0.0005585842, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 450, training loss Total= 0.0005570557, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 500, training loss Total= 0.0005555362, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 550, training loss Total= 0.0005540281, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 600, training loss Total= 0.00055253087, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 650, training loss Total= 0.0005510449, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 700, training loss Total= 0.00054956926, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 750, training loss Total= 0.0005481044, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 800, training loss Total= 0.0005466495, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 850, training loss Total= 0.00054520584, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 900, training loss Total= 0.0005437727, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 950, training loss Total= 0.0005423503, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "ValidValid acc= 98.58 %\n",
            "ValidTest acc= 98.58 %\n",
            "==================================================\n",
            "W1\n",
            "2\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nC_Np5WE7jE1",
        "colab_type": "code",
        "outputId": "99ba3ff2-36f4-415d-aeac-4f21465d6904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "# ## 123 Building the graph - Best!\n",
        "# saver = tf.train.Saver()\n",
        "# number_of_ex = combined_train_valid.shape[0]\n",
        "# hid_neuron = [104]\n",
        "# num_steps = 20000\n",
        "# # num_steps = 20000\n",
        "# number_of_epoch = 1000\n",
        "# batch_size = 4112\n",
        "# # batch_size = train_data.shape[0]\n",
        "\n",
        "# train_losses = []\n",
        "# test_acc = []\n",
        "# train_accuracy = []\n",
        "# val_accuracy = []\n",
        "# plot_every = 50\n",
        "# best_accuracy_valid = 0\n",
        "# learning_rate = 0.001\n",
        "# track_step = []\n",
        "# tracked_valid_accuracy = []\n",
        "# total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "# step = 0\n",
        "# X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "# Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# lr = tf.placeholder(tf.float32, shape = [])\n",
        "# W_track = []\n",
        "# ValidAccuracy_Track = []\n",
        "# def neural_net(x,train = True):\n",
        "#     layer_outputs = []\n",
        "#     layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "#     layer_1 = tf.nn.relu(layer_1)\n",
        "#     layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_2 = tf.nn.relu(layer_2)\n",
        "#     out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "#     layer_outputs.append(out_layer)\n",
        "#     for loop in range(0,2):        \n",
        "#         layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "#         layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "#         layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "#         layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "#         layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "#         layer_1 = tf.nn.relu(layer_1)\n",
        "#         layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#         layer_2 = tf.nn.relu(layer_2)\n",
        "#         out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "#         layer_outputs.append(out_layer)\n",
        "#     if train == True:\n",
        "#         return layer_outputs\n",
        "#     else:\n",
        "#         return layer_outputs[0]\n",
        "\n",
        "\n",
        "# wLoss1 = 2\n",
        "# wLoss2 = 1\n",
        "# wLoss3 = 0\n",
        "# loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "# loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "# loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "# loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "# # G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "# # G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "# # G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "# # G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "# # G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "# # G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "# # G_w_out_h1 = tf.Variable(xavier_init([10,300]))\n",
        "# # G_b_out_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "# # G_w_h2_h1 = tf.Variable(xavier_init([100,300]))\n",
        "# # G_b_h2_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "# # G_W1f = tf.Variable(xavier_init(clf.coefs_[0].shape))\n",
        "# # G_b1f = tf.Variable(xavier_init(clf.intercepts_ [0].shape))\n",
        "\n",
        "\n",
        "\n",
        "# loss_reg = (tf.nn.l2_loss(G_W1) + tf.nn.l2_loss(G_b1) + \n",
        "#             tf.nn.l2_loss(G_W2) + tf.nn.l2_loss(G_b2) + \n",
        "#             tf.nn.l2_loss(G_W3) + tf.nn.l2_loss(G_b3) + \n",
        "#             tf.nn.l2_loss(G_w_out_h1) + tf.nn.l2_loss(G_b_out_h1) +\n",
        "#             tf.nn.l2_loss(G_w_h2_h1) + tf.nn.l2_loss(G_b_out_h1) + \n",
        "#             tf.nn.l2_loss(G_W1f) + tf.nn.l2_loss(G_b1f))\n",
        "\n",
        "# loss_total = loss + .000*loss_reg\n",
        "\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "# train_op = optimizer.minimize(loss_total)\n",
        "# correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# ### Initialization and running the model\n",
        "# with tf.Session() as sess:\n",
        "# #     sess.run(tf.global_variables_initializer())\n",
        "#     saver.restore(sess, './statlog_letterReducedSGDTempv2')\n",
        "#     best_accuracy_valid = 0\n",
        "#     for ep in range(0,number_of_epoch):\n",
        "#       if ep<50:\n",
        "#         learn = .0005\n",
        "#       elif ep >=300 and ep <= 500:\n",
        "#         learn = .0005\n",
        "#       else:\n",
        "#         learn = .0005\n",
        "#       for step in range(0, total_steps_for_one_pass):\n",
        "# #         print(step)\n",
        "# #         if (step>5000):\n",
        "# #           plot_every = 10\n",
        "        \n",
        "#         if step>=number_of_ex//batch_size:\n",
        "#           batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "# #           print(step,'Finishing',step*batch_size )\n",
        "#           step = 0\n",
        "          \n",
        "#         else:\n",
        "          \n",
        "#           start = step*batch_size\n",
        "#           finish = (step+1)*batch_size\n",
        "# #           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "#           batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "# #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "#         sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "#       if ep % plot_every == 0:\n",
        "#           train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "# #             train_accuracy.append(train_acc)\n",
        "# #             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "#           track_step.append(step)\n",
        "#           train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "#           train_accuracy.append(train_acc_total)\n",
        "#           train_losses.append(train_loss_total)\n",
        "#           print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "#           validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#           print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#           tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "#           if ep%plot_every == 0:\n",
        "#             if (validationTest_accuracy >= best_accuracy_valid):\n",
        "#               best_accuracy_valid = validationTest_accuracy\n",
        "#               saver.save(sess, './statlog_letterReducedSGDTempv3')\n",
        "#               G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "#   #         if(train_loss_total<0.033881765):\n",
        "#   #           break\n",
        "                                         \n",
        "#     validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "#     print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "#     saver.save(sess, './statlog_letterReducedSGDEndv3')\n",
        "#     validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#     print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#     this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "#     W_track.append(this_params)\n",
        "# #     saver.save(sess, './statlog_letterAdam')\n",
        "#     print(\"=\"*50)\n",
        "#     print(\"W1\")\n",
        "#     print(wLoss1)\n",
        "\n",
        "#     print(\"W2\")\n",
        "#     print(wLoss2)\n",
        "#     print(\"*\"*50)\n",
        "    \n",
        "#     print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedSGDTempv2\n",
            "epoch 0, training loss Total= 0.0005555223, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 50, training loss Total= 0.0005548153, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 100, training loss Total= 0.0005541112, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 150, training loss Total= 0.0005534102, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 200, training loss Total= 0.0005527118, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 250, training loss Total= 0.0005520169, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 300, training loss Total= 0.00055132504, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 350, training loss Total= 0.0005506368, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 400, training loss Total= 0.0005499514, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 450, training loss Total= 0.0005492693, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 500, training loss Total= 0.0005485903, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 550, training loss Total= 0.00054791395, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 600, training loss Total= 0.0005472399, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 650, training loss Total= 0.0005465696, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 700, training loss Total= 0.0005459017, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 750, training loss Total= 0.0005452366, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 800, training loss Total= 0.0005445741, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 850, training loss Total= 0.0005439146, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 900, training loss Total= 0.0005432581, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "epoch 950, training loss Total= 0.00054260425, training acc total= 100.0%\n",
            "ValidTest acc= 98.58 %\n",
            "ValidValid acc= 98.58 %\n",
            "ValidTest acc= 98.58 %\n",
            "==================================================\n",
            "W1\n",
            "2\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bfn0ctGsorTD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Continue further training"
      ]
    },
    {
      "metadata": {
        "id": "HhHoRGntavcF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check on test set!"
      ]
    },
    {
      "metadata": {
        "id": "QlzkjrMfavOm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "FoEs31Jr47Ez",
        "colab_type": "code",
        "outputId": "9e2dd96b-e32b-4cc1-ed20-b86924f71b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, './statlog_letterMnist1stIterv4')\n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_accuracy), \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterMnist1stIterv4\n",
            "Train acc= 0.9866 %\n",
            "Test acc= 98.479996 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMFFgIRuk3Mg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "statlog_letterMnist1stIter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fbimt80vnoO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "cb1c9449-4af4-4744-a73e-bf90c083eeb5"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, './statlog_letterMnist1stIter')\n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_accuracy), \"%\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./statlog_letterMnist1stIter\n",
            "Train acc= 0.9868 %\n",
            "Test acc= 98.43 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}