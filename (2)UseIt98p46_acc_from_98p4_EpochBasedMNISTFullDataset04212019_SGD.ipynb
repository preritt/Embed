{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(2)UseIt98p46 acc from 98p4 EpochBasedMNISTFullDataset04212019_SGD.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "GoIsmarzTPqQ",
        "lZ5IPzM1aO3S"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/(2)UseIt98p46_acc_from_98p4_EpochBasedMNISTFullDataset04212019_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0SINGreLFCRz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import packages"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "My4EmvydE3bW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_9T8Y6Y2J4g",
        "colab_type": "code",
        "outputId": "b224e76e-957e-4432-844b-545a44ea0214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
        "\n",
        "assert(len(X_train) == len(y_train))\n",
        "assert(len(X_validation) == len(y_validation))\n",
        "assert(len(X_test) == len(y_test))\n",
        "\n",
        "print()\n",
        "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "print()\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-566519a95339>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "\n",
            "Image Shape: (28, 28, 1)\n",
            "\n",
            "Training Set:   55000 samples\n",
            "Validation Set: 5000 samples\n",
            "Test Set:       10000 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UU12uT1x2kNS",
        "colab_type": "code",
        "outputId": "0be29fa7-93f8-463e-834c-5604d57f9a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_validation.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ahO5eMXW2ODm",
        "colab_type": "code",
        "outputId": "ad43dff1-775c-44ed-9472-03f95707bb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "yy = X_train.reshape(X_train.shape[0],-1)\n",
        "yy.shape\n",
        "# yy = np.reshape(X_train. newshape=(X_train.shape[0],-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "3goNaizqshmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = X_train.reshape(X_train.shape[0],-1)\n",
        "train_label = y_train\n",
        "validation_data = X_validation.reshape(X_validation.shape[0],-1)\n",
        "validation_label = y_validation\n",
        "test_data = X_test.reshape(X_test.shape[0],-1)\n",
        "test_label = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XdbYPAMNxDZg",
        "colab_type": "code",
        "outputId": "e0f8bf6e-aa4d-4771-ec0f-b0338f78d4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(train_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5444., 6179., 5470., 5638., 5307., 4987., 5417., 5715., 5389.,\n",
              "        5454.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xSuKAIntmzfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dTAGPqvlFEuQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# np.max(train_data[1,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ovvpmlXeFH1x",
        "outputId": "fe2c2ed6-31f0-4108-a65f-170b71fb5e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "train_data_pandas = pd.DataFrame(train_data)\n",
        "train_data_labels = pd.DataFrame(train_label)\n",
        "train_data_pandas.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3    4    5    6    7    8    9   ...   774  775  776  777  \\\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
              "\n",
              "   778  779  780  781  782  783  \n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "epqBn1YjFlII",
        "outputId": "32f217c9-d579-4b40-8d87-1551c96a9c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "train_data_labels.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  7\n",
              "1  3\n",
              "2  4\n",
              "3  6\n",
              "4  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ewLyg3iuFqkO",
        "outputId": "705ebbff-f29e-4df1-a2d3-48d53f4c460a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w5wMHmhIFthO",
        "outputId": "fc280c81-1ff2-4803-a7d7-a436a73fa87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5Jg0BONTGBA1"
      },
      "cell_type": "markdown",
      "source": [
        "#### Combine Validation and train data for MLP classifier - and set validation fraction to 4500/15000 = 0.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8krXltl9GPfv",
        "outputId": "216d9f4f-25a0-4e43-a9c6-63fd76d3a285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_combined.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NUWNzsz4v04T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bWN_sAWEFNtb"
      },
      "cell_type": "markdown",
      "source": [
        "#### Fit MLP Classifier"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QSdXJQLnFKa2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clf = MLPClassifier(hidden_layer_sizes=(104),validation_fraction=0.3)\n",
        "# clf.fit(train_data, train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnBnT6NdTqyO",
        "colab_type": "code",
        "outputId": "a03ef181-6359-45a0-aa4d-a8ac59f83ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "20*90/36"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xfKh_nDUvj5G",
        "outputId": "897b554f-1831-48a9-ba1a-2f557aa4ecd2",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf = MLPClassifier(hidden_layer_sizes=(300,100,), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "# clf.fit(train_valid_combined, train_valid_label)\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.27088284\n",
            "Iteration 2, loss = 0.09284897\n",
            "Iteration 3, loss = 0.06362837\n",
            "Iteration 4, loss = 0.04513577\n",
            "Iteration 5, loss = 0.03265040\n",
            "Iteration 6, loss = 0.02556718\n",
            "Iteration 7, loss = 0.01983507\n",
            "Iteration 8, loss = 0.01265771\n",
            "Iteration 9, loss = 0.00954815\n",
            "Iteration 10, loss = 0.00746641\n",
            "Iteration 11, loss = 0.00487287\n",
            "Iteration 12, loss = 0.00280570\n",
            "Iteration 13, loss = 0.00162220\n",
            "Iteration 14, loss = 0.00092841\n",
            "Iteration 15, loss = 0.00073810\n",
            "Iteration 16, loss = 0.00063660\n",
            "Iteration 17, loss = 0.00065059\n",
            "Iteration 18, loss = 0.00056893\n",
            "Iteration 19, loss = 0.00052885\n",
            "Iteration 20, loss = 0.00050482\n",
            "Iteration 21, loss = 0.00049094\n",
            "Iteration 22, loss = 0.00047780\n",
            "Iteration 23, loss = 0.00046712\n",
            "Iteration 24, loss = 0.00045812\n",
            "Iteration 25, loss = 0.00044891\n",
            "Iteration 26, loss = 0.00044162\n",
            "Iteration 27, loss = 0.00043538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(300, 100), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lLNA4D0qGxJi"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "02O8VTAoGqnG",
        "outputId": "29357304-2627-4a36-e440-fc77dc3b5926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "whn8u2m5iY7M"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pii8wXXSG1r7"
      },
      "cell_type": "markdown",
      "source": [
        "#### Validation Accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SakclBGkGvI0",
        "outputId": "5e605b80-0c4e-49c8-c570-41f3dc49c693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VbIkGX5gG5ZG"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QLo_AzFVG3ca",
        "outputId": "0e61079f-845f-4057-f8c9-7151905d0b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "InLIF676HEES"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow model using weights initialized from numpy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tcBNfKZNG9Pm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ortxRVBMH7W7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z7mWVCDVEgLm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hid_neuron = [90]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LR62GfKJv_6E",
        "outputId": "df8db6ac-c235-4f69-ad68-0b559a8fe98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EylNp0IJONbz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Base NN model in tensor flow"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VzJnI_o2xD5C"
      },
      "cell_type": "markdown",
      "source": [
        "#### 36 -> 90 -> 6"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "73Way2v2Pbys"
      },
      "cell_type": "markdown",
      "source": [
        "## Train baseline model in tensorflow"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L-hUDOm5xClH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IMHh0nROw5O-",
        "outputId": "fd1c3f40-b410-436b-8c57-a40c27ee2db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yV4xtxJLvyNj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wj_W9eCBvyKy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TKQ6nMqMvyJD",
        "outputId": "42435f49-9a4a-4332-e1ee-19bceb3f9850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jy2mQcHAEn20",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf.train.GradientDescentOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eHe01FffvyEJ",
        "outputId": "caa0ef5d-3d2f-498d-d21b-6623a8423fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        }
      },
      "cell_type": "code",
      "source": [
        "## Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "learning_rate = 0.001\n",
        "hid_neuron = [374]\n",
        "num_steps = 20000\n",
        "batch_size = 200\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "#     layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    return out_layer\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X), labels=Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X), 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "  ### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % 1000 == 0:\n",
        "            train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "            print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "            train_losses.append(train_loss)\n",
        "            validation_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "            if step%1000 == 0:\n",
        "              print(\"Validation Accuracy {} ...\".format(validation_accuracy))\n",
        "              print()\n",
        "              if (validation_accuracy >= best_accuracy_valid):\n",
        "                best_accuracy_valid = validation_accuracy\n",
        "                saver.save(sess, './statlog_letter')\n",
        "                test_Accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_Accuracy), \"%\")\n",
        "    print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "\n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-25-4106bc23d37b>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step 0, training loss= 0.09654394, training acc= 96.49999737739563%\n",
            "Validation Accuracy 88.58000946044922 ...\n",
            "\n",
            "step 1000, training loss= 0.074345045, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 2000, training loss= 0.0795926, training acc= 99.00000095367432%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 3000, training loss= 0.09808998, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 4000, training loss= 0.07493803, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 5000, training loss= 0.095866315, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 6000, training loss= 0.07703011, training acc= 98.00000190734863%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 7000, training loss= 0.074125834, training acc= 98.50000143051147%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 8000, training loss= 0.099417955, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 9000, training loss= 0.09222789, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 10000, training loss= 0.08109907, training acc= 96.49999737739563%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 11000, training loss= 0.11149396, training acc= 95.49999833106995%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 12000, training loss= 0.06741232, training acc= 99.00000095367432%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 13000, training loss= 0.109245956, training acc= 95.99999785423279%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 14000, training loss= 0.07493451, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 15000, training loss= 0.090577155, training acc= 97.50000238418579%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 16000, training loss= 0.13477787, training acc= 94.49999928474426%\n",
            "Validation Accuracy 88.73027801513672 ...\n",
            "\n",
            "step 17000, training loss= 0.07381674, training acc= 98.00000190734863%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "step 18000, training loss= 0.07977779, training acc= 97.00000286102295%\n",
            "Validation Accuracy 88.65514373779297 ...\n",
            "\n",
            "step 19000, training loss= 0.102517195, training acc= 95.99999785423279%\n",
            "Validation Accuracy 88.80540466308594 ...\n",
            "\n",
            "Test acc= 89.450005 %\n",
            "Valid acc= 88.805405 %\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5A_PHV3bS7ui"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8RFK2bW4JZ7w"
      },
      "cell_type": "markdown",
      "source": [
        "#### My model with feedback"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "G5BxkTLzUAok"
      },
      "cell_type": "markdown",
      "source": [
        "## Divide valid in two parts for validation and validation-testÂ¶"
      ]
    },
    {
      "metadata": {
        "id": "mejHTwMYhEzu",
        "colab_type": "code",
        "outputId": "072ebe53-d899-4a6d-cc48-064d332417cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(validation_data.shape)\n",
        "print(train_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 784)\n",
            "(55000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jVm6nWpSJn1l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data = validation_data[0:4000,:]\n",
        "valid_validation_data_label = validation_label_one_hot[0:4000,:]\n",
        "valid_test_data = validation_data[4000:,:]\n",
        "valid_test_data_label = validation_label_one_hot[4000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wcT7Xaz1KNcU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_shape = train_label_one_hot.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ydDcWHWsJcJ-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "G_w_out_h1 = tf.Variable(xavier_init([10,300]))\n",
        "G_b_out_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "G_w_h2_h1 = tf.Variable(xavier_init([100,300]))\n",
        "G_b_h2_h1 = tf.Variable(xavier_init([300]))\n",
        "\n",
        "G_W1f = tf.Variable(xavier_init(clf.coefs_[0].shape))\n",
        "G_b1f = tf.Variable(xavier_init(clf.intercepts_ [0].shape))\n",
        "\n",
        "# G_w_h1_input = tf.Variable(xavier_init([90,180]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([180]))\n",
        "\n",
        "\n",
        "# G_w_input_h1_h2 = tf.Variable(xavier_init([180,90]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([90]))\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xguK-SPLUrkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# valid_validation_data = validation_data[0:1000,:]\n",
        "# valid_validation_data_label = validation_label_one_hot[0:1000,:]\n",
        "# valid_test_data = validation_data[1000:,:]\n",
        "# valid_test_data_label = validation_label_one_hot[1000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgzAMkJCXVq2",
        "colab_type": "code",
        "outputId": "224a8eb4-45f7-4882-ada2-eaed955c525a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "valid_validation_data_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "qT_XdektXjmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plt.scatter(np.argmax(valid_validation_data_label,axis = 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RS8J8lVeXGoK",
        "colab_type": "code",
        "outputId": "0bdf9d5f-86fb-4234-9946-dae91008fc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(valid_validation_data_label,axis = 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([379., 444., 397., 392., 430., 350., 405., 433., 373., 397.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AJtaOCHUc8Us",
        "colab_type": "code",
        "outputId": "c4bc3aaa-b5c4-4afb-ffda-4e9369eb8e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "KrEu6ndlUZh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "metadata": {
        "id": "Q5TyGgw4Ub9n",
        "colab_type": "code",
        "outputId": "b0f285b4-9430-496b-dd34-53651fbe02e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15844
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Building the graph\n",
        "# saver = tf.train.Saver()\n",
        "# hid_neuron = [90]\n",
        "num_steps = 30000\n",
        "batch_size = 4112\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "plot_every = 50\n",
        "number_of_epoch = 450\n",
        "# learning_rate = 0.001\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "number_of_ex = train_data.shape[0]\n",
        "\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "weights = {\n",
        "    'h1': tf.Variable(np.float32(clf.coefs_[0])),\n",
        "    'out': tf.Variable(np.float32(clf.coefs_[1]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(np.float32(clf.intercepts_ [0])),\n",
        "    'out': tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "}\n",
        "saver = tf.train.Saver()\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "ValidAccuracy_Test_track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "    \n",
        "for wL1 in range(1,6):\n",
        "  for WL2 in range(1,wL1+1):\n",
        "    for WL3 in range(0,2):\n",
        "\n",
        "        wLoss1 = wL1\n",
        "        wLoss2 = WL2\n",
        "        wLoss3 = WL3\n",
        "        loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "        loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "        loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "        loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss)\n",
        "        correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        ### Initialization and running the model\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            best_accuracy_valid = 0\n",
        "            for ep in range(0,number_of_epoch):\n",
        "              if ep<10:\n",
        "                learn = .001\n",
        "              elif ep >=50 and ep <= 500:\n",
        "                learn = .001\n",
        "              else:\n",
        "                learn = .001\n",
        "#               learn = .01/(10+ep)\n",
        "              for step in range(0, total_steps_for_one_pass):\n",
        "\n",
        "                if step>=number_of_ex//batch_size:\n",
        "                  batch_x, batch_y = train_data[step*batch_size:,:],train_label_one_hot[step*batch_size:,:]\n",
        "#                   print(step,'Finishing',step*batch_size )\n",
        "                  step = 0\n",
        "\n",
        "                else:\n",
        "\n",
        "                  start = step*batch_size\n",
        "                  finish = (step+1)*batch_size\n",
        "#                   print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "                  batch_x, batch_y = train_data[step:finish,:],train_label_one_hot[step:finish,:]\n",
        "        #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})              \n",
        "\n",
        "\n",
        "\n",
        "  #                 batch_x, batch_y = next_batch(batch_size, train_data, train_label_one_hot)\n",
        "  #                 sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "              if ep % plot_every == 0:\n",
        "                  train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "                  print(\"epoch \" + str(ep) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "                  train_losses.append(train_loss)\n",
        "                  validation_accuracy = sess.run(accuracy*100, feed_dict={X: valid_validation_data,Y:valid_validation_data_label})\n",
        "                  if ep%plot_every == 0:\n",
        "                    print(\"Validation Accuracy valid {} ...\".format(validation_accuracy))\n",
        "                    print()\n",
        "                    if (validation_accuracy >= best_accuracy_valid):\n",
        "                      best_accuracy_valid = validation_accuracy\n",
        "                      saver.save(sess, './statimgTrack')\n",
        "                      G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "            print(\"Valid acc=\",str(best_accuracy_valid), \"%\")\n",
        "            ValidAccuracy_Track.append(best_accuracy_valid)\n",
        "            this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "            W_track.append(this_params)\n",
        "            # code for checking accuracy of valid_test\n",
        "            validation_test_accuracy = sess.run(accuracy*100, feed_dict={X: valid_test_data,Y:valid_test_data_label})\n",
        "            ValidAccuracy_Test_track.append(validation_test_accuracy)\n",
        "            print(\"Validation Accuracy Test {} ...\".format(validation_test_accuracy))\n",
        "            print(\"=\"*50)\n",
        "            print(\"W1 = {} ...\".format(wLoss1))\n",
        "            print(\"W2 = {} ...\".format(wLoss2))\n",
        "            print(\"W3 = {} ...\".format(wLoss3))\n",
        "\n",
        "            print(\"*\"*50)\n",
        "            print(\"=\"*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss= 0.7109242, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.05016519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.027451145, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018565519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.013296039, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 250, training loss= 0.009975923, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.007783676, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 350, training loss= 0.006240866, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.005150044, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.29999542236328 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.275104, training acc= 100.0%\n",
            "Validation Accuracy valid 98.67500305175781 ...\n",
            "\n",
            "epoch 50, training loss= 0.07700079, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.044071674, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.033072904, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.024925202, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 250, training loss= 0.019121464, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.01557137, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 350, training loss= 0.013073087, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.011052867, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.675 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 1 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5889836, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.04744353, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.026481321, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018393058, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.014033845, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 250, training loss= 0.011114601, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.008924034, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 350, training loss= 0.00729046, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.006095372, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.2114537, training acc= 100.0%\n",
            "Validation Accuracy valid 98.6500015258789 ...\n",
            "\n",
            "epoch 50, training loss= 0.07262697, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.04132989, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 150, training loss= 0.030908164, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.0253057, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 250, training loss= 0.020380713, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.016077153, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 350, training loss= 0.013543984, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.011615271, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.65 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7109242, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.05016519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.027451145, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018565519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.013296039, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 250, training loss= 0.009975923, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.007783676, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 350, training loss= 0.006240866, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.005150044, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.29999542236328 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.1699575, training acc= 100.0%\n",
            "Validation Accuracy valid 98.67500305175781 ...\n",
            "\n",
            "epoch 50, training loss= 0.0717616, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.041060973, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.029912788, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.023642702, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.019239916, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 300, training loss= 0.016237736, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 350, training loss= 0.013116697, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.010708294, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.675 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 2 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.4977525, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.044963203, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.02536491, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.017838102, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.013714415, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 250, training loss= 0.011103613, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.00927194, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 350, training loss= 0.0077926163, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 400, training loss= 0.0065988805, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.1583214, training acc= 100.0%\n",
            "Validation Accuracy valid 98.6500015258789 ...\n",
            "\n",
            "epoch 50, training loss= 0.0700105, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.040099625, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.028870834, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.023823842, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 250, training loss= 0.020461021, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.017377121, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 350, training loss= 0.014357062, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.01208255, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.65 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.64620197, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 50, training loss= 0.048677254, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.027059782, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018665323, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.013943687, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.010741374, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.008510427, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 350, training loss= 0.0069290795, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.00577853, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.575005 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.1265693, training acc= 100.0%\n",
            "Validation Accuracy valid 98.6500015258789 ...\n",
            "\n",
            "epoch 50, training loss= 0.06960058, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.039767824, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.028852364, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.023075363, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 250, training loss= 0.019191837, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.0162948, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 350, training loss= 0.014161575, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.011881197, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.65 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7109242, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.05016519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.027451145, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018565519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.013296039, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 250, training loss= 0.009975923, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.007783676, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 350, training loss= 0.006240866, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.005150044, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.29999542236328 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0986671, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.069465615, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.03933058, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.028312739, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.02214268, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 250, training loss= 0.017858153, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 300, training loss= 0.01503541, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 350, training loss= 0.012776853, training acc= 100.0%\n",
            "Validation Accuracy valid 98.3499984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.0108009195, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 3 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.42941603, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.04243242, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.02462476, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.017341923, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 200, training loss= 0.013432619, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 250, training loss= 0.010940413, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.009227981, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 350, training loss= 0.007960872, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 400, training loss= 0.0069364966, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0999078, training acc= 100.0%\n",
            "Validation Accuracy valid 98.6500015258789 ...\n",
            "\n",
            "epoch 50, training loss= 0.06875821, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.038996313, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.02777113, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.022403892, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.019433616, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.017151725, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 350, training loss= 0.014967169, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.012760198, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.65 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5889836, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.04744353, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.026481321, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018393058, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.014033845, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 250, training loss= 0.011114601, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.008924034, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 350, training loss= 0.00729046, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.006095372, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0817604, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.06833908, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.03877811, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.028063469, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.022436889, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 250, training loss= 0.018687423, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.016031504, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 350, training loss= 0.013829411, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.012011539, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.66704154, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.049119707, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.027244013, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 150, training loss= 0.01872423, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.0138452705, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.010526427, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.0083098635, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 350, training loss= 0.006749887, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 400, training loss= 0.005602898, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.29999542236328 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0631173, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.067980446, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.03867407, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.027730122, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.022012712, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 250, training loss= 0.01800398, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.015122756, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 350, training loss= 0.01315712, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 400, training loss= 0.011325408, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.7109242, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.05016519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.027451145, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018565519, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.013296039, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 250, training loss= 0.009975923, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.007783676, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 350, training loss= 0.006240866, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.005150044, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.29999542236328 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0461655, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.06743942, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 100, training loss= 0.03824736, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 150, training loss= 0.02741854, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 200, training loss= 0.021427596, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 250, training loss= 0.017288128, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 300, training loss= 0.014338274, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "epoch 350, training loss= 0.012287922, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "epoch 400, training loss= 0.010626513, training acc= 100.0%\n",
            "Validation Accuracy valid 98.375 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 4 ...\n",
            "W2 = 4 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.37682766, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.03989137, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 100, training loss= 0.02400856, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.016949426, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 200, training loss= 0.01315882, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.010751008, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.009120482, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 350, training loss= 0.007920611, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 400, training loss= 0.0069787297, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0458912, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.067417875, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.0379286, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 150, training loss= 0.026986938, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.021568445, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.018463343, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.016474634, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 350, training loss= 0.014783653, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 400, training loss= 0.013150237, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 1 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.5398885, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.046241328, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.025876593, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018108543, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 200, training loss= 0.013867414, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 250, training loss= 0.0111618135, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.009146194, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 350, training loss= 0.007581248, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 400, training loss= 0.0063664895, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.6 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0390416, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.06688825, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 100, training loss= 0.038000725, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.02729982, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.02201854, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 250, training loss= 0.018454034, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4749984741211 ...\n",
            "\n",
            "epoch 300, training loss= 0.015758716, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 350, training loss= 0.013815277, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.012067578, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 2 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.62617385, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 50, training loss= 0.048260774, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.026882019, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 150, training loss= 0.018585227, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.014031064, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.010883593, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 300, training loss= 0.008633672, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 350, training loss= 0.00702542, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "epoch 400, training loss= 0.0058679543, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n",
            "Valid acc= 98.575005 %\n",
            "Validation Accuracy Test 98.4000015258789 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 0 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 1.0274674, training acc= 100.0%\n",
            "Validation Accuracy valid 98.625 ...\n",
            "\n",
            "epoch 50, training loss= 0.06660181, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.038121007, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 150, training loss= 0.02709373, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 200, training loss= 0.021642188, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 250, training loss= 0.017978545, training acc= 100.0%\n",
            "Validation Accuracy valid 98.44999694824219 ...\n",
            "\n",
            "epoch 300, training loss= 0.015191598, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 350, training loss= 0.013117105, training acc= 100.0%\n",
            "Validation Accuracy valid 98.42500305175781 ...\n",
            "\n",
            "epoch 400, training loss= 0.011520056, training acc= 100.0%\n",
            "Validation Accuracy valid 98.4000015258789 ...\n",
            "\n",
            "Valid acc= 98.625 %\n",
            "Validation Accuracy Test 98.5 ...\n",
            "==================================================\n",
            "W1 = 5 ...\n",
            "W2 = 3 ...\n",
            "W3 = 1 ...\n",
            "**************************************************\n",
            "==================================================\n",
            "epoch 0, training loss= 0.67775905, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5999984741211 ...\n",
            "\n",
            "epoch 50, training loss= 0.049337566, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 100, training loss= 0.0273137, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5 ...\n",
            "\n",
            "epoch 150, training loss= 0.018740568, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 200, training loss= 0.0137587385, training acc= 100.0%\n",
            "Validation Accuracy valid 98.5250015258789 ...\n",
            "\n",
            "epoch 250, training loss= 0.010394447, training acc= 100.0%\n",
            "Validation Accuracy valid 98.54999542236328 ...\n",
            "\n",
            "epoch 300, training loss= 0.0081801275, training acc= 100.0%\n",
            "Validation Accuracy valid 98.57500457763672 ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_d9m5Qxk3nFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(ValidAccuracy_Track)\n",
        "plt.plot(ValidAccuracy_Test_track)\n",
        "\n",
        "plt.ylabel('Iter')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "my-rDMMN3nvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.max(ValidAccuracy_Track))\n",
        "print(np.argmax(ValidAccuracy_Track))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFZuM6nyJCTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Best wt 411"
      ]
    },
    {
      "metadata": {
        "id": "yFe9-HrrMO-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Copy correct NN model below"
      ]
    },
    {
      "metadata": {
        "id": "LG0RQz3FI1vV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qw9Rb1WxH91p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### weights are 1,1,1"
      ]
    },
    {
      "metadata": {
        "id": "SNa5wb-fNgdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffling_indices_validation_data = np.random.permutation(validation_data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wo0Ckop2O-zr",
        "colab_type": "code",
        "outputId": "f8e2b1a6-f5e1-406f-d1ab-c30cfe524558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "shuffling_indices_validation_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "ffB0tBLCLyxQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffled_validation_data = validation_data[shuffling_indices_validation_data,:]\n",
        "shuffled_validation_label = validation_label_one_hot[shuffling_indices_validation_data,:]\n",
        "train_valid_combined_shuffled = np.concatenate((train_data, shuffled_validation_data))\n",
        "train_valid_combined_shuffled_label = np.concatenate((train_label_one_hot, shuffled_validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWTB1ulkVKth",
        "colab_type": "code",
        "outputId": "ae39f7cf-fa91-4dc2-8bb7-0542dfd86484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "validation_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "v5ENx3_HSDoW",
        "colab_type": "code",
        "outputId": "b364b129-76fa-4365-e88a-26ad8f28e2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([479., 563., 488., 493., 535., 434., 501., 550., 462., 495.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Lun1902_I2aP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # keep aside \n",
        "# aside_examples= 300\n",
        "# aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "# aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "# combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "# combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFcQiksEQNKy",
        "colab_type": "code",
        "outputId": "6c1162c5-30d8-47d8-ea97-676ea8ed907d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined_shuffled.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "H4ELGdrQQD6G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # keep aside \n",
        "# aside_examples= 400\n",
        "# aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "# aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "# combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "# combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ag4ONRjCL88-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 5000\n",
        "aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJReYXGJVQlk",
        "colab_type": "code",
        "outputId": "85583018-ee43-44ef-e675-831adaf8d775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "combined_train_valid.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "0QFZESfMJErm",
        "colab_type": "code",
        "outputId": "a1273ccb-a98b-4d4c-d150-fadd8e702db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(aside_valid_test_label,axis = 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([479., 563., 488., 493., 535., 434., 501., 550., 462., 495.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tMF1ajQrJWRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Determine how many epochs are required"
      ]
    },
    {
      "metadata": {
        "id": "PiTBKrflH9K7",
        "colab_type": "code",
        "outputId": "782ffb38-b24c-4725-9d38-4d4e475056a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 1000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 4\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<100:\n",
        "        learn = .1\n",
        "      elif ep >=100 and ep <= 400:\n",
        "        learn = .1\n",
        "      else:\n",
        "        learn = .01\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 0.0928024, training acc total= 99.3254542350769%\n",
            "ValidTest acc= 97.74 %\n",
            "epoch 50, training loss Total= 0.0042735366, training acc total= 99.99818205833435%\n",
            "ValidTest acc= 98.34 %\n",
            "epoch 100, training loss Total= 0.002150851, training acc total= 99.99818205833435%\n",
            "ValidTest acc= 98.46 %\n",
            "epoch 150, training loss Total= 0.0014125848, training acc total= 100.0%\n",
            "ValidTest acc= 98.46 %\n",
            "epoch 200, training loss Total= 0.0010120461, training acc total= 100.0%\n",
            "ValidTest acc= 98.479996 %\n",
            "epoch 250, training loss Total= 0.0007988668, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 300, training loss Total= 0.0006712744, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 350, training loss Total= 0.0005836758, training acc total= 100.0%\n",
            "ValidTest acc= 98.5 %\n",
            "epoch 400, training loss Total= 0.0005187863, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 450, training loss Total= 0.0005132301, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 500, training loss Total= 0.00050779287, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 550, training loss Total= 0.0005025029, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 600, training loss Total= 0.0004974102, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 650, training loss Total= 0.0004924985, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 700, training loss Total= 0.00048768916, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 750, training loss Total= 0.00048301122, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 800, training loss Total= 0.00047842728, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 850, training loss Total= 0.00047396432, training acc total= 100.0%\n",
            "ValidTest acc= 98.52 %\n",
            "epoch 900, training loss Total= 0.0004696154, training acc total= 100.0%\n",
            "ValidTest acc= 98.54 %\n",
            "epoch 950, training loss Total= 0.00046538434, training acc total= 100.0%\n",
            "ValidTest acc= 98.56 %\n",
            "ValidValid acc= 98.56 %\n",
            "ValidTest acc= 98.56 %\n",
            "==================================================\n",
            "W1\n",
            "4\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bKhf_hp-eq1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Delete later - continus trainig from above - restore"
      ]
    },
    {
      "metadata": {
        "id": "ZsA3iv89evvJ",
        "colab_type": "code",
        "outputId": "5069891e-c9aa-4f12-c614-b5ea0cf328f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 1000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 4\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, './statlog_letterReducedSGD')\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<100:\n",
        "        learn = .1\n",
        "      elif ep >=100 and ep <= 400:\n",
        "        learn = .1\n",
        "      else:\n",
        "        learn = .001\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDv2')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedSGD\n",
            "epoch 0, training loss Total= 0.00046452947, training acc total= 100.0%\n",
            "ValidTest acc= 98.56 %\n",
            "epoch 50, training loss Total= 0.00042625214, training acc total= 100.0%\n",
            "ValidTest acc= 98.56 %\n",
            "epoch 100, training loss Total= 0.00039450754, training acc total= 100.0%\n",
            "ValidTest acc= 98.56 %\n",
            "epoch 150, training loss Total= 0.0003684589, training acc total= 100.0%\n",
            "ValidTest acc= 98.56 %\n",
            "epoch 200, training loss Total= 0.00034644315, training acc total= 100.0%\n",
            "ValidTest acc= 98.56 %\n",
            "epoch 250, training loss Total= 0.00032750235, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 300, training loss Total= 0.0003111477, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 350, training loss Total= 0.0002968709, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 400, training loss Total= 0.00028424725, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 450, training loss Total= 0.00028417452, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 500, training loss Total= 0.00028410146, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 550, training loss Total= 0.00028402853, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 600, training loss Total= 0.00028395586, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 650, training loss Total= 0.0002838833, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 700, training loss Total= 0.0002838108, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 750, training loss Total= 0.0002837385, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 800, training loss Total= 0.00028366642, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 850, training loss Total= 0.00028359427, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 900, training loss Total= 0.00028352224, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 950, training loss Total= 0.0002834503, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "ValidValid acc= 98.6 %\n",
            "ValidTest acc= 98.6 %\n",
            "==================================================\n",
            "W1\n",
            "4\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5eidMhdNkdq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Continue training further"
      ]
    },
    {
      "metadata": {
        "id": "8PxSe4tHkgl0",
        "colab_type": "code",
        "outputId": "764a6fa3-c72f-452e-9799-c9bac2f5e52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 1000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 4\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, './statlog_letterReducedSGDv2')\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<100:\n",
        "        learn = .1\n",
        "      elif ep >=100 and ep <= 800:\n",
        "        learn = .1\n",
        "      else:\n",
        "        learn = .01\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDv3')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedSGDv2\n",
            "epoch 0, training loss Total= 0.0002832128, training acc total= 100.0%\n",
            "ValidTest acc= 98.6 %\n",
            "epoch 50, training loss Total= 0.0002720635, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 100, training loss Total= 0.00026196075, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 150, training loss Total= 0.00025272742, training acc total= 100.0%\n",
            "ValidTest acc= 98.619995 %\n",
            "epoch 200, training loss Total= 0.00024428585, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 250, training loss Total= 0.00023655684, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 300, training loss Total= 0.00022942362, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 350, training loss Total= 0.00022283859, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 400, training loss Total= 0.00021672503, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 450, training loss Total= 0.0002110172, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 500, training loss Total= 0.00020564109, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 550, training loss Total= 0.00020057504, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 600, training loss Total= 0.00019579584, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 650, training loss Total= 0.00019129807, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 700, training loss Total= 0.0001870167, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 750, training loss Total= 0.00018292722, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 800, training loss Total= 0.00017903323, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 850, training loss Total= 0.0001786684, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 900, training loss Total= 0.00017830345, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 950, training loss Total= 0.00017794121, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "ValidValid acc= 98.64 %\n",
            "ValidTest acc= 98.64 %\n",
            "==================================================\n",
            "W1\n",
            "4\n",
            "W2\n",
            "1\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bfn0ctGsorTD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Continue further training"
      ]
    },
    {
      "metadata": {
        "id": "H_s6l49IkgZm",
        "colab_type": "code",
        "outputId": "b5a44c55-534e-44f0-97f4-a1b09692a33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1200
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 1000\n",
        "batch_size = 4112\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 50\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    out_layer = tf.matmul(layer_2, G_W3) + G_b3\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.tanh(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "wLoss1 = 4\n",
        "wLoss2 = 1\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, './statlog_letterReducedSGDv3')\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<100:\n",
        "        learn = .3\n",
        "      elif ep >=100 and ep <= 800:\n",
        "        learn = .2\n",
        "      else:\n",
        "        learn = .1\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGDv4')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedSGDv3\n",
            "epoch 0, training loss Total= 0.00017771152, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 50, training loss Total= 0.00016759773, training acc total= 100.0%\n",
            "ValidTest acc= 98.64 %\n",
            "epoch 100, training loss Total= 0.0001589612, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 150, training loss Total= 0.00015384436, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n",
            "epoch 200, training loss Total= 0.0001491598, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 250, training loss Total= 0.00014485916, training acc total= 100.0%\n",
            "ValidTest acc= 98.68 %\n",
            "epoch 300, training loss Total= 0.00014088204, training acc total= 100.0%\n",
            "ValidTest acc= 98.659996 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-7062f439706a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m           \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_train_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcombined_train_valid_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplot_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m           \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_5Q1MGzgGR48",
        "colab_type": "code",
        "outputId": "26adb962-2b8b-4bb8-8ad4-0295a2dd71ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1591
        }
      },
      "cell_type": "code",
      "source": [
        "# ## 123 Building the graph - Best!\n",
        "# saver = tf.train.Saver()\n",
        "# number_of_ex = combined_train_valid.shape[0]\n",
        "# hid_neuron = [104]\n",
        "# num_steps = 20000\n",
        "# # num_steps = 20000\n",
        "# number_of_epoch = 500000\n",
        "# batch_size = 2056\n",
        "# # batch_size = train_data.shape[0]\n",
        "\n",
        "# train_losses = []\n",
        "# test_acc = []\n",
        "# train_accuracy = []\n",
        "# val_accuracy = []\n",
        "# plot_every = 10000\n",
        "# best_accuracy_valid = 0\n",
        "# learning_rate = 0.001\n",
        "# track_step = []\n",
        "# tracked_valid_accuracy = []\n",
        "# total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "# step = 0\n",
        "# X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "# Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# lr = tf.placeholder(tf.float32, shape = [])\n",
        "# W_track = []\n",
        "# ValidAccuracy_Track = []\n",
        "# def neural_net(x,train = True):\n",
        "#     layer_outputs = []\n",
        "#     layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "#     layer_1 = tf.nn.relu(layer_1)\n",
        "#     out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_outputs.append(out_layer)\n",
        "#     for loop in range(0,2):        \n",
        "#         layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "#         layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "#         layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "#         layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "#         layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "#         layer_1 = tf.nn.relu(layer_1)\n",
        "#         layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#         layer_2 = tf.nn.relu(layer_2)\n",
        "#         out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "#         layer_outputs.append(out_layer)\n",
        "#     if train == True:\n",
        "#         return layer_outputs\n",
        "#     else:\n",
        "#         return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "# wLoss1 = 6\n",
        "# wLoss2 = 6\n",
        "# wLoss3 = 1\n",
        "# loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "# loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "# loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "# loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "# train_op = optimizer.minimize(loss)\n",
        "# correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# ### Initialization and running the model\n",
        "# with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "#     best_accuracy_valid = 0\n",
        "#     for ep in range(0,number_of_epoch):\n",
        "#       if ep<5000:\n",
        "#         learn = .1\n",
        "#       elif ep >=5000 and ep <= 8000:\n",
        "#         learn = .1\n",
        "#       elif ep >=8000 and ep <= 30000:\n",
        "#         learn = .1\n",
        "#       else:\n",
        "#         learn = .1\n",
        "#       for step in range(0, total_steps_for_one_pass):\n",
        "# #         print(step)\n",
        "# #         if (step>5000):\n",
        "# #           plot_every = 10\n",
        "        \n",
        "#         if step>=number_of_ex//batch_size:\n",
        "#           batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "# #           print(step,'Finishing',step*batch_size )\n",
        "#           step = 0\n",
        "          \n",
        "#         else:\n",
        "          \n",
        "#           start = step*batch_size\n",
        "#           finish = (step+1)*batch_size\n",
        "# #           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "#           batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "# #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "#         sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "#       if ep % plot_every == 0:\n",
        "#           train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "# #             train_accuracy.append(train_acc)\n",
        "# #             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "#           track_step.append(step)\n",
        "#           train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "#           train_accuracy.append(train_acc_total)\n",
        "#           train_losses.append(train_loss_total)\n",
        "#           print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "#           validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#           print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#           tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "#           if ep%plot_every == 0:\n",
        "#             if (validationTest_accuracy >= best_accuracy_valid):\n",
        "#               best_accuracy_valid = validationTest_accuracy\n",
        "#               saver.save(sess, './statlog_letterReducedSGD')\n",
        "#               G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "#   #         if(train_loss_total<0.033881765):\n",
        "#   #           break\n",
        "                                         \n",
        "#     validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "#     print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "#     validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "#     print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#     this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "#     W_track.append(this_params)\n",
        "# #     saver.save(sess, './statlog_letterAdam')\n",
        "#     print(\"=\"*50)\n",
        "#     print(\"W1\")\n",
        "#     print(wLoss1)\n",
        "\n",
        "#     print(\"W2\")\n",
        "#     print(wLoss2)\n",
        "#     print(\"*\"*50)\n",
        "    \n",
        "#     print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 1.3372589, training acc total= 11.152416467666626%\n",
            "ValidTest acc= 9.0 %\n",
            "epoch 10000, training loss Total= 0.088571586, training acc total= 93.28376650810242%\n",
            "ValidTest acc= 92.0 %\n",
            "epoch 20000, training loss Total= 0.0610336, training acc total= 95.36555409431458%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 30000, training loss Total= 0.04483484, training acc total= 96.67905569076538%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 40000, training loss Total= 0.033418465, training acc total= 97.89343476295471%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 50000, training loss Total= 0.025500294, training acc total= 98.76084327697754%\n",
            "ValidTest acc= 92.5 %\n",
            "epoch 60000, training loss Total= 0.019763455, training acc total= 99.25650358200073%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 70000, training loss Total= 0.01554927, training acc total= 99.52911734580994%\n",
            "ValidTest acc= 92.5 %\n",
            "epoch 80000, training loss Total= 0.012403308, training acc total= 99.7273862361908%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 90000, training loss Total= 0.010048573, training acc total= 99.85129833221436%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 100000, training loss Total= 0.008284461, training acc total= 99.90086555480957%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 110000, training loss Total= 0.006932225, training acc total= 99.95043277740479%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 120000, training loss Total= 0.0058803027, training acc total= 99.95043277740479%\n",
            "ValidTest acc= 92.0 %\n",
            "epoch 130000, training loss Total= 0.005063422, training acc total= 99.97521638870239%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 140000, training loss Total= 0.004413701, training acc total= 100.0%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 150000, training loss Total= 0.003883867, training acc total= 100.0%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 160000, training loss Total= 0.0034489057, training acc total= 100.0%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 170000, training loss Total= 0.0030876966, training acc total= 100.0%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 180000, training loss Total= 0.0027836643, training acc total= 100.0%\n",
            "ValidTest acc= 92.25 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-1174e413988d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m           \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_train_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcombined_train_valid_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplot_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m           \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GoIsmarzTPqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### fine tune for higher precision for no. epochs"
      ]
    },
    {
      "metadata": {
        "id": "isv5BHBYI-JI",
        "colab_type": "code",
        "outputId": "e6e371a2-85f7-4ae4-d84b-f74461faf396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 100000\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 10000\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 6\n",
        "wLoss2 = 6\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<5000:\n",
        "        learn = .1\n",
        "      elif ep >=5000 and ep <= 8000:\n",
        "        learn = .1\n",
        "      elif ep >=8000 and ep <= 30000:\n",
        "        learn = .1\n",
        "      else:\n",
        "        learn = .1\n",
        "      if ep>90000:\n",
        "        plot_every = 1000\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "          if ep%plot_every == 0:\n",
        "            if (validationTest_accuracy >= best_accuracy_valid):\n",
        "              best_accuracy_valid = validationTest_accuracy\n",
        "              saver.save(sess, './statlog_letterReducedSGD')\n",
        "              G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "    print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "    validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_letterAdam')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 1.3372589, training acc total= 11.152416467666626%\n",
            "ValidTest acc= 9.0 %\n",
            "epoch 10000, training loss Total= 0.088571586, training acc total= 93.28376650810242%\n",
            "ValidTest acc= 92.0 %\n",
            "epoch 20000, training loss Total= 0.0610336, training acc total= 95.36555409431458%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 30000, training loss Total= 0.04483484, training acc total= 96.67905569076538%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 40000, training loss Total= 0.033418465, training acc total= 97.89343476295471%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 50000, training loss Total= 0.025500294, training acc total= 98.76084327697754%\n",
            "ValidTest acc= 92.5 %\n",
            "epoch 60000, training loss Total= 0.019763455, training acc total= 99.25650358200073%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 70000, training loss Total= 0.01554927, training acc total= 99.52911734580994%\n",
            "ValidTest acc= 92.5 %\n",
            "epoch 80000, training loss Total= 0.012403308, training acc total= 99.7273862361908%\n",
            "ValidTest acc= 92.25 %\n",
            "epoch 90000, training loss Total= 0.010048573, training acc total= 99.85129833221436%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 91000, training loss Total= 0.009847421, training acc total= 99.85129833221436%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 92000, training loss Total= 0.009653716, training acc total= 99.87608194351196%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 93000, training loss Total= 0.009467222, training acc total= 99.87608194351196%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 94000, training loss Total= 0.009284329, training acc total= 99.87608194351196%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 95000, training loss Total= 0.009105676, training acc total= 99.87608194351196%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 96000, training loss Total= 0.008931247, training acc total= 99.87608194351196%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 97000, training loss Total= 0.008763032, training acc total= 99.90086555480957%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 98000, training loss Total= 0.008598233, training acc total= 99.90086555480957%\n",
            "ValidTest acc= 92.75 %\n",
            "epoch 99000, training loss Total= 0.008438703, training acc total= 99.90086555480957%\n",
            "ValidTest acc= 92.75 %\n",
            "ValidValid acc= 97.595795 %\n",
            "ValidTest acc= 92.75 %\n",
            "==================================================\n",
            "W1\n",
            "6\n",
            "W2\n",
            "6\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lZ5IPzM1aO3S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train entire data till 10000 epochs"
      ]
    },
    {
      "metadata": {
        "id": "Z6C75YaCI-Fo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 1\n",
        "aside_valid_test = train_valid_combined_shuffled[-aside_examples:]\n",
        "aside_valid_test_label = train_valid_combined_shuffled_label[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined_shuffled[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = train_valid_combined_shuffled_label[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qBmylidkbByd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_valid_combined_shuffled.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mg-HRK0eeRsS",
        "colab_type": "code",
        "outputId": "dfbe6085-3051-4bed-aa77-ecc814bc09a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "number_of_ex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "I9njcFoVeUah",
        "colab_type": "code",
        "outputId": "4ff57911-ce86-4342-e75d-480a2f869728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "cell_type": "code",
      "source": [
        "## 123 Building the graph - Best!\n",
        "saver = tf.train.Saver()\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "hid_neuron = [104]\n",
        "num_steps = 20000\n",
        "# num_steps = 20000\n",
        "number_of_epoch = 95000\n",
        "batch_size = 2056\n",
        "# batch_size = train_data.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_acc = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "plot_every = 10000\n",
        "best_accuracy_valid = 0\n",
        "learning_rate = 0.001\n",
        "track_step = []\n",
        "tracked_valid_accuracy = []\n",
        "total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "step = 0\n",
        "X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "lr = tf.placeholder(tf.float32, shape = [])\n",
        "W_track = []\n",
        "ValidAccuracy_Track = []\n",
        "def neural_net(x,train = True):\n",
        "    layer_outputs = []\n",
        "    layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "    layer_outputs.append(out_layer)\n",
        "    for loop in range(0,2):        \n",
        "        layer1_feedback1 = tf.add(tf.matmul(out_layer, G_w_out_h1), G_b_out_h1)\n",
        "        layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "        layer1_feedback2 = tf.add(tf.matmul(layer_2, G_w_h2_h1), G_b_h2_h1)\n",
        "        layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "\n",
        "        layer_1 = layer_1 + layer1_feedback1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1f), G_b1f)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.matmul(layer_1, G_W2) + G_b2\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        out_layer = tf.matmul(layer_2, G_W3) + G_b3 #+ tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "\n",
        "        layer_outputs.append(out_layer)\n",
        "    if train == True:\n",
        "        return layer_outputs\n",
        "    else:\n",
        "        return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "wLoss1 = 6\n",
        "wLoss2 = 6\n",
        "wLoss3 = 1\n",
        "loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "train_op = optimizer.minimize(loss)\n",
        "correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "### Initialization and running the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    best_accuracy_valid = 0\n",
        "    for ep in range(0,number_of_epoch):\n",
        "      if ep<5000:\n",
        "        learn = .1\n",
        "      elif ep >=5000 and ep <= 8000:\n",
        "        learn = .1\n",
        "      elif ep >=8000 and ep <= 30000:\n",
        "        learn = .1\n",
        "      else:\n",
        "        learn = .1\n",
        "      if ep>90000:\n",
        "        plot_every = 1000\n",
        "      for step in range(0, total_steps_for_one_pass):\n",
        "#         print(step)\n",
        "#         if (step>5000):\n",
        "#           plot_every = 10\n",
        "        \n",
        "        if step>=number_of_ex//batch_size:\n",
        "          batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "#           print(step,'Finishing',step*batch_size )\n",
        "          step = 0\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          start = step*batch_size\n",
        "          finish = (step+1)*batch_size\n",
        "#           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "          batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "#         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "      if ep % plot_every == 0:\n",
        "          train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "#             train_accuracy.append(train_acc)\n",
        "#             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "          track_step.append(step)\n",
        "          train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "          train_accuracy.append(train_acc_total)\n",
        "          train_losses.append(train_loss_total)\n",
        "          print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "          validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "          print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "          tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "#           if ep%plot_every == 0:\n",
        "#             if (validationTest_accuracy >= best_accuracy_valid):\n",
        "#               best_accuracy_valid = validationTest_accuracy\n",
        "#               saver.save(sess, './statlog_letterReducedSGD')\n",
        "#               G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "  #         if(train_loss_total<0.033881765):\n",
        "  #           break\n",
        "                                         \n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "    W_track.append(this_params)\n",
        "    saver.save(sess, './statlog_satimFullSGDFinal')\n",
        "    print(\"=\"*50)\n",
        "    print(\"W1\")\n",
        "    print(wLoss1)\n",
        "\n",
        "    print(\"W2\")\n",
        "    print(wLoss2)\n",
        "    print(\"*\"*50)\n",
        "    \n",
        "    print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 1.1931542, training acc total= 18.155165016651154%\n",
            "ValidTest acc= 0.0 %\n",
            "epoch 10000, training loss Total= 0.08674385, training acc total= 93.121337890625%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 20000, training loss Total= 0.058712892, training acc total= 95.42174339294434%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 30000, training loss Total= 0.042636193, training acc total= 97.00044989585876%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 40000, training loss Total= 0.032625627, training acc total= 97.94767498970032%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 50000, training loss Total= 0.025930004, training acc total= 98.46639633178711%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 60000, training loss Total= 0.020940715, training acc total= 98.73703122138977%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 70000, training loss Total= 0.017129704, training acc total= 99.00766611099243%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 80000, training loss Total= 0.0141686825, training acc total= 99.32340979576111%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 90000, training loss Total= 0.011864761, training acc total= 99.50383305549622%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 91000, training loss Total= 0.011662132, training acc total= 99.54894185066223%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 92000, training loss Total= 0.011462668, training acc total= 99.54894185066223%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 93000, training loss Total= 0.011267941, training acc total= 99.57149028778076%\n",
            "ValidTest acc= 100.0 %\n",
            "epoch 94000, training loss Total= 0.011080176, training acc total= 99.61659908294678%\n",
            "ValidTest acc= 100.0 %\n",
            "Train acc= 0.99594045 %\n",
            "==================================================\n",
            "W1\n",
            "6\n",
            "W2\n",
            "6\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g6zM5JVcI-Bv",
        "colab_type": "code",
        "outputId": "b60d9e04-3733-47d9-d72a-716e5f8de85c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "cell_type": "code",
      "source": [
        "# ## 123 Building the graph - Best!\n",
        "# saver = tf.train.Saver()\n",
        "# number_of_ex = combined_train_valid.shape[0]\n",
        "# hid_neuron = [104]a\n",
        "# num_steps = 150000\n",
        "# # num_steps = 20000\n",
        "# number_of_epoch = 95000\n",
        "# batch_size = 2056\n",
        "# # batch_size = train_data.shape[0]\n",
        "\n",
        "# train_losses = []\n",
        "# test_acc = []\n",
        "# train_accuracy = []\n",
        "# val_accuracy = []\n",
        "# plot_every = 1000\n",
        "# best_accuracy_valid = 0\n",
        "# # learning_rate = 0.001\n",
        "# track_step = []\n",
        "# tracked_valid_accuracy = []\n",
        "# total_steps_for_one_pass = number_of_ex//batch_size + 1\n",
        "# step = 0\n",
        "# X = tf.placeholder(\"float\", [None, train_data.shape[1]])\n",
        "# Y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# lr = tf.placeholder(tf.float32, shape = [])\n",
        "# W_track = []\n",
        "# ValidAccuracy_Track = []\n",
        "# def neural_net(x,train = True):\n",
        "#     layer_outputs = []\n",
        "#     layer_1 = tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "#     layer_1 = tf.nn.relu(layer_1)\n",
        "#     out_layer = tf.matmul(layer_1, G_W2) + G_b2\n",
        "#     layer_outputs.append(out_layer)\n",
        "#     for loop in range(0,2):        \n",
        "#         layer1_feedback1 = tf.add(tf.matmul(out_layer, GwLoop), G_bLoop)\n",
        "#         layer1_feedback1 = tf.nn.relu(layer1_feedback1)\n",
        "#         layer1_feedback2 = tf.add(tf.matmul(layer1_feedback1, GwLoop2), G_bLoop2)\n",
        "#         layer1_feedback2 = tf.nn.tanh(layer1_feedback2)\n",
        "#         layer_1 = layer_1 + layer1_feedback2 + tf.add(tf.matmul(x, G_W1), G_b1)\n",
        "#         out_layer = (tf.matmul(layer_1, G_W2) + G_b2) + tf.nn.tanh((tf.matmul(out_layer, GLossW) + GLossb))\n",
        "#         layer_outputs.append(out_layer)\n",
        "#     if train == True:\n",
        "#         return layer_outputs\n",
        "#     else:\n",
        "#         return layer_outputs[0]\n",
        "\n",
        "\n",
        "\n",
        "# wLoss1 = 6\n",
        "# wLoss2 = 6\n",
        "# wLoss3 = 1\n",
        "# loss1 = wLoss1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[0], labels=Y))\n",
        "# loss2 = wLoss2*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[1], labels=Y))\n",
        "# loss3 = wLoss3*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_net(X)[2], labels=Y))\n",
        "\n",
        "# loss = (loss1+loss2+loss3)/(wLoss1+ wLoss2 + wLoss3)\n",
        "\n",
        "\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "\n",
        "# train_op = optimizer.minimize(loss)\n",
        "# correct_pred = tf.equal(tf.argmax(neural_net(X)[0], 1), tf.argmax(Y, 1))\n",
        "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# ### Initialization and running the model\n",
        "# with tf.Session() as sess:\n",
        "#     sess.run(tf.global_variables_initializer())\n",
        "#     best_accuracy_valid = 0\n",
        "#     for ep in range(0,number_of_epoch):\n",
        "#       if ep<5000:\n",
        "#         learn = .1\n",
        "#       elif ep >=5000 and ep <= 8000:\n",
        "#         learn = .1\n",
        "#       elif ep >=8000 and ep <= 30000:\n",
        "#         learn = .1\n",
        "#       else:\n",
        "#         learn = .1\n",
        "#       if ep>90000:\n",
        "#         plot_every = 1000\n",
        "#       for step in range(0, total_steps_for_one_pass):\n",
        "# #         print(step)\n",
        "# #         if (step>5000):\n",
        "# #           plot_every = 10\n",
        "        \n",
        "#         if step>=number_of_ex//batch_size:\n",
        "#           batch_x, batch_y = combined_train_valid[step*batch_size:,:],combined_train_valid_label[step*batch_size:,:]\n",
        "# #           print(step,'Finishing',step*batch_size )\n",
        "#           step = 0\n",
        "          \n",
        "#         else:\n",
        "          \n",
        "#           start = step*batch_size\n",
        "#           finish = (step+1)*batch_size\n",
        "# #           print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "#           batch_x, batch_y = combined_train_valid[step:finish,:],combined_train_valid_label[step:finish,:]\n",
        "# #         batch_x, batch_y = next_batch(batch_size, combined_train_valid, combined_train_valid_label)\n",
        "#         sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, lr: learn})\n",
        "#       if ep % plot_every == 0:\n",
        "#           train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "# #             train_accuracy.append(train_acc)\n",
        "# #             print(\"step \" + str(step) + \", training loss= \" + str(train_loss) +\", training acc= \"+str(train_acc*100)+\"%\")\n",
        "#           track_step.append(step)\n",
        "#           train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "#           train_accuracy.append(train_acc_total)\n",
        "#           train_losses.append(train_loss_total)\n",
        "#           print(\"epoch \" + str(ep) + \", training loss Total= \" + str(train_loss_total) +\", training acc total= \"+str(train_acc_total*100)+\"%\")\n",
        "#           validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "# #           print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#           tracked_valid_accuracy.append(validationTest_accuracy)\n",
        "# #           if ep%plot_every == 0:\n",
        "# #             if (validationTest_accuracy >= best_accuracy_valid):\n",
        "# #               best_accuracy_valid = validationTest_accuracy\n",
        "# #               saver.save(sess, './statlog_letterReducedSGD')\n",
        "# #               G_W1np, G_b1np, G_W2np, G_b2np = sess.run([G_W1, G_b1, G_W2, G_b2])\n",
        "#   #         if(train_loss_total<0.033881765):\n",
        "#   #           break\n",
        "                                         \n",
        "# #     validationValid_accuracy = sess.run(accuracy*100, feed_dict={X: validation_data,Y:validation_label_one_hot})\n",
        "# #     print(\"ValidValid acc=\",str(validationValid_accuracy), \"%\")\n",
        "    \n",
        "# #     validationTest_accuracy = sess.run(accuracy*100, feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "# #     print(\"ValidTest acc=\",str(validationTest_accuracy), \"%\")\n",
        "#     train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: combined_train_valid,Y: combined_train_valid_label})\n",
        "#     print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "#     this_params = G_W1np, G_b1np, G_W2np, G_b2np\n",
        "#     W_track.append(this_params)\n",
        "#     saver.save(sess, './statlog_satimFullSGDFinal')\n",
        "#     print(\"=\"*50)\n",
        "#     print(\"W1\")\n",
        "#     print(wLoss1)\n",
        "\n",
        "#     print(\"W2\")\n",
        "#     print(wLoss2)\n",
        "#     print(\"*\"*50)\n",
        "    \n",
        "#     print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, training loss Total= 1.6421824, training acc total= 89.24221992492676%\n",
            "epoch 1000, training loss Total= 0.1167705, training acc total= 95.24131417274475%\n",
            "epoch 2000, training loss Total= 0.10244689, training acc total= 95.82769274711609%\n",
            "epoch 3000, training loss Total= 0.091625914, training acc total= 95.94045877456665%\n",
            "epoch 4000, training loss Total= 0.08408962, training acc total= 96.23364806175232%\n",
            "epoch 5000, training loss Total= 0.073307745, training acc total= 96.32385969161987%\n",
            "epoch 6000, training loss Total= 0.066499114, training acc total= 96.52683734893799%\n",
            "epoch 7000, training loss Total= 0.060303714, training acc total= 96.68470621109009%\n",
            "epoch 8000, training loss Total= 0.05692737, training acc total= 96.68470621109009%\n",
            "epoch 8100, training loss Total= 0.056117997, training acc total= 96.68470621109009%\n",
            "epoch 8200, training loss Total= 0.055982567, training acc total= 96.66215777397156%\n",
            "epoch 8300, training loss Total= 0.0558603, training acc total= 96.66215777397156%\n",
            "epoch 8400, training loss Total= 0.055742033, training acc total= 96.68470621109009%\n",
            "epoch 8500, training loss Total= 0.055630244, training acc total= 96.66215777397156%\n",
            "epoch 8600, training loss Total= 0.055525698, training acc total= 96.68470621109009%\n",
            "epoch 8700, training loss Total= 0.05542909, training acc total= 96.66215777397156%\n",
            "epoch 8800, training loss Total= 0.05533664, training acc total= 96.7072606086731%\n",
            "epoch 8900, training loss Total= 0.05524673, training acc total= 96.7298150062561%\n",
            "epoch 9000, training loss Total= 0.055158224, training acc total= 96.7072606086731%\n",
            "epoch 9100, training loss Total= 0.055071894, training acc total= 96.7072606086731%\n",
            "epoch 9200, training loss Total= 0.054983452, training acc total= 96.7072606086731%\n",
            "epoch 9300, training loss Total= 0.054898534, training acc total= 96.7072606086731%\n",
            "epoch 9400, training loss Total= 0.05481777, training acc total= 96.7298150062561%\n",
            "Train acc= 0.96729815 %\n",
            "==================================================\n",
            "W1\n",
            "6\n",
            "W2\n",
            "6\n",
            "**************************************************\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_tPpj4B9bzVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HhHoRGntavcF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check on test set!"
      ]
    },
    {
      "metadata": {
        "id": "QlzkjrMfavOm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "uYTrY2rwI9-C",
        "colab_type": "code",
        "outputId": "e753b2d4-abec-4537-b58f-1131a64fa59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, './statlog_letterReducedSGDv4')\n",
        "    train_loss_total, train_acc_total = sess.run([loss, accuracy], feed_dict={X: aside_valid_test,Y:aside_valid_test_label})\n",
        "    print(\"Train acc=\",str(train_acc_total), \"%\")\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={X: test_data,Y:test_label_one_hot})\n",
        "    print(\"Test acc=\",str(test_accuracy), \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./statlog_letterReducedSGDv4\n",
            "Train acc= 0.9868 %\n",
            "Test acc= 98.46 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iLSesmCEI96T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}