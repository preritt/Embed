{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(2)PendigitFullTest97p4DatasetMomentSGD_ProbabilityBased04282019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/(2)PendigitFullTest97p4DatasetMomentSGD_ProbabilityBased04282019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qWL1XvRKt0EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4rTq2SzhyZgD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "# X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "# X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "# X_test, y_test             = mnist.test.images, mnist.test.labels\n",
        "\n",
        "# assert(len(X_train) == len(y_train))\n",
        "# assert(len(X_validation) == len(y_validation))\n",
        "# assert(len(X_test) == len(y_test))\n",
        "\n",
        "# print()\n",
        "# print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "# print()\n",
        "# print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "# print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "# print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lopHk729ydu5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_data = X_train.reshape(X_train.shape[0],-1)\n",
        "# train_label = y_train\n",
        "# validation_data = X_validation.reshape(X_validation.shape[0],-1)\n",
        "# validation_label = y_validation\n",
        "# test_data = X_test.reshape(X_test.shape[0],-1)\n",
        "# test_label = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yuoNYnTshaZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4FSwFTChgdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_label = np.concatenate((train_label, validation_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VloppGYRyrEX",
        "colab_type": "code",
        "outputId": "f0c99e69-57cb-411e-a67f-cca5051a0fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8690
        }
      },
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(80, 40, ), max_iter=500, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "\n",
        "\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.95489119\n",
            "Iteration 2, loss = 0.98170180\n",
            "Iteration 3, loss = 0.50873149\n",
            "Iteration 4, loss = 0.33459205\n",
            "Iteration 5, loss = 0.25779496\n",
            "Iteration 6, loss = 0.21278694\n",
            "Iteration 7, loss = 0.18346620\n",
            "Iteration 8, loss = 0.15914957\n",
            "Iteration 9, loss = 0.14152727\n",
            "Iteration 10, loss = 0.12701235\n",
            "Iteration 11, loss = 0.11560525\n",
            "Iteration 12, loss = 0.10628861\n",
            "Iteration 13, loss = 0.09794105\n",
            "Iteration 14, loss = 0.09102073\n",
            "Iteration 15, loss = 0.08553084\n",
            "Iteration 16, loss = 0.08047449\n",
            "Iteration 17, loss = 0.07596875\n",
            "Iteration 18, loss = 0.07220618\n",
            "Iteration 19, loss = 0.06878648\n",
            "Iteration 20, loss = 0.06588877\n",
            "Iteration 21, loss = 0.06339299\n",
            "Iteration 22, loss = 0.06088005\n",
            "Iteration 23, loss = 0.05833481\n",
            "Iteration 24, loss = 0.05645611\n",
            "Iteration 25, loss = 0.05408837\n",
            "Iteration 26, loss = 0.05232453\n",
            "Iteration 27, loss = 0.05047872\n",
            "Iteration 28, loss = 0.04933457\n",
            "Iteration 29, loss = 0.04746194\n",
            "Iteration 30, loss = 0.04640093\n",
            "Iteration 31, loss = 0.04464777\n",
            "Iteration 32, loss = 0.04361155\n",
            "Iteration 33, loss = 0.04231529\n",
            "Iteration 34, loss = 0.04125936\n",
            "Iteration 35, loss = 0.04053062\n",
            "Iteration 36, loss = 0.03940358\n",
            "Iteration 37, loss = 0.03834965\n",
            "Iteration 38, loss = 0.03753469\n",
            "Iteration 39, loss = 0.03654027\n",
            "Iteration 40, loss = 0.03572254\n",
            "Iteration 41, loss = 0.03497387\n",
            "Iteration 42, loss = 0.03433652\n",
            "Iteration 43, loss = 0.03348544\n",
            "Iteration 44, loss = 0.03283693\n",
            "Iteration 45, loss = 0.03218934\n",
            "Iteration 46, loss = 0.03149528\n",
            "Iteration 47, loss = 0.03094953\n",
            "Iteration 48, loss = 0.03041304\n",
            "Iteration 49, loss = 0.02985878\n",
            "Iteration 50, loss = 0.02926056\n",
            "Iteration 51, loss = 0.02890535\n",
            "Iteration 52, loss = 0.02831707\n",
            "Iteration 53, loss = 0.02764853\n",
            "Iteration 54, loss = 0.02736471\n",
            "Iteration 55, loss = 0.02673735\n",
            "Iteration 56, loss = 0.02637397\n",
            "Iteration 57, loss = 0.02589611\n",
            "Iteration 58, loss = 0.02558105\n",
            "Iteration 59, loss = 0.02499821\n",
            "Iteration 60, loss = 0.02478820\n",
            "Iteration 61, loss = 0.02425511\n",
            "Iteration 62, loss = 0.02397880\n",
            "Iteration 63, loss = 0.02371509\n",
            "Iteration 64, loss = 0.02320203\n",
            "Iteration 65, loss = 0.02280332\n",
            "Iteration 66, loss = 0.02251338\n",
            "Iteration 67, loss = 0.02227458\n",
            "Iteration 68, loss = 0.02191484\n",
            "Iteration 69, loss = 0.02143715\n",
            "Iteration 70, loss = 0.02130694\n",
            "Iteration 71, loss = 0.02096261\n",
            "Iteration 72, loss = 0.02074336\n",
            "Iteration 73, loss = 0.02036040\n",
            "Iteration 74, loss = 0.01999286\n",
            "Iteration 75, loss = 0.01976113\n",
            "Iteration 76, loss = 0.01964329\n",
            "Iteration 77, loss = 0.01946378\n",
            "Iteration 78, loss = 0.01919776\n",
            "Iteration 79, loss = 0.01877982\n",
            "Iteration 80, loss = 0.01851510\n",
            "Iteration 81, loss = 0.01831039\n",
            "Iteration 82, loss = 0.01810026\n",
            "Iteration 83, loss = 0.01805545\n",
            "Iteration 84, loss = 0.01765292\n",
            "Iteration 85, loss = 0.01760867\n",
            "Iteration 86, loss = 0.01721313\n",
            "Iteration 87, loss = 0.01696317\n",
            "Iteration 88, loss = 0.01687569\n",
            "Iteration 89, loss = 0.01657610\n",
            "Iteration 90, loss = 0.01636889\n",
            "Iteration 91, loss = 0.01614179\n",
            "Iteration 92, loss = 0.01606604\n",
            "Iteration 93, loss = 0.01585744\n",
            "Iteration 94, loss = 0.01567951\n",
            "Iteration 95, loss = 0.01550237\n",
            "Iteration 96, loss = 0.01521800\n",
            "Iteration 97, loss = 0.01512489\n",
            "Iteration 98, loss = 0.01492536\n",
            "Iteration 99, loss = 0.01469476\n",
            "Iteration 100, loss = 0.01461964\n",
            "Iteration 101, loss = 0.01457004\n",
            "Iteration 102, loss = 0.01431109\n",
            "Iteration 103, loss = 0.01412683\n",
            "Iteration 104, loss = 0.01398944\n",
            "Iteration 105, loss = 0.01378379\n",
            "Iteration 106, loss = 0.01364602\n",
            "Iteration 107, loss = 0.01360926\n",
            "Iteration 108, loss = 0.01347256\n",
            "Iteration 109, loss = 0.01331756\n",
            "Iteration 110, loss = 0.01312382\n",
            "Iteration 111, loss = 0.01295702\n",
            "Iteration 112, loss = 0.01286147\n",
            "Iteration 113, loss = 0.01276808\n",
            "Iteration 114, loss = 0.01260671\n",
            "Iteration 115, loss = 0.01249043\n",
            "Iteration 116, loss = 0.01247270\n",
            "Iteration 117, loss = 0.01230549\n",
            "Iteration 118, loss = 0.01221680\n",
            "Iteration 119, loss = 0.01207492\n",
            "Iteration 120, loss = 0.01196955\n",
            "Iteration 121, loss = 0.01197146\n",
            "Iteration 122, loss = 0.01170303\n",
            "Iteration 123, loss = 0.01161692\n",
            "Iteration 124, loss = 0.01153516\n",
            "Iteration 125, loss = 0.01145848\n",
            "Iteration 126, loss = 0.01133453\n",
            "Iteration 127, loss = 0.01119464\n",
            "Iteration 128, loss = 0.01112531\n",
            "Iteration 129, loss = 0.01105446\n",
            "Iteration 130, loss = 0.01099228\n",
            "Iteration 131, loss = 0.01089062\n",
            "Iteration 132, loss = 0.01071887\n",
            "Iteration 133, loss = 0.01065959\n",
            "Iteration 134, loss = 0.01060974\n",
            "Iteration 135, loss = 0.01056412\n",
            "Iteration 136, loss = 0.01042939\n",
            "Iteration 137, loss = 0.01034429\n",
            "Iteration 138, loss = 0.01023560\n",
            "Iteration 139, loss = 0.01021425\n",
            "Iteration 140, loss = 0.01011674\n",
            "Iteration 141, loss = 0.01008340\n",
            "Iteration 142, loss = 0.00993935\n",
            "Iteration 143, loss = 0.00991796\n",
            "Iteration 144, loss = 0.00977318\n",
            "Iteration 145, loss = 0.00978138\n",
            "Iteration 146, loss = 0.00967579\n",
            "Iteration 147, loss = 0.00962418\n",
            "Iteration 148, loss = 0.00965236\n",
            "Iteration 149, loss = 0.00946203\n",
            "Iteration 150, loss = 0.00942940\n",
            "Iteration 151, loss = 0.00927938\n",
            "Iteration 152, loss = 0.00921360\n",
            "Iteration 153, loss = 0.00917654\n",
            "Iteration 154, loss = 0.00909055\n",
            "Iteration 155, loss = 0.00899737\n",
            "Iteration 156, loss = 0.00901189\n",
            "Iteration 157, loss = 0.00897226\n",
            "Iteration 158, loss = 0.00892190\n",
            "Iteration 159, loss = 0.00878435\n",
            "Iteration 160, loss = 0.00877368\n",
            "Iteration 161, loss = 0.00876143\n",
            "Iteration 162, loss = 0.00861978\n",
            "Iteration 163, loss = 0.00851081\n",
            "Iteration 164, loss = 0.00851410\n",
            "Iteration 165, loss = 0.00844975\n",
            "Iteration 166, loss = 0.00838342\n",
            "Iteration 167, loss = 0.00837549\n",
            "Iteration 168, loss = 0.00828231\n",
            "Iteration 169, loss = 0.00817145\n",
            "Iteration 170, loss = 0.00816321\n",
            "Iteration 171, loss = 0.00808684\n",
            "Iteration 172, loss = 0.00806949\n",
            "Iteration 173, loss = 0.00800791\n",
            "Iteration 174, loss = 0.00796241\n",
            "Iteration 175, loss = 0.00796155\n",
            "Iteration 176, loss = 0.00788167\n",
            "Iteration 177, loss = 0.00788849\n",
            "Iteration 178, loss = 0.00779037\n",
            "Iteration 179, loss = 0.00774054\n",
            "Iteration 180, loss = 0.00771362\n",
            "Iteration 181, loss = 0.00765176\n",
            "Iteration 182, loss = 0.00760335\n",
            "Iteration 183, loss = 0.00757704\n",
            "Iteration 184, loss = 0.00752110\n",
            "Iteration 185, loss = 0.00749897\n",
            "Iteration 186, loss = 0.00748372\n",
            "Iteration 187, loss = 0.00739768\n",
            "Iteration 188, loss = 0.00737927\n",
            "Iteration 189, loss = 0.00728785\n",
            "Iteration 190, loss = 0.00731329\n",
            "Iteration 191, loss = 0.00724249\n",
            "Iteration 192, loss = 0.00719424\n",
            "Iteration 193, loss = 0.00716928\n",
            "Iteration 194, loss = 0.00713646\n",
            "Iteration 195, loss = 0.00705879\n",
            "Iteration 196, loss = 0.00699678\n",
            "Iteration 197, loss = 0.00704082\n",
            "Iteration 198, loss = 0.00694456\n",
            "Iteration 199, loss = 0.00693006\n",
            "Iteration 200, loss = 0.00703286\n",
            "Iteration 201, loss = 0.00683479\n",
            "Iteration 202, loss = 0.00679797\n",
            "Iteration 203, loss = 0.00681204\n",
            "Iteration 204, loss = 0.00674059\n",
            "Iteration 205, loss = 0.00681136\n",
            "Iteration 206, loss = 0.00673915\n",
            "Iteration 207, loss = 0.00664000\n",
            "Iteration 208, loss = 0.00673062\n",
            "Iteration 209, loss = 0.00658162\n",
            "Iteration 210, loss = 0.00653296\n",
            "Iteration 211, loss = 0.00651722\n",
            "Iteration 212, loss = 0.00649511\n",
            "Iteration 213, loss = 0.00642087\n",
            "Iteration 214, loss = 0.00637117\n",
            "Iteration 215, loss = 0.00632132\n",
            "Iteration 216, loss = 0.00625817\n",
            "Iteration 217, loss = 0.00630890\n",
            "Iteration 218, loss = 0.00628571\n",
            "Iteration 219, loss = 0.00623650\n",
            "Iteration 220, loss = 0.00622738\n",
            "Iteration 221, loss = 0.00617780\n",
            "Iteration 222, loss = 0.00612142\n",
            "Iteration 223, loss = 0.00613518\n",
            "Iteration 224, loss = 0.00606616\n",
            "Iteration 225, loss = 0.00604892\n",
            "Iteration 226, loss = 0.00604637\n",
            "Iteration 227, loss = 0.00600258\n",
            "Iteration 228, loss = 0.00594099\n",
            "Iteration 229, loss = 0.00595039\n",
            "Iteration 230, loss = 0.00590738\n",
            "Iteration 231, loss = 0.00588093\n",
            "Iteration 232, loss = 0.00584840\n",
            "Iteration 233, loss = 0.00589290\n",
            "Iteration 234, loss = 0.00582259\n",
            "Iteration 235, loss = 0.00577467\n",
            "Iteration 236, loss = 0.00573641\n",
            "Iteration 237, loss = 0.00575422\n",
            "Iteration 238, loss = 0.00570247\n",
            "Iteration 239, loss = 0.00564626\n",
            "Iteration 240, loss = 0.00564861\n",
            "Iteration 241, loss = 0.00565070\n",
            "Iteration 242, loss = 0.00571307\n",
            "Iteration 243, loss = 0.00554996\n",
            "Iteration 244, loss = 0.00558301\n",
            "Iteration 245, loss = 0.00555869\n",
            "Iteration 246, loss = 0.00549021\n",
            "Iteration 247, loss = 0.00546189\n",
            "Iteration 248, loss = 0.00544382\n",
            "Iteration 249, loss = 0.00544870\n",
            "Iteration 250, loss = 0.00549972\n",
            "Iteration 251, loss = 0.00536773\n",
            "Iteration 252, loss = 0.00537383\n",
            "Iteration 253, loss = 0.00531100\n",
            "Iteration 254, loss = 0.00527442\n",
            "Iteration 255, loss = 0.00527742\n",
            "Iteration 256, loss = 0.00528258\n",
            "Iteration 257, loss = 0.00528959\n",
            "Iteration 258, loss = 0.00523992\n",
            "Iteration 259, loss = 0.00533636\n",
            "Iteration 260, loss = 0.00516047\n",
            "Iteration 261, loss = 0.00515355\n",
            "Iteration 262, loss = 0.00514762\n",
            "Iteration 263, loss = 0.00509594\n",
            "Iteration 264, loss = 0.00514277\n",
            "Iteration 265, loss = 0.00503560\n",
            "Iteration 266, loss = 0.00511873\n",
            "Iteration 267, loss = 0.00502775\n",
            "Iteration 268, loss = 0.00505321\n",
            "Iteration 269, loss = 0.00498597\n",
            "Iteration 270, loss = 0.00496915\n",
            "Iteration 271, loss = 0.00493941\n",
            "Iteration 272, loss = 0.00493335\n",
            "Iteration 273, loss = 0.00497301\n",
            "Iteration 274, loss = 0.00490693\n",
            "Iteration 275, loss = 0.00492691\n",
            "Iteration 276, loss = 0.00486849\n",
            "Iteration 277, loss = 0.00486416\n",
            "Iteration 278, loss = 0.00481117\n",
            "Iteration 279, loss = 0.00484692\n",
            "Iteration 280, loss = 0.00488550\n",
            "Iteration 281, loss = 0.00475579\n",
            "Iteration 282, loss = 0.00474359\n",
            "Iteration 283, loss = 0.00475242\n",
            "Iteration 284, loss = 0.00473396\n",
            "Iteration 285, loss = 0.00470902\n",
            "Iteration 286, loss = 0.00468909\n",
            "Iteration 287, loss = 0.00467066\n",
            "Iteration 288, loss = 0.00466165\n",
            "Iteration 289, loss = 0.00461290\n",
            "Iteration 290, loss = 0.00462931\n",
            "Iteration 291, loss = 0.00462332\n",
            "Iteration 292, loss = 0.00455671\n",
            "Iteration 293, loss = 0.00457927\n",
            "Iteration 294, loss = 0.00451369\n",
            "Iteration 295, loss = 0.00449999\n",
            "Iteration 296, loss = 0.00447554\n",
            "Iteration 297, loss = 0.00445528\n",
            "Iteration 298, loss = 0.00452140\n",
            "Iteration 299, loss = 0.00449118\n",
            "Iteration 300, loss = 0.00442181\n",
            "Iteration 301, loss = 0.00445303\n",
            "Iteration 302, loss = 0.00443806\n",
            "Iteration 303, loss = 0.00443319\n",
            "Iteration 304, loss = 0.00437559\n",
            "Iteration 305, loss = 0.00440870\n",
            "Iteration 306, loss = 0.00434778\n",
            "Iteration 307, loss = 0.00431126\n",
            "Iteration 308, loss = 0.00430928\n",
            "Iteration 309, loss = 0.00427581\n",
            "Iteration 310, loss = 0.00427756\n",
            "Iteration 311, loss = 0.00427304\n",
            "Iteration 312, loss = 0.00426384\n",
            "Iteration 313, loss = 0.00422628\n",
            "Iteration 314, loss = 0.00420066\n",
            "Iteration 315, loss = 0.00427019\n",
            "Iteration 316, loss = 0.00419086\n",
            "Iteration 317, loss = 0.00419690\n",
            "Iteration 318, loss = 0.00416257\n",
            "Iteration 319, loss = 0.00416440\n",
            "Iteration 320, loss = 0.00416966\n",
            "Iteration 321, loss = 0.00414175\n",
            "Iteration 322, loss = 0.00412208\n",
            "Iteration 323, loss = 0.00407990\n",
            "Iteration 324, loss = 0.00405518\n",
            "Iteration 325, loss = 0.00403128\n",
            "Iteration 326, loss = 0.00404249\n",
            "Iteration 327, loss = 0.00399875\n",
            "Iteration 328, loss = 0.00399386\n",
            "Iteration 329, loss = 0.00402780\n",
            "Iteration 330, loss = 0.00400091\n",
            "Iteration 331, loss = 0.00396788\n",
            "Iteration 332, loss = 0.00399715\n",
            "Iteration 333, loss = 0.00394555\n",
            "Iteration 334, loss = 0.00396941\n",
            "Iteration 335, loss = 0.00393710\n",
            "Iteration 336, loss = 0.00397572\n",
            "Iteration 337, loss = 0.00385372\n",
            "Iteration 338, loss = 0.00396587\n",
            "Iteration 339, loss = 0.00387148\n",
            "Iteration 340, loss = 0.00388042\n",
            "Iteration 341, loss = 0.00388566\n",
            "Iteration 342, loss = 0.00385984\n",
            "Iteration 343, loss = 0.00386238\n",
            "Iteration 344, loss = 0.00382927\n",
            "Iteration 345, loss = 0.00380803\n",
            "Iteration 346, loss = 0.00381661\n",
            "Iteration 347, loss = 0.00380419\n",
            "Iteration 348, loss = 0.00378038\n",
            "Iteration 349, loss = 0.00374509\n",
            "Iteration 350, loss = 0.00372517\n",
            "Iteration 351, loss = 0.00373168\n",
            "Iteration 352, loss = 0.00374224\n",
            "Iteration 353, loss = 0.00369326\n",
            "Iteration 354, loss = 0.00372348\n",
            "Iteration 355, loss = 0.00375712\n",
            "Iteration 356, loss = 0.00368898\n",
            "Iteration 357, loss = 0.00367743\n",
            "Iteration 358, loss = 0.00366758\n",
            "Iteration 359, loss = 0.00365088\n",
            "Iteration 360, loss = 0.00368836\n",
            "Iteration 361, loss = 0.00365602\n",
            "Iteration 362, loss = 0.00362505\n",
            "Iteration 363, loss = 0.00362486\n",
            "Iteration 364, loss = 0.00356480\n",
            "Iteration 365, loss = 0.00359288\n",
            "Iteration 366, loss = 0.00363284\n",
            "Iteration 367, loss = 0.00356600\n",
            "Iteration 368, loss = 0.00358883\n",
            "Iteration 369, loss = 0.00354849\n",
            "Iteration 370, loss = 0.00356038\n",
            "Iteration 371, loss = 0.00357143\n",
            "Iteration 372, loss = 0.00350776\n",
            "Iteration 373, loss = 0.00348075\n",
            "Iteration 374, loss = 0.00347811\n",
            "Iteration 375, loss = 0.00346571\n",
            "Iteration 376, loss = 0.00348707\n",
            "Iteration 377, loss = 0.00344693\n",
            "Iteration 378, loss = 0.00347096\n",
            "Iteration 379, loss = 0.00342289\n",
            "Iteration 380, loss = 0.00342945\n",
            "Iteration 381, loss = 0.00337333\n",
            "Iteration 382, loss = 0.00343206\n",
            "Iteration 383, loss = 0.00343639\n",
            "Iteration 384, loss = 0.00339029\n",
            "Iteration 385, loss = 0.00337384\n",
            "Iteration 386, loss = 0.00338089\n",
            "Iteration 387, loss = 0.00335513\n",
            "Iteration 388, loss = 0.00338561\n",
            "Iteration 389, loss = 0.00342000\n",
            "Iteration 390, loss = 0.00333907\n",
            "Iteration 391, loss = 0.00335668\n",
            "Iteration 392, loss = 0.00330550\n",
            "Iteration 393, loss = 0.00330339\n",
            "Iteration 394, loss = 0.00332740\n",
            "Iteration 395, loss = 0.00327492\n",
            "Iteration 396, loss = 0.00331883\n",
            "Iteration 397, loss = 0.00324529\n",
            "Iteration 398, loss = 0.00329314\n",
            "Iteration 399, loss = 0.00324244\n",
            "Iteration 400, loss = 0.00324618\n",
            "Iteration 401, loss = 0.00324587\n",
            "Iteration 402, loss = 0.00323059\n",
            "Iteration 403, loss = 0.00327293\n",
            "Iteration 404, loss = 0.00322800\n",
            "Iteration 405, loss = 0.00325070\n",
            "Iteration 406, loss = 0.00319387\n",
            "Iteration 407, loss = 0.00322651\n",
            "Iteration 408, loss = 0.00318656\n",
            "Iteration 409, loss = 0.00317515\n",
            "Iteration 410, loss = 0.00320850\n",
            "Iteration 411, loss = 0.00317645\n",
            "Iteration 412, loss = 0.00316274\n",
            "Iteration 413, loss = 0.00315269\n",
            "Iteration 414, loss = 0.00314416\n",
            "Iteration 415, loss = 0.00312457\n",
            "Iteration 416, loss = 0.00312417\n",
            "Iteration 417, loss = 0.00312532\n",
            "Iteration 418, loss = 0.00312577\n",
            "Iteration 419, loss = 0.00312253\n",
            "Iteration 420, loss = 0.00311638\n",
            "Iteration 421, loss = 0.00309282\n",
            "Iteration 422, loss = 0.00306576\n",
            "Iteration 423, loss = 0.00305618\n",
            "Iteration 424, loss = 0.00309111\n",
            "Iteration 425, loss = 0.00308422\n",
            "Iteration 426, loss = 0.00304633\n",
            "Iteration 427, loss = 0.00307799\n",
            "Iteration 428, loss = 0.00302185\n",
            "Iteration 429, loss = 0.00302186\n",
            "Iteration 430, loss = 0.00303481\n",
            "Iteration 431, loss = 0.00302606\n",
            "Iteration 432, loss = 0.00299016\n",
            "Iteration 433, loss = 0.00298981\n",
            "Iteration 434, loss = 0.00297354\n",
            "Iteration 435, loss = 0.00299139\n",
            "Iteration 436, loss = 0.00297133\n",
            "Iteration 437, loss = 0.00300404\n",
            "Iteration 438, loss = 0.00295802\n",
            "Iteration 439, loss = 0.00296736\n",
            "Iteration 440, loss = 0.00291530\n",
            "Iteration 441, loss = 0.00298328\n",
            "Iteration 442, loss = 0.00292571\n",
            "Iteration 443, loss = 0.00300233\n",
            "Iteration 444, loss = 0.00291312\n",
            "Iteration 445, loss = 0.00297511\n",
            "Iteration 446, loss = 0.00294667\n",
            "Iteration 447, loss = 0.00294276\n",
            "Iteration 448, loss = 0.00288473\n",
            "Iteration 449, loss = 0.00285166\n",
            "Iteration 450, loss = 0.00290582\n",
            "Iteration 451, loss = 0.00288795\n",
            "Iteration 452, loss = 0.00293587\n",
            "Iteration 453, loss = 0.00289487\n",
            "Iteration 454, loss = 0.00285619\n",
            "Iteration 455, loss = 0.00285278\n",
            "Iteration 456, loss = 0.00285270\n",
            "Iteration 457, loss = 0.00288030\n",
            "Iteration 458, loss = 0.00280499\n",
            "Iteration 459, loss = 0.00282446\n",
            "Iteration 460, loss = 0.00284159\n",
            "Iteration 461, loss = 0.00280711\n",
            "Iteration 462, loss = 0.00280588\n",
            "Iteration 463, loss = 0.00282300\n",
            "Iteration 464, loss = 0.00280240\n",
            "Iteration 465, loss = 0.00279544\n",
            "Iteration 466, loss = 0.00275683\n",
            "Iteration 467, loss = 0.00281095\n",
            "Iteration 468, loss = 0.00284461\n",
            "Iteration 469, loss = 0.00281742\n",
            "Iteration 470, loss = 0.00273885\n",
            "Iteration 471, loss = 0.00277373\n",
            "Iteration 472, loss = 0.00272039\n",
            "Iteration 473, loss = 0.00273633\n",
            "Iteration 474, loss = 0.00272530\n",
            "Iteration 475, loss = 0.00269705\n",
            "Iteration 476, loss = 0.00273527\n",
            "Iteration 477, loss = 0.00270789\n",
            "Iteration 478, loss = 0.00270207\n",
            "Iteration 479, loss = 0.00279358\n",
            "Iteration 480, loss = 0.00278503\n",
            "Iteration 481, loss = 0.00268502\n",
            "Iteration 482, loss = 0.00267545\n",
            "Iteration 483, loss = 0.00268753\n",
            "Iteration 484, loss = 0.00271023\n",
            "Iteration 485, loss = 0.00266200\n",
            "Iteration 486, loss = 0.00264580\n",
            "Iteration 487, loss = 0.00266053\n",
            "Iteration 488, loss = 0.00267585\n",
            "Iteration 489, loss = 0.00270909\n",
            "Iteration 490, loss = 0.00267350\n",
            "Iteration 491, loss = 0.00262176\n",
            "Iteration 492, loss = 0.00261702\n",
            "Iteration 493, loss = 0.00262449\n",
            "Iteration 494, loss = 0.00258929\n",
            "Iteration 495, loss = 0.00262050\n",
            "Iteration 496, loss = 0.00258803\n",
            "Iteration 497, loss = 0.00262777\n",
            "Iteration 498, loss = 0.00256719\n",
            "Iteration 499, loss = 0.00255993\n",
            "Iteration 500, loss = 0.00257754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(80, 40), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "8m9_X9bUdZJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "# train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo_lFxdIc85n",
        "colab_type": "code",
        "outputId": "82e85b2c-47a8-43c3-8569-d2c8e00c27a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1173
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf2 = MLPClassifier(hidden_layer_sizes=(300,100,), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "clf2.fit(train_valid_combined, train_valid_label)\n",
        "# clf2.fit(train_data, train_label)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.54899152\n",
            "Iteration 2, loss = 0.06616612\n",
            "Iteration 3, loss = 0.04279345\n",
            "Iteration 4, loss = 0.03166336\n",
            "Iteration 5, loss = 0.02479369\n",
            "Iteration 6, loss = 0.02170618\n",
            "Iteration 7, loss = 0.01858626\n",
            "Iteration 8, loss = 0.01581559\n",
            "Iteration 9, loss = 0.01390799\n",
            "Iteration 10, loss = 0.01265809\n",
            "Iteration 11, loss = 0.01127673\n",
            "Iteration 12, loss = 0.00996930\n",
            "Iteration 13, loss = 0.00956242\n",
            "Iteration 14, loss = 0.00903562\n",
            "Iteration 15, loss = 0.00835131\n",
            "Iteration 16, loss = 0.00850045\n",
            "Iteration 17, loss = 0.00744452\n",
            "Iteration 18, loss = 0.00710270\n",
            "Iteration 19, loss = 0.00679402\n",
            "Iteration 20, loss = 0.00656596\n",
            "Iteration 21, loss = 0.00575884\n",
            "Iteration 22, loss = 0.00510601\n",
            "Iteration 23, loss = 0.00508025\n",
            "Iteration 24, loss = 0.00522847\n",
            "Iteration 25, loss = 0.00518619\n",
            "Iteration 26, loss = 0.00499809\n",
            "Iteration 27, loss = 0.00462652\n",
            "Iteration 28, loss = 0.00430399\n",
            "Iteration 29, loss = 0.00405920\n",
            "Iteration 30, loss = 0.00449292\n",
            "Iteration 31, loss = 0.00388696\n",
            "Iteration 32, loss = 0.00381280\n",
            "Iteration 33, loss = 0.00355093\n",
            "Iteration 34, loss = 0.00384297\n",
            "Iteration 35, loss = 0.00370512\n",
            "Iteration 36, loss = 0.00326534\n",
            "Iteration 37, loss = 0.00390881\n",
            "Iteration 38, loss = 0.00309825\n",
            "Iteration 39, loss = 0.00313235\n",
            "Iteration 40, loss = 0.00316818\n",
            "Iteration 41, loss = 0.00308970\n",
            "Iteration 42, loss = 0.00278589\n",
            "Iteration 43, loss = 0.00309007\n",
            "Iteration 44, loss = 0.00278668\n",
            "Iteration 45, loss = 0.00268005\n",
            "Iteration 46, loss = 0.00268217\n",
            "Iteration 47, loss = 0.00271514\n",
            "Iteration 48, loss = 0.00259554\n",
            "Iteration 49, loss = 0.00240461\n",
            "Iteration 50, loss = 0.00263695\n",
            "Iteration 51, loss = 0.00245054\n",
            "Iteration 52, loss = 0.00231362\n",
            "Iteration 53, loss = 0.00246221\n",
            "Iteration 54, loss = 0.00262239\n",
            "Iteration 55, loss = 0.00235124\n",
            "Iteration 56, loss = 0.00263333\n",
            "Iteration 57, loss = 0.00231352\n",
            "Iteration 58, loss = 0.00240500\n",
            "Iteration 59, loss = 0.00244188\n",
            "Iteration 60, loss = 0.00229447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(300, 100), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "Wi_0y1C6e9Er",
        "colab_type": "code",
        "outputId": "fa2124c1-2623-4747-bf50-db43bbf71283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7494, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "oy5MNJqFys-H",
        "colab_type": "code",
        "outputId": "5e610cba-8c8c-4515-b66f-b33dd3924861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9996663886572144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "T0aiYNsdyuBR",
        "colab_type": "code",
        "outputId": "abeea735-148b-41e8-e606-2d01e960dcc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9953302201467645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "w7DSKjQcyvL0",
        "colab_type": "code",
        "outputId": "5c47eb2b-7d2b-4c2e-af7a-fdf2c41e2603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736992567181246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "bOJLm3y6dkCs",
        "colab_type": "code",
        "outputId": "aae5866c-88b0-499a-eeb4-15fef873ddeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(train_data,train_label)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9996663886572144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "hnUFUI4rdjwi",
        "colab_type": "code",
        "outputId": "0ae3aa31-c2a7-4603-c5a5-5ddf94b95864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(validation_data,validation_label)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "1u7u5mLsirGr",
        "colab_type": "code",
        "outputId": "002de8c0-6cff-4a44-e96e-9e762281bc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5995, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "BtNN_G2Bdkpr",
        "colab_type": "code",
        "outputId": "af7c60f4-c561-4ca1-d784-3d3510f5fefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(test_data,test_label)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9722698684962836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "63SIF9YCiJ9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9T8IoNGpkVv_",
        "colab_type": "code",
        "outputId": "9378fd98-a7d8-4b57-ac70-c6d9527bd5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3498, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "kxWBf1tjiJm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYHOkr0CiOxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDauxfwliQZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "\n",
        "\n",
        "# saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlClhIpViZoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Divide valid in two parts for validation and validation-test"
      ]
    },
    {
      "metadata": {
        "id": "NhtjHgUwl0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLrSUA61iQW_",
        "colab_type": "code",
        "outputId": "b0bda814-a1cc-40de-90b3-22d18321b4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(train_label_one_hot,axis = 1))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([624., 623., 624., 575., 624., 576., 576., 623., 575., 575.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADtRJREFUeJzt3X+sX3V9x/Hna1T8gQsFuWtYW3dJ\nbDRkCUJuSB2L2ei2CBrLH0owmzSkyf2HOZwmrvrPsmR/aLKIkiwkDdWVjamkamiQOEnBLPsD5q0w\nFKrhjoFtV+hVof4gzjHf++N+Om67lvu9vd/LaT99PpKb7+d8zud8z/t72vu6p597zmmqCklSv35t\n6AIkSSvLoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1btXQBQBcdNFFNTk5OXQZ\nknRG2bt37w+ramKxcadF0E9OTjIzMzN0GZJ0RknyzCjjnLqRpM4Z9JLUOYNekjpn0EtS5wx6Seqc\nQS9JnTPoJalzBr0kdc6gl6TOnRZ3xi7H5LavDV3CWeXpT757kP0O9ec81Ocd0pDfU2fj8X41eEYv\nSZ0z6CWpcwa9JHVupKBPsjrJriTfS7IvyTuSXJjk/iRPttcL2tgkuS3JbJLHklyxsh9BkvRKRj2j\n/yzw9ap6G3AZsA/YBuypqg3AnrYMcA2woX1NA7ePtWJJ0pIsGvRJzgfeCewAqKpfVtULwGZgZxu2\nE7iutTcDd9a8h4DVSS4ee+WSpJGMckZ/CTAHfD7JI0nuSHIesKaqDrUxzwJrWnstsH/B9gdanyRp\nAKME/SrgCuD2qroc+DkvT9MAUFUF1FJ2nGQ6yUySmbm5uaVsKklaglGC/gBwoKoebsu7mA/+545O\nybTXw239QWD9gu3Xtb5jVNX2qpqqqqmJiUX/y0NJ0ilaNOir6llgf5K3tq5NwBPAbmBL69sC3NPa\nu4Eb29U3G4EjC6Z4JEmvslEfgfAh4K4k5wJPATcx/0Pi7iRbgWeA69vY+4BrgVngxTZWkjSQkYK+\nqh4Fpk6watMJxhZw8zLrkiSNiXfGSlLnDHpJ6pxBL0mdy/yU+rCmpqZqZmbmlLb1efSSzmTLeQZ/\nkr1VdaLfnx7DM3pJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPo\nJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHVupKBP8nSS7yR5NMlM\n67swyf1JnmyvF7T+JLktyWySx5JcsZIfQJL0ypZyRv/7VfX2Bf/j+DZgT1VtAPa0ZYBrgA3taxq4\nfVzFSpKWbjlTN5uBna29E7huQf+dNe8hYHWSi5exH0nSMowa9AV8I8neJNOtb01VHWrtZ4E1rb0W\n2L9g2wOtT5I0gFUjjvvdqjqY5DeA+5N8b+HKqqoktZQdtx8Y0wBvfvObl7KpJGkJRjqjr6qD7fUw\n8FXgSuC5o1My7fVwG34QWL9g83Wt7/j33F5VU1U1NTExceqfQJL0ihYN+iTnJfn1o23gj4DvAruB\nLW3YFuCe1t4N3NiuvtkIHFkwxSNJepWNMnWzBvhqkqPj/7Gqvp7kW8DdSbYCzwDXt/H3AdcCs8CL\nwE1jr1qSNLJFg76qngIuO0H/j4BNJ+gv4OaxVCdJWjbvjJWkzhn0ktQ5g16SOmfQS1LnDHpJ6pxB\nL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS\n1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHVu5KBPck6SR5Lc25YvSfJwktkkX0pybut/bVue\nbesnV6Z0SdIolnJGfwuwb8Hyp4Bbq+otwPPA1ta/FXi+9d/axkmSBjJS0CdZB7wbuKMtB7ga2NWG\n7ASua+3NbZm2flMbL0kawKhn9J8BPgb8qi2/CXihql5qyweAta29FtgP0NYfaeMlSQNYNOiTvAc4\nXFV7x7njJNNJZpLMzM3NjfOtJUkLjHJGfxXw3iRPA19kfsrms8DqJKvamHXAwdY+CKwHaOvPB350\n/JtW1faqmqqqqYmJiWV9CEnSyS0a9FX18apaV1WTwA3AA1X1x8CDwPvasC3APa29uy3T1j9QVTXW\nqiVJI1vOdfR/AXwkySzzc/A7Wv8O4E2t/yPAtuWVKElajlWLD3lZVX0T+GZrPwVceYIxvwDeP4ba\nJElj4J2xktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6\nSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJek\nzhn0ktS5RYM+yeuS/GuSf0vyeJK/av2XJHk4yWySLyU5t/W/ti3PtvWTK/sRJEmvZJQz+v8Crq6q\ny4C3A+9KshH4FHBrVb0FeB7Y2sZvBZ5v/be2cZKkgSwa9DXvZ23xNe2rgKuBXa1/J3Bda29uy7T1\nm5JkbBVLkpZkpDn6JOckeRQ4DNwP/DvwQlW91IYcANa29lpgP0BbfwR40wneczrJTJKZubm55X0K\nSdJJjRT0VfU/VfV2YB1wJfC25e64qrZX1VRVTU1MTCz37SRJJ7Gkq26q6gXgQeAdwOokq9qqdcDB\n1j4IrAdo688HfjSWaiVJSzbKVTcTSVa39uuBPwT2MR/472vDtgD3tPbutkxb/0BV1TiLliSNbtXi\nQ7gY2JnkHOZ/MNxdVfcmeQL4YpK/Bh4BdrTxO4C/TzIL/Bi4YQXqliSNaNGgr6rHgMtP0P8U8/P1\nx/f/Anj/WKqTJC2bd8ZKUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ\n6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TO\nGfSS1DmDXpI6t2jQJ1mf5MEkTyR5PMktrf/CJPcnebK9XtD6k+S2JLNJHktyxUp/CEnSyY1yRv8S\n8NGquhTYCNyc5FJgG7CnqjYAe9oywDXAhvY1Ddw+9qolSSNbNOir6lBVfbu1fwrsA9YCm4GdbdhO\n4LrW3gzcWfMeAlYnuXjslUuSRrKkOfokk8DlwMPAmqo61FY9C6xp7bXA/gWbHWh9x7/XdJKZJDNz\nc3NLLFuSNKqRgz7JG4EvAx+uqp8sXFdVBdRSdlxV26tqqqqmJiYmlrKpJGkJRgr6JK9hPuTvqqqv\ntO7njk7JtNfDrf8gsH7B5utanyRpAKNcdRNgB7Cvqj69YNVuYEtrbwHuWdB/Y7v6ZiNwZMEUjyTp\nVbZqhDFXAR8EvpPk0db3CeCTwN1JtgLPANe3dfcB1wKzwIvATWOtWJK0JIsGfVX9C5CTrN50gvEF\n3LzMuiRJY+KdsZLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BL\nUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1\nzqCXpM4tGvRJPpfkcJLvLui7MMn9SZ5srxe0/iS5LclskseSXLGSxUuSFjfKGf3fAe86rm8bsKeq\nNgB72jLANcCG9jUN3D6eMiVJp2rRoK+qfwZ+fFz3ZmBna+8ErlvQf2fNewhYneTicRUrSVq6U52j\nX1NVh1r7WWBNa68F9i8Yd6D1SZIGsuxfxlZVAbXU7ZJMJ5lJMjM3N7fcMiRJJ3GqQf/c0SmZ9nq4\n9R8E1i8Yt671/T9Vtb2qpqpqamJi4hTLkCQt5lSDfjewpbW3APcs6L+xXX2zETiyYIpHkjSAVYsN\nSPIF4PeAi5IcAP4S+CRwd5KtwDPA9W34fcC1wCzwInDTCtQsSVqCRYO+qj5wklWbTjC2gJuXW5Qk\naXy8M1aSOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9J\nnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktS5\nFQn6JO9K8v0ks0m2rcQ+JEmjGXvQJzkH+FvgGuBS4ANJLh33fiRJo1mJM/orgdmqeqqqfgl8Edi8\nAvuRJI1gJYJ+LbB/wfKB1idJGsCqoXacZBqYbos/S/L9U3yri4AfjqeqLng8juXxeJnH4linxfHI\np5a1+W+NMmglgv4gsH7B8rrWd4yq2g5sX+7OksxU1dRy36cXHo9jeTxe5rE41tl0PFZi6uZbwIYk\nlyQ5F7gB2L0C+5EkjWDsZ/RV9VKSPwX+CTgH+FxVPT7u/UiSRrMic/RVdR9w30q89wkse/qnMx6P\nY3k8XuaxONZZczxSVUPXIElaQT4CQZI6d0YHvY9amJdkfZIHkzyR5PEktwxd0+kgyTlJHkly79C1\nDC3J6iS7knwvyb4k7xi6pqEk+fP2ffLdJF9I8rqha1ppZ2zQ+6iFY7wEfLSqLgU2AjefxcdioVuA\nfUMXcZr4LPD1qnobcBln6XFJshb4M2Cqqn6b+QtGbhi2qpV3xgY9Pmrh/1TVoar6dmv/lPlv4rP6\nbuQk64B3A3cMXcvQkpwPvBPYAVBVv6yqF4atalCrgNcnWQW8AfjPgetZcWdy0PuohRNIMglcDjw8\nbCWD+wzwMeBXQxdyGrgEmAM+36ay7khy3tBFDaGqDgJ/A/wAOAQcqapvDFvVyjuTg17HSfJG4MvA\nh6vqJ0PXM5Qk7wEOV9XeoWs5TawCrgBur6rLgZ8DZ+XvtJJcwPy//C8BfhM4L8mfDFvVyjuTg36k\nRy2cLZK8hvmQv6uqvjJ0PQO7CnhvkqeZn9K7Osk/DFvSoA4AB6rq6L/ydjEf/GejPwD+o6rmquq/\nga8AvzNwTSvuTA56H7XQJAnz86/7qurTQ9cztKr6eFWtq6pJ5v9ePFBV3Z+1nUxVPQvsT/LW1rUJ\neGLAkob0A2Bjkje075tNnAW/mB7s6ZXL5aMWjnEV8EHgO0kebX2faHcoSwAfAu5qJ0VPATcNXM8g\nqurhJLuAbzN/tdojnAV3yHpnrCR17kyeupEkjcCgl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z\n6CWpc/8L8ki0lvD1ev0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XfNKrbvrmDl6",
        "colab_type": "code",
        "outputId": "193d5076-f2ab-4706-ef8c-420b23e04bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([156., 156., 156., 144., 156., 144., 144., 155., 144., 144.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWqcQS9JjTPoJalxBr0kNc6gl6TG/S/I+e4LEKaj1wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fX0eodcgiQTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYJZs7GgCy5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle  #train_data, train_label\n",
        "\n",
        "X_train, y_train = shuffle(train_data, train_label_one_hot)\n",
        "validation_data, validation_label_one_hot = validation_data, validation_label_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4WFIdiuCy53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup TensorFlow\n",
        "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "est-t83SCy55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vgad6Ny0OjqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "connection_probability = tf.Variable(.9999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZqUctyopqzZ",
        "colab_type": "code",
        "outputId": "c4a277d3-1a02-4504-f721-9002b2991164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "# print(G_W1.shape)\n",
        "# print(G_W2.shape)\n",
        "# print(G_W3.shape)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5995, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6aNutv83RcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define the network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "stGmwMYz8vws",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "G_w_out_h1 = tf.Variable(xavier_init([10,80]))\n",
        "G_b_out_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "G_w_h2_h1 = tf.Variable(xavier_init([40,80]))\n",
        "G_b_h2_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "\n",
        "G_w_h1_input = tf.Variable(xavier_init([80,16]))\n",
        "G_b_h1_input = tf.Variable(xavier_init([16]))\n",
        "\n",
        "\n",
        "G_w_input_h1_h2 = tf.Variable(xavier_init([16,40]))\n",
        "G_b_h1_input = tf.Variable(xavier_init([40]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sfSdtHU3JfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x, test_mode = False):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    layer_depth = {\n",
        "        'layer_1' : 6,\n",
        "        'layer_2' : 16,\n",
        "        'layer_3' : 120,\n",
        "        'layer_f1' : 84\n",
        "    }\n",
        "\n",
        "\n",
        "    \n",
        "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
        "    x_flat = flatten(x)\n",
        "    fc1 = flatten(x)\n",
        "    fdense = fc1\n",
        "    \n",
        "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fc1_w = G_W1# tf.Variable(tf.truncated_normal(shape = (X_train.shape[1]*X_train.shape[2],300), mean = mu, stddev = sigma))\n",
        "    fc1_b = G_b1# tf.Variable(tf.zeros(300))\n",
        "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "    fc2_w = G_W2# tf.Variable(tf.truncated_normal(shape = (300,100), mean = mu, stddev = sigma))\n",
        "    fc2_b = G_b2# tf.Variable(tf.zeros(100))\n",
        "    fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
        "    # TODO: Activation.\n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    \n",
        "    \n",
        "    #################\n",
        "    ##### Inset probability connection from x to conv2\n",
        "    fc2p_w = tf.Variable(xavier_init([X_train.shape[1],clf.coefs_[1].shape[1]]))\n",
        "    fc2p_b = tf.Variable(xavier_init([clf.coefs_[1].shape[1]]))\n",
        "    fc2_2nd_input = tf.matmul(x_flat,fc2p_w) + fc2p_b\n",
        "    fc2_2nd_input = tf.nn.relu(fc2_2nd_input)\n",
        "    connect2 = tf.logical_and(tf.random.uniform(shape = tf.shape(connection_probability)) < connection_probability, tf.equal(test_mode,False))\n",
        "    fc2 = tf.cond(connect2,lambda: fc2 + fc2_2nd_input, lambda: fc2 )    \n",
        "    ################    \n",
        "    fc3_w = G_W3\n",
        "    fc3_b = G_b3\n",
        "    \n",
        "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
        "#     print(logits.shape)\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGmN34tg3_tv",
        "colab_type": "code",
        "outputId": "6dfe81e6-31c3-414c-dbf8-4501079614ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label_one_hot.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5995, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "-NX_lWUB6zue",
        "colab_type": "code",
        "outputId": "71da8cd3-b4ca-453b-bb63-9b3e033de078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 3, ..., 6, 3, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "M3U_MKYr34Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.name_scope('Input'):\n",
        "\n",
        "  x = tf.placeholder(tf.float32, (None, train_data.shape[1]), name='X')\n",
        "  y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# one_hot_y = tf.one_hot(y, train_label_one_hot.shape[1])\n",
        "is_testing= tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rtTKpeM4P8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-e6BG9DI3Jb3",
        "colab_type": "code",
        "outputId": "dee3c050-f07a-4725-d621-f1eba34833eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rate = 0.001\n",
        "decay_rate = 1.0005**(X_train.shape[0]/BATCH_SIZE);\n",
        "decay_rate = 1.2\n",
        "print(decay_rate)\n",
        "logits = LeNet(x,is_testing)\n",
        "with tf.name_scope('Train'):\n",
        "#   cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
        "#   loss_operation = tf.reduce_mean(cross_entropy, name='loss')\n",
        "  loss_operation = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=LeNet(x, test_mode=False), labels=y))\n",
        "  tf.summary.scalar('loss', loss_operation)\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate = rate,momentum=.9)\n",
        "# optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "# tf.train.natural_exp_decay()\n",
        "training_operation = optimizer.minimize(loss_operation)\n",
        "new_prob = connection_probability.assign(connection_probability/decay_rate)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8eQKHOw7PHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def evaluate(X_data, y_data):\n",
        "correct_pred = tf.equal(tf.argmax(LeNet(x,test_mode=True), 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tb-sFE34OGp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "# accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# saver = tf.train.Saver()\n",
        "\n",
        "# def evaluate(X_data, y_data):\n",
        "#     num_examples = len(X_data)\n",
        "#     total_accuracy = 0\n",
        "#     sess = tf.get_default_session()\n",
        "#     for offset in range(0, num_examples, BATCH_SIZE):\n",
        "#         batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "#         accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, is_testing: True})\n",
        "#         total_accuracy += (accuracy * len(batch_x))\n",
        "#     tot_acc = total_accuracy / num_examples\n",
        "#     with tf.name_scope('Accuracy'):\n",
        "#       tf.summary.scalar('accuracy', tot_acc)\n",
        "#     return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCovfr0E4oJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the mode"
      ]
    },
    {
      "metadata": {
        "id": "NGF2PRgw9nLL",
        "colab_type": "code",
        "outputId": "0c9d1e30-c855-405f-a93f-4c2b44631b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "2056*2"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cl89uCj9hjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4112"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4H5Dgkf69TOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nk5Kihg685LI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQaEeNO285BK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VP4QWKJ4ODG",
        "colab_type": "code",
        "outputId": "cf770c74-7a5f-4f6d-bc2a-0946d7cd985c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8568
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = X_train.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "epoch_track = []\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          epoch_track.append(i)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "            saver.save(sess, './PendigitSGDBased')\n",
        "        \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.83325\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.1345745\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.021734525\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0035102463\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0005669242\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 99.53302\n",
            "9.156141e-05\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.4787675e-05\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.3882915e-06\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 99.59973\n",
            "3.8572227e-07\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 99.59973\n",
            "6.229629e-08\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.0061196e-08\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.6249389e-09\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 99.59973\n",
            "2.6243663e-10\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 99.59973\n",
            "4.238497e-11\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 99.59973\n",
            "6.845409e-12\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.1055714e-12\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.7855591e-13\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 99.59973\n",
            "2.8837775e-14\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 99.59973\n",
            "4.6574606e-15\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 99.59973\n",
            "7.522058e-16\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 201 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.214854e-16\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 211 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.9620562e-17\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 221 ...\n",
            "Validation Accuracy = 99.59973\n",
            "3.1688293e-18\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 231 ...\n",
            "Validation Accuracy = 99.59973\n",
            "5.117835e-19\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 241 ...\n",
            "Validation Accuracy = 99.59973\n",
            "8.2655874e-20\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 251 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.33493824e-20\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 261 ...\n",
            "Validation Accuracy = 99.59973\n",
            "2.1559994e-21\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 271 ...\n",
            "Validation Accuracy = 99.59973\n",
            "3.4820585e-22\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 281 ...\n",
            "Validation Accuracy = 99.59973\n",
            "5.6237174e-23\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 291 ...\n",
            "Validation Accuracy = 99.59973\n",
            "9.0826155e-24\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 301 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.4668928e-24\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 311 ...\n",
            "Validation Accuracy = 99.59973\n",
            "2.369113e-25\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 321 ...\n",
            "Validation Accuracy = 99.59973\n",
            "3.8262487e-26\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 331 ...\n",
            "Validation Accuracy = 99.59973\n",
            "6.1796043e-27\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 341 ...\n",
            "Validation Accuracy = 99.59973\n",
            "9.980403e-28\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 351 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.6118906e-28\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 361 ...\n",
            "Validation Accuracy = 99.59973\n",
            "2.6032927e-29\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 371 ...\n",
            "Validation Accuracy = 99.59973\n",
            "4.2044616e-30\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 381 ...\n",
            "Validation Accuracy = 99.59973\n",
            "6.7904387e-31\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 391 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.0966934e-31\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 401 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.771221e-32\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 411 ...\n",
            "Validation Accuracy = 99.59973\n",
            "2.8606202e-33\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 421 ...\n",
            "Validation Accuracy = 99.59973\n",
            "4.6200606e-34\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 431 ...\n",
            "Validation Accuracy = 99.59973\n",
            "7.461655e-35\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 441 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.2050985e-35\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 451 ...\n",
            "Validation Accuracy = 99.59973\n",
            "1.946301e-36\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 461 ...\n",
            "Validation Accuracy = 99.59973\n",
            "3.1433837e-37\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 471 ...\n",
            "Validation Accuracy = 99.59973\n",
            "5.076739e-38\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 481 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 491 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 501 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 511 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 521 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 531 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 541 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 551 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 561 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 571 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 581 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 591 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 601 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 611 ...\n",
            "Validation Accuracy = 99.59973\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 621 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 631 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 641 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 651 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 661 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 671 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 681 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 691 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 701 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 711 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 721 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 731 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 741 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 751 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 761 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 771 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 781 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 791 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 801 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 811 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 821 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 831 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 841 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 851 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 861 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 871 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 881 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 891 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 901 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 911 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 921 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 931 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 941 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 951 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 961 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 971 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 981 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 991 ...\n",
            "Validation Accuracy = 99.66644\n",
            "0.0\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zydEA48dDeNA",
        "colab_type": "code",
        "outputId": "34cc440b-7437-4e23-c50d-e9688d539010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.argmax(validation_accuracy_track)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "oDevMH2N3JZu",
        "colab_type": "code",
        "outputId": "d7c3b319-6d2b-46fb-e078-0ed53f2dbd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "best_accuracy_valid"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.66644"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "metadata": {
        "id": "uhbuJEM0-sVm",
        "colab_type": "code",
        "outputId": "ffb1ac1e-5609-494d-92ad-0bbd170f26f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './PendigitSGDBased')\n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./PendigitSGDBased\n",
            "Validation Accuracy = 99.666443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D5okZYa4CSqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import spline\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ou2-UqDCXZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, 4861, print_every)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6G17qZY_Pxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03d13ba0-cbb8-4065-bc5d-112bb00480d5"
      },
      "cell_type": "code",
      "source": [
        "len(validation_accuracy_track)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "metadata": {
        "id": "sNS-YE0XCg4s",
        "colab_type": "code",
        "outputId": "9a23f2c3-d60b-4c72-e247-506d153cc93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "# plt.plot( savgol_filter(np.asarray(validation_accuracy_track),51,1))\n",
        "plt.plot( (validation_accuracy_track))\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff2ae410dd8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA4KZmQHw/wETfMQlwBuPKQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BBlFJwfn-45J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec8a88ea-0faf-416c-8d84-05921be6a98b"
      },
      "cell_type": "code",
      "source": [
        "len(steps_plot)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "487"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "MCKAiyEFCg2F",
        "colab_type": "code",
        "outputId": "e4777ef7-0a56-455a-9653-14f750ddca2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "steps_plot = epoch_track# [step for step in range(0, 2291, print_every)]\n",
        "# plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "# plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "plt.plot(steps_plot, np.asarray(train_accuracy_track))  \n",
        "plt.plot(steps_plot, validation_accuracy_track)\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEYCAYAAAAwH9PuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncWEzM7NCcWEzM7NCcWEzM7NC+f8SZAXhBuAZngAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u7hYf8U_CXWh",
        "colab_type": "code",
        "outputId": "f1751212-9d8e-4c09-8da0-5c8a07adf0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "valid_accuracy_filtered = validation_accuracy_track\n",
        "print(max(valid_accuracy_filtered))\n",
        "valid_accuracy_filtered_np = np.asarray(valid_accuracy_filtered)\n",
        "print(np.argmax(valid_accuracy_filtered))\n",
        "print(steps_plot[np.argmax(valid_accuracy_filtered)])"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.66644\n",
            "62\n",
            "620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKucNn6bEZzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lrwM4UUCXTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('PendigitFullDatasetMoment_ProbabilityBasedValid97p4.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "                                       'train_accuracy_track':train_accuracy_track,\n",
        "                                       'connection_probability_track':connection_probability_track,\n",
        "                                       'epochTrack':epoch_track, 'TestAcc':test_accuracy,\n",
        "                                                         'BestValidation':best_accuracy_valid})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFCL8d7-CIxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now  retrain til 1900 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "qoqMxUbZCIRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 100\n",
        "aside_valid_test = train_valid_combined[-aside_examples:]\n",
        "aside_valid_test_label = validation_test_label_one_hot[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = validation_test_label_one_hot[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5n7bYp9-FCn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 625"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puoBTQ3fCIOT",
        "colab_type": "code",
        "outputId": "1c50e553-ef26-4777-f81b-8e75aeeca87c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5423
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(combined_train_valid, combined_train_valid_label)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: aside_valid_test,y:aside_valid_test_label, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "#             saver.save(sess, './PendigitSGDBased')\n",
        "    saver.save(sess, './PendigitAdamBasedAllPass')   \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.83325\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.1345745\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.021734525\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0035102463\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0005669242\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 100.00000\n",
            "9.156141e-05\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.4787675e-05\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.3882915e-06\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.8572227e-07\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.229629e-08\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.0061196e-08\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.6249389e-09\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.6243663e-10\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.238497e-11\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.845409e-12\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.1055714e-12\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.7855591e-13\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.8837775e-14\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.6574606e-15\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 100.00000\n",
            "7.522058e-16\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 201 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.214854e-16\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 211 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.9620562e-17\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 221 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.1688293e-18\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 231 ...\n",
            "Validation Accuracy = 100.00000\n",
            "5.117835e-19\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 241 ...\n",
            "Validation Accuracy = 100.00000\n",
            "8.2655874e-20\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 251 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.33493824e-20\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 261 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.1559994e-21\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 271 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.4820585e-22\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 281 ...\n",
            "Validation Accuracy = 100.00000\n",
            "5.6237174e-23\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 291 ...\n",
            "Validation Accuracy = 100.00000\n",
            "9.0826155e-24\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 301 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.4668928e-24\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 311 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.369113e-25\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 321 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.8262487e-26\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 331 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.1796043e-27\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 341 ...\n",
            "Validation Accuracy = 100.00000\n",
            "9.980403e-28\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 351 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.6118906e-28\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 361 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.6032927e-29\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 371 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.2044616e-30\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 381 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.7904387e-31\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 391 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.0966934e-31\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 401 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.771221e-32\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 411 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.8606202e-33\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 421 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.6200606e-34\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 431 ...\n",
            "Validation Accuracy = 100.00000\n",
            "7.461655e-35\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 441 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.2050985e-35\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 451 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.946301e-36\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 461 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.1433837e-37\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 471 ...\n",
            "Validation Accuracy = 100.00000\n",
            "5.076739e-38\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 481 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 491 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 501 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 511 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 521 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 531 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 541 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 551 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 561 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 571 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 581 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 591 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 601 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 611 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.90533\n",
            "EPOCH 621 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ImtSTCNJF9E_",
        "colab_type": "code",
        "outputId": "8ce9fb46-90b8-44bb-f861-fe0b68ae635f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './PendigitAdamBasedAllPass')\n",
        "    saver.save(sess, './PendigitMomentBasedAllPass')  \n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./PendigitAdamBasedAllPass\n",
            "Validation Accuracy = 99.666443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FVMAeXpCIL8",
        "colab_type": "code",
        "outputId": "e825e395-48b0-4bd6-f675-c1c65625036a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Without all pass\n",
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './PendigitMomentBasedAllPass')\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "    print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./PendigitMomentBasedAllPass\n",
            "Test Accuracy = 97.398514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VGUWHQR3CIJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HXCpiAuCIGT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGYzlt7KCIDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hr57zkSqCIAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q08ZXCdr-sTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Without all pass\n",
        "# with tf.Session() as sess:\n",
        "# #     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "#     saver.restore(sess, './PendigitSGDBased')\n",
        "#     test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "#     print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9z1P1DG-sQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW6V7O1e-sNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochTrack = epoch_track\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r7lCgbXR3JXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFfzfjOB3JTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}