{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HARFullDatasetProbBasedTestAcc95p15 .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/HARFullDatasetProbBasedTestAcc95p15_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qWL1XvRKt0EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yuoNYnTshaZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4FSwFTChgdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_label = np.concatenate((train_label, validation_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VloppGYRyrEX",
        "colab_type": "code",
        "outputId": "8d2a6ad1-dbb6-4abb-a794-f2a7c490c979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4794
        }
      },
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(374, ), max_iter=500, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.52720526\n",
            "Iteration 2, loss = 0.15379691\n",
            "Iteration 3, loss = 0.11074311\n",
            "Iteration 4, loss = 0.08947817\n",
            "Iteration 5, loss = 0.07609836\n",
            "Iteration 6, loss = 0.06703077\n",
            "Iteration 7, loss = 0.06092138\n",
            "Iteration 8, loss = 0.05482786\n",
            "Iteration 9, loss = 0.05074759\n",
            "Iteration 10, loss = 0.04820633\n",
            "Iteration 11, loss = 0.04462717\n",
            "Iteration 12, loss = 0.04117033\n",
            "Iteration 13, loss = 0.03949230\n",
            "Iteration 14, loss = 0.03816921\n",
            "Iteration 15, loss = 0.03526902\n",
            "Iteration 16, loss = 0.03338227\n",
            "Iteration 17, loss = 0.03256753\n",
            "Iteration 18, loss = 0.03047405\n",
            "Iteration 19, loss = 0.02924046\n",
            "Iteration 20, loss = 0.02849386\n",
            "Iteration 21, loss = 0.02730756\n",
            "Iteration 22, loss = 0.02559763\n",
            "Iteration 23, loss = 0.02459467\n",
            "Iteration 24, loss = 0.02391017\n",
            "Iteration 25, loss = 0.02354070\n",
            "Iteration 26, loss = 0.02216005\n",
            "Iteration 27, loss = 0.02149137\n",
            "Iteration 28, loss = 0.02112266\n",
            "Iteration 29, loss = 0.02016297\n",
            "Iteration 30, loss = 0.01943691\n",
            "Iteration 31, loss = 0.01848743\n",
            "Iteration 32, loss = 0.01833975\n",
            "Iteration 33, loss = 0.01734477\n",
            "Iteration 34, loss = 0.01683132\n",
            "Iteration 35, loss = 0.01656447\n",
            "Iteration 36, loss = 0.01629328\n",
            "Iteration 37, loss = 0.01554519\n",
            "Iteration 38, loss = 0.01475836\n",
            "Iteration 39, loss = 0.01461295\n",
            "Iteration 40, loss = 0.01419653\n",
            "Iteration 41, loss = 0.01353610\n",
            "Iteration 42, loss = 0.01370225\n",
            "Iteration 43, loss = 0.01320427\n",
            "Iteration 44, loss = 0.01308621\n",
            "Iteration 45, loss = 0.01274597\n",
            "Iteration 46, loss = 0.01203195\n",
            "Iteration 47, loss = 0.01174829\n",
            "Iteration 48, loss = 0.01125364\n",
            "Iteration 49, loss = 0.01074622\n",
            "Iteration 50, loss = 0.01062621\n",
            "Iteration 51, loss = 0.01044433\n",
            "Iteration 52, loss = 0.01012032\n",
            "Iteration 53, loss = 0.00998317\n",
            "Iteration 54, loss = 0.00957855\n",
            "Iteration 55, loss = 0.00919682\n",
            "Iteration 56, loss = 0.00903952\n",
            "Iteration 57, loss = 0.00882828\n",
            "Iteration 58, loss = 0.00842398\n",
            "Iteration 59, loss = 0.00830306\n",
            "Iteration 60, loss = 0.00800349\n",
            "Iteration 61, loss = 0.00785930\n",
            "Iteration 62, loss = 0.00785182\n",
            "Iteration 63, loss = 0.00758100\n",
            "Iteration 64, loss = 0.00740849\n",
            "Iteration 65, loss = 0.00748268\n",
            "Iteration 66, loss = 0.00716864\n",
            "Iteration 67, loss = 0.00700393\n",
            "Iteration 68, loss = 0.00692167\n",
            "Iteration 69, loss = 0.00658257\n",
            "Iteration 70, loss = 0.00648939\n",
            "Iteration 71, loss = 0.00624277\n",
            "Iteration 72, loss = 0.00612378\n",
            "Iteration 73, loss = 0.00603359\n",
            "Iteration 74, loss = 0.00594559\n",
            "Iteration 75, loss = 0.00585920\n",
            "Iteration 76, loss = 0.00570434\n",
            "Iteration 77, loss = 0.00561283\n",
            "Iteration 78, loss = 0.00554028\n",
            "Iteration 79, loss = 0.00541069\n",
            "Iteration 80, loss = 0.00524976\n",
            "Iteration 81, loss = 0.00527214\n",
            "Iteration 82, loss = 0.00516337\n",
            "Iteration 83, loss = 0.00497239\n",
            "Iteration 84, loss = 0.00485493\n",
            "Iteration 85, loss = 0.00490756\n",
            "Iteration 86, loss = 0.00467278\n",
            "Iteration 87, loss = 0.00460029\n",
            "Iteration 88, loss = 0.00454493\n",
            "Iteration 89, loss = 0.00442256\n",
            "Iteration 90, loss = 0.00432667\n",
            "Iteration 91, loss = 0.00433931\n",
            "Iteration 92, loss = 0.00421722\n",
            "Iteration 93, loss = 0.00419655\n",
            "Iteration 94, loss = 0.00406313\n",
            "Iteration 95, loss = 0.00407564\n",
            "Iteration 96, loss = 0.00401053\n",
            "Iteration 97, loss = 0.00392400\n",
            "Iteration 98, loss = 0.00386804\n",
            "Iteration 99, loss = 0.00379496\n",
            "Iteration 100, loss = 0.00373337\n",
            "Iteration 101, loss = 0.00365417\n",
            "Iteration 102, loss = 0.00360392\n",
            "Iteration 103, loss = 0.00356796\n",
            "Iteration 104, loss = 0.00349700\n",
            "Iteration 105, loss = 0.00347712\n",
            "Iteration 106, loss = 0.00346129\n",
            "Iteration 107, loss = 0.00341398\n",
            "Iteration 108, loss = 0.00332927\n",
            "Iteration 109, loss = 0.00332159\n",
            "Iteration 110, loss = 0.00330862\n",
            "Iteration 111, loss = 0.00318258\n",
            "Iteration 112, loss = 0.00317291\n",
            "Iteration 113, loss = 0.00313192\n",
            "Iteration 114, loss = 0.00312075\n",
            "Iteration 115, loss = 0.00306214\n",
            "Iteration 116, loss = 0.00303800\n",
            "Iteration 117, loss = 0.00300883\n",
            "Iteration 118, loss = 0.00297746\n",
            "Iteration 119, loss = 0.00287678\n",
            "Iteration 120, loss = 0.00285229\n",
            "Iteration 121, loss = 0.00280417\n",
            "Iteration 122, loss = 0.00281288\n",
            "Iteration 123, loss = 0.00279715\n",
            "Iteration 124, loss = 0.00272476\n",
            "Iteration 125, loss = 0.00271742\n",
            "Iteration 126, loss = 0.00268767\n",
            "Iteration 127, loss = 0.00261441\n",
            "Iteration 128, loss = 0.00265474\n",
            "Iteration 129, loss = 0.00261967\n",
            "Iteration 130, loss = 0.00256700\n",
            "Iteration 131, loss = 0.00249650\n",
            "Iteration 132, loss = 0.00251667\n",
            "Iteration 133, loss = 0.00249242\n",
            "Iteration 134, loss = 0.00244083\n",
            "Iteration 135, loss = 0.00249760\n",
            "Iteration 136, loss = 0.00237943\n",
            "Iteration 137, loss = 0.00237679\n",
            "Iteration 138, loss = 0.00234850\n",
            "Iteration 139, loss = 0.00232762\n",
            "Iteration 140, loss = 0.00229602\n",
            "Iteration 141, loss = 0.00230112\n",
            "Iteration 142, loss = 0.00225280\n",
            "Iteration 143, loss = 0.00222079\n",
            "Iteration 144, loss = 0.00221551\n",
            "Iteration 145, loss = 0.00218910\n",
            "Iteration 146, loss = 0.00220109\n",
            "Iteration 147, loss = 0.00214222\n",
            "Iteration 148, loss = 0.00212741\n",
            "Iteration 149, loss = 0.00211983\n",
            "Iteration 150, loss = 0.00208963\n",
            "Iteration 151, loss = 0.00208430\n",
            "Iteration 152, loss = 0.00205445\n",
            "Iteration 153, loss = 0.00202581\n",
            "Iteration 154, loss = 0.00201077\n",
            "Iteration 155, loss = 0.00198594\n",
            "Iteration 156, loss = 0.00201227\n",
            "Iteration 157, loss = 0.00197451\n",
            "Iteration 158, loss = 0.00197394\n",
            "Iteration 159, loss = 0.00191715\n",
            "Iteration 160, loss = 0.00191562\n",
            "Iteration 161, loss = 0.00192284\n",
            "Iteration 162, loss = 0.00191362\n",
            "Iteration 163, loss = 0.00187164\n",
            "Iteration 164, loss = 0.00184135\n",
            "Iteration 165, loss = 0.00183465\n",
            "Iteration 166, loss = 0.00182088\n",
            "Iteration 167, loss = 0.00180899\n",
            "Iteration 168, loss = 0.00180355\n",
            "Iteration 169, loss = 0.00177716\n",
            "Iteration 170, loss = 0.00176685\n",
            "Iteration 171, loss = 0.00175030\n",
            "Iteration 172, loss = 0.00175049\n",
            "Iteration 173, loss = 0.00171984\n",
            "Iteration 174, loss = 0.00171184\n",
            "Iteration 175, loss = 0.00170115\n",
            "Iteration 176, loss = 0.00169457\n",
            "Iteration 177, loss = 0.00166902\n",
            "Iteration 178, loss = 0.00166703\n",
            "Iteration 179, loss = 0.00163954\n",
            "Iteration 180, loss = 0.00163217\n",
            "Iteration 181, loss = 0.00163243\n",
            "Iteration 182, loss = 0.00162558\n",
            "Iteration 183, loss = 0.00159687\n",
            "Iteration 184, loss = 0.00164311\n",
            "Iteration 185, loss = 0.00157965\n",
            "Iteration 186, loss = 0.00155642\n",
            "Iteration 187, loss = 0.00156807\n",
            "Iteration 188, loss = 0.00153528\n",
            "Iteration 189, loss = 0.00155032\n",
            "Iteration 190, loss = 0.00152579\n",
            "Iteration 191, loss = 0.00152152\n",
            "Iteration 192, loss = 0.00152949\n",
            "Iteration 193, loss = 0.00149234\n",
            "Iteration 194, loss = 0.00149090\n",
            "Iteration 195, loss = 0.00149407\n",
            "Iteration 196, loss = 0.00146058\n",
            "Iteration 197, loss = 0.00146076\n",
            "Iteration 198, loss = 0.00144024\n",
            "Iteration 199, loss = 0.00146338\n",
            "Iteration 200, loss = 0.00143834\n",
            "Iteration 201, loss = 0.00144446\n",
            "Iteration 202, loss = 0.00140414\n",
            "Iteration 203, loss = 0.00139807\n",
            "Iteration 204, loss = 0.00139003\n",
            "Iteration 205, loss = 0.00138725\n",
            "Iteration 206, loss = 0.00138091\n",
            "Iteration 207, loss = 0.00136549\n",
            "Iteration 208, loss = 0.00136043\n",
            "Iteration 209, loss = 0.00136405\n",
            "Iteration 210, loss = 0.00134716\n",
            "Iteration 211, loss = 0.00133971\n",
            "Iteration 212, loss = 0.00133517\n",
            "Iteration 213, loss = 0.00133948\n",
            "Iteration 214, loss = 0.00131070\n",
            "Iteration 215, loss = 0.00130429\n",
            "Iteration 216, loss = 0.00129578\n",
            "Iteration 217, loss = 0.00129759\n",
            "Iteration 218, loss = 0.00128454\n",
            "Iteration 219, loss = 0.00127488\n",
            "Iteration 220, loss = 0.00127705\n",
            "Iteration 221, loss = 0.00125503\n",
            "Iteration 222, loss = 0.00125035\n",
            "Iteration 223, loss = 0.00124939\n",
            "Iteration 224, loss = 0.00123950\n",
            "Iteration 225, loss = 0.00124269\n",
            "Iteration 226, loss = 0.00122294\n",
            "Iteration 227, loss = 0.00121358\n",
            "Iteration 228, loss = 0.00121795\n",
            "Iteration 229, loss = 0.00121993\n",
            "Iteration 230, loss = 0.00120013\n",
            "Iteration 231, loss = 0.00120214\n",
            "Iteration 232, loss = 0.00118955\n",
            "Iteration 233, loss = 0.00119117\n",
            "Iteration 234, loss = 0.00117541\n",
            "Iteration 235, loss = 0.00118291\n",
            "Iteration 236, loss = 0.00115907\n",
            "Iteration 237, loss = 0.00115156\n",
            "Iteration 238, loss = 0.00115538\n",
            "Iteration 239, loss = 0.00113846\n",
            "Iteration 240, loss = 0.00114294\n",
            "Iteration 241, loss = 0.00113441\n",
            "Iteration 242, loss = 0.00113424\n",
            "Iteration 243, loss = 0.00113737\n",
            "Iteration 244, loss = 0.00112972\n",
            "Iteration 245, loss = 0.00111104\n",
            "Iteration 246, loss = 0.00111092\n",
            "Iteration 247, loss = 0.00110113\n",
            "Iteration 248, loss = 0.00109721\n",
            "Iteration 249, loss = 0.00108678\n",
            "Iteration 250, loss = 0.00108536\n",
            "Iteration 251, loss = 0.00108186\n",
            "Iteration 252, loss = 0.00108212\n",
            "Iteration 253, loss = 0.00107221\n",
            "Iteration 254, loss = 0.00105986\n",
            "Iteration 255, loss = 0.00105490\n",
            "Iteration 256, loss = 0.00106770\n",
            "Iteration 257, loss = 0.00104865\n",
            "Iteration 258, loss = 0.00105722\n",
            "Iteration 259, loss = 0.00104949\n",
            "Iteration 260, loss = 0.00104859\n",
            "Iteration 261, loss = 0.00103729\n",
            "Iteration 262, loss = 0.00102181\n",
            "Iteration 263, loss = 0.00102176\n",
            "Iteration 264, loss = 0.00101276\n",
            "Iteration 265, loss = 0.00100853\n",
            "Iteration 266, loss = 0.00100495\n",
            "Iteration 267, loss = 0.00100003\n",
            "Iteration 268, loss = 0.00100249\n",
            "Iteration 269, loss = 0.00099136\n",
            "Iteration 270, loss = 0.00099639\n",
            "Iteration 271, loss = 0.00098411\n",
            "Iteration 272, loss = 0.00097758\n",
            "Iteration 273, loss = 0.00097843\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(374,), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "8m9_X9bUdZJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "# train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo_lFxdIc85n",
        "colab_type": "code",
        "outputId": "6035930c-0545-4933-8d5c-a870f4b0eed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4726
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf2 = MLPClassifier(hidden_layer_sizes=(374, ), max_iter=500, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "clf2.fit(train_valid_combined, train_valid_label)\n",
        "# clf2.fit(train_data, train_label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.44882975\n",
            "Iteration 2, loss = 0.13607917\n",
            "Iteration 3, loss = 0.09662677\n",
            "Iteration 4, loss = 0.08014645\n",
            "Iteration 5, loss = 0.06919744\n",
            "Iteration 6, loss = 0.06095543\n",
            "Iteration 7, loss = 0.05541882\n",
            "Iteration 8, loss = 0.05067991\n",
            "Iteration 9, loss = 0.04701436\n",
            "Iteration 10, loss = 0.04438256\n",
            "Iteration 11, loss = 0.04070436\n",
            "Iteration 12, loss = 0.03874675\n",
            "Iteration 13, loss = 0.03677753\n",
            "Iteration 14, loss = 0.03475249\n",
            "Iteration 15, loss = 0.03333789\n",
            "Iteration 16, loss = 0.03146704\n",
            "Iteration 17, loss = 0.03011758\n",
            "Iteration 18, loss = 0.02941739\n",
            "Iteration 19, loss = 0.02901677\n",
            "Iteration 20, loss = 0.02689154\n",
            "Iteration 21, loss = 0.02619802\n",
            "Iteration 22, loss = 0.02489499\n",
            "Iteration 23, loss = 0.02393288\n",
            "Iteration 24, loss = 0.02263525\n",
            "Iteration 25, loss = 0.02174490\n",
            "Iteration 26, loss = 0.02141342\n",
            "Iteration 27, loss = 0.02010452\n",
            "Iteration 28, loss = 0.02013348\n",
            "Iteration 29, loss = 0.01888217\n",
            "Iteration 30, loss = 0.01785186\n",
            "Iteration 31, loss = 0.01820409\n",
            "Iteration 32, loss = 0.01721222\n",
            "Iteration 33, loss = 0.01650539\n",
            "Iteration 34, loss = 0.01628898\n",
            "Iteration 35, loss = 0.01541210\n",
            "Iteration 36, loss = 0.01505605\n",
            "Iteration 37, loss = 0.01445838\n",
            "Iteration 38, loss = 0.01441996\n",
            "Iteration 39, loss = 0.01377937\n",
            "Iteration 40, loss = 0.01312874\n",
            "Iteration 41, loss = 0.01286070\n",
            "Iteration 42, loss = 0.01253221\n",
            "Iteration 43, loss = 0.01239311\n",
            "Iteration 44, loss = 0.01144773\n",
            "Iteration 45, loss = 0.01161134\n",
            "Iteration 46, loss = 0.01085590\n",
            "Iteration 47, loss = 0.01061685\n",
            "Iteration 48, loss = 0.01039154\n",
            "Iteration 49, loss = 0.00998364\n",
            "Iteration 50, loss = 0.00968856\n",
            "Iteration 51, loss = 0.00937859\n",
            "Iteration 52, loss = 0.00926044\n",
            "Iteration 53, loss = 0.00889519\n",
            "Iteration 54, loss = 0.00854484\n",
            "Iteration 55, loss = 0.00847438\n",
            "Iteration 56, loss = 0.00834194\n",
            "Iteration 57, loss = 0.00806105\n",
            "Iteration 58, loss = 0.00781842\n",
            "Iteration 59, loss = 0.00783464\n",
            "Iteration 60, loss = 0.00741121\n",
            "Iteration 61, loss = 0.00731605\n",
            "Iteration 62, loss = 0.00733372\n",
            "Iteration 63, loss = 0.00681892\n",
            "Iteration 64, loss = 0.00683511\n",
            "Iteration 65, loss = 0.00652259\n",
            "Iteration 66, loss = 0.00658821\n",
            "Iteration 67, loss = 0.00635351\n",
            "Iteration 68, loss = 0.00633271\n",
            "Iteration 69, loss = 0.00599124\n",
            "Iteration 70, loss = 0.00587947\n",
            "Iteration 71, loss = 0.00576366\n",
            "Iteration 72, loss = 0.00555610\n",
            "Iteration 73, loss = 0.00544236\n",
            "Iteration 74, loss = 0.00541646\n",
            "Iteration 75, loss = 0.00523634\n",
            "Iteration 76, loss = 0.00506038\n",
            "Iteration 77, loss = 0.00509779\n",
            "Iteration 78, loss = 0.00500708\n",
            "Iteration 79, loss = 0.00489139\n",
            "Iteration 80, loss = 0.00469857\n",
            "Iteration 81, loss = 0.00481912\n",
            "Iteration 82, loss = 0.00457415\n",
            "Iteration 83, loss = 0.00454171\n",
            "Iteration 84, loss = 0.00447233\n",
            "Iteration 85, loss = 0.00433273\n",
            "Iteration 86, loss = 0.00415945\n",
            "Iteration 87, loss = 0.00422140\n",
            "Iteration 88, loss = 0.00424781\n",
            "Iteration 89, loss = 0.00403332\n",
            "Iteration 90, loss = 0.00393742\n",
            "Iteration 91, loss = 0.00390622\n",
            "Iteration 92, loss = 0.00386170\n",
            "Iteration 93, loss = 0.00394609\n",
            "Iteration 94, loss = 0.00372787\n",
            "Iteration 95, loss = 0.00367967\n",
            "Iteration 96, loss = 0.00354163\n",
            "Iteration 97, loss = 0.00347813\n",
            "Iteration 98, loss = 0.00345223\n",
            "Iteration 99, loss = 0.00342826\n",
            "Iteration 100, loss = 0.00346441\n",
            "Iteration 101, loss = 0.00326340\n",
            "Iteration 102, loss = 0.00332718\n",
            "Iteration 103, loss = 0.00323854\n",
            "Iteration 104, loss = 0.00316799\n",
            "Iteration 105, loss = 0.00309726\n",
            "Iteration 106, loss = 0.00310772\n",
            "Iteration 107, loss = 0.00303951\n",
            "Iteration 108, loss = 0.00297889\n",
            "Iteration 109, loss = 0.00295424\n",
            "Iteration 110, loss = 0.00292031\n",
            "Iteration 111, loss = 0.00288638\n",
            "Iteration 112, loss = 0.00281013\n",
            "Iteration 113, loss = 0.00279513\n",
            "Iteration 114, loss = 0.00275034\n",
            "Iteration 115, loss = 0.00273546\n",
            "Iteration 116, loss = 0.00266933\n",
            "Iteration 117, loss = 0.00262570\n",
            "Iteration 118, loss = 0.00266562\n",
            "Iteration 119, loss = 0.00259828\n",
            "Iteration 120, loss = 0.00258702\n",
            "Iteration 121, loss = 0.00251899\n",
            "Iteration 122, loss = 0.00251984\n",
            "Iteration 123, loss = 0.00246867\n",
            "Iteration 124, loss = 0.00244869\n",
            "Iteration 125, loss = 0.00242041\n",
            "Iteration 126, loss = 0.00237897\n",
            "Iteration 127, loss = 0.00238714\n",
            "Iteration 128, loss = 0.00232946\n",
            "Iteration 129, loss = 0.00232926\n",
            "Iteration 130, loss = 0.00227304\n",
            "Iteration 131, loss = 0.00226318\n",
            "Iteration 132, loss = 0.00222355\n",
            "Iteration 133, loss = 0.00221099\n",
            "Iteration 134, loss = 0.00217522\n",
            "Iteration 135, loss = 0.00217525\n",
            "Iteration 136, loss = 0.00212115\n",
            "Iteration 137, loss = 0.00211664\n",
            "Iteration 138, loss = 0.00211601\n",
            "Iteration 139, loss = 0.00206141\n",
            "Iteration 140, loss = 0.00201891\n",
            "Iteration 141, loss = 0.00204408\n",
            "Iteration 142, loss = 0.00200838\n",
            "Iteration 143, loss = 0.00202695\n",
            "Iteration 144, loss = 0.00198529\n",
            "Iteration 145, loss = 0.00195953\n",
            "Iteration 146, loss = 0.00191794\n",
            "Iteration 147, loss = 0.00193518\n",
            "Iteration 148, loss = 0.00188395\n",
            "Iteration 149, loss = 0.00187842\n",
            "Iteration 150, loss = 0.00189756\n",
            "Iteration 151, loss = 0.00185541\n",
            "Iteration 152, loss = 0.00183208\n",
            "Iteration 153, loss = 0.00181260\n",
            "Iteration 154, loss = 0.00179042\n",
            "Iteration 155, loss = 0.00179149\n",
            "Iteration 156, loss = 0.00177992\n",
            "Iteration 157, loss = 0.00175603\n",
            "Iteration 158, loss = 0.00173314\n",
            "Iteration 159, loss = 0.00171528\n",
            "Iteration 160, loss = 0.00172426\n",
            "Iteration 161, loss = 0.00169803\n",
            "Iteration 162, loss = 0.00169339\n",
            "Iteration 163, loss = 0.00166459\n",
            "Iteration 164, loss = 0.00165770\n",
            "Iteration 165, loss = 0.00163939\n",
            "Iteration 166, loss = 0.00162479\n",
            "Iteration 167, loss = 0.00161945\n",
            "Iteration 168, loss = 0.00159240\n",
            "Iteration 169, loss = 0.00158392\n",
            "Iteration 170, loss = 0.00156689\n",
            "Iteration 171, loss = 0.00155327\n",
            "Iteration 172, loss = 0.00154970\n",
            "Iteration 173, loss = 0.00153183\n",
            "Iteration 174, loss = 0.00151458\n",
            "Iteration 175, loss = 0.00152655\n",
            "Iteration 176, loss = 0.00149384\n",
            "Iteration 177, loss = 0.00150214\n",
            "Iteration 178, loss = 0.00148268\n",
            "Iteration 179, loss = 0.00147885\n",
            "Iteration 180, loss = 0.00148551\n",
            "Iteration 181, loss = 0.00142935\n",
            "Iteration 182, loss = 0.00143810\n",
            "Iteration 183, loss = 0.00141899\n",
            "Iteration 184, loss = 0.00142194\n",
            "Iteration 185, loss = 0.00142556\n",
            "Iteration 186, loss = 0.00139208\n",
            "Iteration 187, loss = 0.00138653\n",
            "Iteration 188, loss = 0.00137839\n",
            "Iteration 189, loss = 0.00136419\n",
            "Iteration 190, loss = 0.00137832\n",
            "Iteration 191, loss = 0.00134710\n",
            "Iteration 192, loss = 0.00134139\n",
            "Iteration 193, loss = 0.00133436\n",
            "Iteration 194, loss = 0.00132959\n",
            "Iteration 195, loss = 0.00132038\n",
            "Iteration 196, loss = 0.00131026\n",
            "Iteration 197, loss = 0.00130248\n",
            "Iteration 198, loss = 0.00129554\n",
            "Iteration 199, loss = 0.00128781\n",
            "Iteration 200, loss = 0.00127092\n",
            "Iteration 201, loss = 0.00127011\n",
            "Iteration 202, loss = 0.00126080\n",
            "Iteration 203, loss = 0.00125355\n",
            "Iteration 204, loss = 0.00125213\n",
            "Iteration 205, loss = 0.00126525\n",
            "Iteration 206, loss = 0.00122575\n",
            "Iteration 207, loss = 0.00121443\n",
            "Iteration 208, loss = 0.00121296\n",
            "Iteration 209, loss = 0.00119943\n",
            "Iteration 210, loss = 0.00122739\n",
            "Iteration 211, loss = 0.00118308\n",
            "Iteration 212, loss = 0.00117539\n",
            "Iteration 213, loss = 0.00118957\n",
            "Iteration 214, loss = 0.00117404\n",
            "Iteration 215, loss = 0.00116651\n",
            "Iteration 216, loss = 0.00116409\n",
            "Iteration 217, loss = 0.00116838\n",
            "Iteration 218, loss = 0.00113910\n",
            "Iteration 219, loss = 0.00113298\n",
            "Iteration 220, loss = 0.00112737\n",
            "Iteration 221, loss = 0.00111339\n",
            "Iteration 222, loss = 0.00112789\n",
            "Iteration 223, loss = 0.00112122\n",
            "Iteration 224, loss = 0.00110211\n",
            "Iteration 225, loss = 0.00109407\n",
            "Iteration 226, loss = 0.00108519\n",
            "Iteration 227, loss = 0.00109583\n",
            "Iteration 228, loss = 0.00107701\n",
            "Iteration 229, loss = 0.00106894\n",
            "Iteration 230, loss = 0.00107478\n",
            "Iteration 231, loss = 0.00105668\n",
            "Iteration 232, loss = 0.00106804\n",
            "Iteration 233, loss = 0.00105771\n",
            "Iteration 234, loss = 0.00105511\n",
            "Iteration 235, loss = 0.00104697\n",
            "Iteration 236, loss = 0.00103423\n",
            "Iteration 237, loss = 0.00102999\n",
            "Iteration 238, loss = 0.00102465\n",
            "Iteration 239, loss = 0.00101850\n",
            "Iteration 240, loss = 0.00101200\n",
            "Iteration 241, loss = 0.00101573\n",
            "Iteration 242, loss = 0.00100120\n",
            "Iteration 243, loss = 0.00100425\n",
            "Iteration 244, loss = 0.00099619\n",
            "Iteration 245, loss = 0.00099272\n",
            "Iteration 246, loss = 0.00098146\n",
            "Iteration 247, loss = 0.00097836\n",
            "Iteration 248, loss = 0.00097299\n",
            "Iteration 249, loss = 0.00097066\n",
            "Iteration 250, loss = 0.00097302\n",
            "Iteration 251, loss = 0.00095894\n",
            "Iteration 252, loss = 0.00096319\n",
            "Iteration 253, loss = 0.00095154\n",
            "Iteration 254, loss = 0.00095233\n",
            "Iteration 255, loss = 0.00094145\n",
            "Iteration 256, loss = 0.00094622\n",
            "Iteration 257, loss = 0.00094099\n",
            "Iteration 258, loss = 0.00092598\n",
            "Iteration 259, loss = 0.00093388\n",
            "Iteration 260, loss = 0.00092043\n",
            "Iteration 261, loss = 0.00091666\n",
            "Iteration 262, loss = 0.00091284\n",
            "Iteration 263, loss = 0.00090701\n",
            "Iteration 264, loss = 0.00090728\n",
            "Iteration 265, loss = 0.00090037\n",
            "Iteration 266, loss = 0.00089943\n",
            "Iteration 267, loss = 0.00089464\n",
            "Iteration 268, loss = 0.00088516\n",
            "Iteration 269, loss = 0.00089420\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(374,), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "Wi_0y1C6e9Er",
        "colab_type": "code",
        "outputId": "eea55c3a-010c-4b9f-9f31-d224c400597b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7352, 561)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "oy5MNJqFys-H",
        "colab_type": "code",
        "outputId": "3688a6b6-9234-4ca8-ebc6-58aa9e234783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "T0aiYNsdyuBR",
        "colab_type": "code",
        "outputId": "1d48cbbc-b60e-40d0-ece6-634d05d772fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.982324949014276"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "w7DSKjQcyvL0",
        "colab_type": "code",
        "outputId": "49267d26-7370-4622-d421-939ed2aeccde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9463861554122837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "bOJLm3y6dkCs",
        "colab_type": "code",
        "outputId": "3646c5c0-5229-484d-f1d8-73cbb640f0ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(train_data,train_label)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "hnUFUI4rdjwi",
        "colab_type": "code",
        "outputId": "0e5f057f-ac73-47e4-dcee-19c905280677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(validation_data,validation_label)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "1u7u5mLsirGr",
        "colab_type": "code",
        "outputId": "a1016770-e0dc-4bba-e709-2ca8578ad47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9463861554122837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "BtNN_G2Bdkpr",
        "colab_type": "code",
        "outputId": "d852cf72-a506-4565-9724-943f61182636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(test_data,test_label)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9501187648456056"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "63SIF9YCiJ9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9T8IoNGpkVv_",
        "colab_type": "code",
        "outputId": "1640ded2-be3e-49bc-9a41-b8195cae81fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2947, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "kxWBf1tjiJm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYHOkr0CiOxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDauxfwliQZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "\n",
        "\n",
        "# saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlClhIpViZoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Divide valid in two parts for validation and validation-test"
      ]
    },
    {
      "metadata": {
        "id": "NhtjHgUwl0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLrSUA61iQW_",
        "colab_type": "code",
        "outputId": "4d15582c-0cef-4542-b9a7-e84fb88d85b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(train_label_one_hot,axis = 1))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 981.,    0.,  858.,    0.,  789.,    0., 1029.,    0., 1099.,\n",
              "        1125.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADihJREFUeJzt3X+s3XV9x/HnaxREcVqEm4a1zS6J\njQsx2SA3yMJiFrsxfsXyhxrIpg3p0n9ww7FE6/4h2/7BZBE1WUgay1YyAhJgoVGiNoAxJoLeIoJQ\nHTcMbBuwV/mhzhnHfO+P88FdK6VwvveeQ+/n+Uhu7vf7+X7O+X5OCDzv+Z4fpKqQJPXnt6a9AEnS\ndBgASeqUAZCkThkASeqUAZCkThkASeqUAZCkThkASeqUAZCkTq2Z9gJeyemnn16zs7PTXoYkHVf2\n7dv3w6qaOda813UAZmdnmZ+fn/YyJOm4kuSpVzPPS0CS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmd\nMgCS1CkDIEmdMgCS1KnX9SeBJWmaZnd8YWrnfvK6S1b8HD4DkKROGQBJ6pQBkKROGQBJ6pQvAkt6\n3Zvmi7Grmc8AJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlT\nfheQdJxZ7f+TEk2OzwAkqVPHDECSG5McTvKdJWNvS7I3yePt96ltPEk+k2QhycNJzllym61t/uNJ\ntq7Mw5EkvVqv5hnAvwIXHjG2A7inqjYB97R9gIuATe1nO3ADjIIBXAu8CzgXuPalaEiSpuOYAaiq\nrwLPHjG8BdjdtncDly0Zv6lG7gfWJjkD+DNgb1U9W1XPAXv5zahIkiZo3NcA1lXV0237GWBd214P\nHFgy72AbO9q4JGlKBr8IXFUF1DKsBYAk25PMJ5lfXFxcrruVJB1h3AD8oF3aof0+3MYPARuXzNvQ\nxo42/huqamdVzVXV3MzMzJjLkyQdy7gB2AO89E6ercBdS8Y/1N4NdB7wQrtU9CXggiSnthd/L2hj\nkqQpOeYHwZLcAvwxcHqSg4zezXMdcFuSbcBTwAfa9LuBi4EF4GfAlQBV9WySfwS+2eb9Q1Ud+cLy\nspvWB2b8sIyk48ExA1BVVxzl0OaXmVvAVUe5nxuBG1/T6iRJK8ZPAktSpwyAJHXKAEhSpwyAJHXK\nAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhS\npwyAJHXKAEhSpwyAJHXKAEhSp9ZMewFaPrM7vjC1cz953SVTO7ek8fgMQJI6ZQAkqVMGQJI6ZQAk\nqVMGQJI6ZQAkqVODApDkb5I8muQ7SW5JcnKSM5M8kGQhyeeSnNTmvqHtL7Tjs8vxACRJ4xk7AEnW\nA38NzFXVO4ETgMuBTwDXV9XbgeeAbe0m24Dn2vj1bZ4kaUqGXgJaA7wxyRrgTcDTwHuA29vx3cBl\nbXtL26cd35wkA88vSRrT2AGoqkPAPwHfZ/Qf/heAfcDzVfVim3YQWN+21wMH2m1fbPNPG/f8kqRh\nhlwCOpXRX/VnAr8DnAJcOHRBSbYnmU8yv7i4OPTuJElHMeQS0J8A/1lVi1X1P8CdwPnA2nZJCGAD\ncKhtHwI2ArTjbwV+dOSdVtXOqpqrqrmZmZkBy5MkvZIhXwb3feC8JG8C/hvYDMwD9wHvA24FtgJ3\ntfl72v7X2/F7q6oGnF/yC/CkAYa8BvAAoxdzHwQeafe1E/gYcE2SBUbX+He1m+wCTmvj1wA7Bqxb\nkjTQoK+DrqprgWuPGH4COPdl5v4ceP+Q80mSlo+fBJakThkASeqUAZCkThkASeqUAZCkThkASeqU\nAZCkThkASeqUAZCkThkASeqUAZCkThkASeqUAZCkThkASeqUAZCkThkASeqUAZCkThkASeqUAZCk\nThkASeqUAZCkThkASeqUAZCkThkASeqUAZCkThkASeqUAZCkThkASerUoAAkWZvk9iTfTbI/yR8m\neVuSvUkeb79PbXOT5DNJFpI8nOSc5XkIkqRxDH0G8Gngi1X1e8DvA/uBHcA9VbUJuKftA1wEbGo/\n24EbBp5bkjTA2AFI8lbg3cAugKr6RVU9D2wBdrdpu4HL2vYW4KYauR9Ym+SMsVcuSRpkyDOAM4FF\n4F+SfCvJZ5OcAqyrqqfbnGeAdW17PXBgye0PtrFfk2R7kvkk84uLiwOWJ0l6JUMCsAY4B7ihqs4G\n/ov/v9wDQFUVUK/lTqtqZ1XNVdXczMzMgOVJkl7JkAAcBA5W1QNt/3ZGQfjBS5d22u/D7fghYOOS\n229oY5KkKRg7AFX1DHAgyTva0GbgMWAPsLWNbQXuatt7gA+1dwOdB7yw5FKRJGnC1gy8/V8BNyc5\nCXgCuJJRVG5Lsg14CvhAm3s3cDGwAPyszZUkTcmgAFTVQ8Dcyxza/DJzC7hqyPkkScvHTwJLUqcM\ngCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1\nygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBIUqcMgCR1ygBI\nUqcMgCR1anAAkpyQ5FtJPt/2z0zyQJKFJJ9LclIbf0PbX2jHZ4eeW5I0vuV4BnA1sH/J/ieA66vq\n7cBzwLY2vg14ro1f3+ZJkqZkUACSbAAuAT7b9gO8B7i9TdkNXNa2t7R92vHNbb4kaQqGPgP4FPBR\n4Jdt/zTg+ap6se0fBNa37fXAAYB2/IU2/9ck2Z5kPsn84uLiwOVJko5m7AAkuRQ4XFX7lnE9VNXO\nqpqrqrmZmZnlvGtJ0hJrBtz2fOC9SS4GTgbeAnwaWJtkTfsrfwNwqM0/BGwEDiZZA7wV+NGA80uS\nBhj7GUBVfbyqNlTVLHA5cG9V/TlwH/C+Nm0rcFfb3tP2acfvraoa9/ySpGFW4nMAHwOuSbLA6Br/\nrja+CzitjV8D7FiBc0uSXqUhl4B+paq+AnylbT8BnPsyc34OvH85zidJGs5PAktSpwyAJHXKAEhS\npwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyA\nJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSp8YOQJKN\nSe5L8liSR5Nc3cbflmRvksfb71PbeJJ8JslCkoeTnLNcD0KS9NoNeQbwIvC3VXUWcB5wVZKzgB3A\nPVW1Cbin7QNcBGxqP9uBGwacW5I00NgBqKqnq+rBtv0TYD+wHtgC7G7TdgOXte0twE01cj+wNskZ\nY69ckjTIsrwGkGQWOBt4AFhXVU+3Q88A69r2euDAkpsdbGOSpCkYHIAkbwbuAD5SVT9eeqyqCqjX\neH/bk8wnmV9cXBy6PEnSUQwKQJITGf3H/+aqurMN/+ClSzvt9+E2fgjYuOTmG9rYr6mqnVU1V1Vz\nMzMzQ5YnSXoFQ94FFGAXsL+qPrnk0B5ga9veCty1ZPxD7d1A5wEvLLlUJEmasDUDbns+8EHgkSQP\ntbG/A64DbkuyDXgK+EA7djdwMbAA/Ay4csC5JUkDjR2AqvoakKMc3vwy8wu4atzzSZKWl58ElqRO\nGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ\n6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQBkKROGQBJ6pQB\nkKROGQBJ6tTEA5DkwiTfS7KQZMekzy9JGploAJKcAPwzcBFwFnBFkrMmuQZJ0siknwGcCyxU1RNV\n9QvgVmDLhNcgSWLyAVgPHFiyf7CNSZImLFU1uZMl7wMurKq/bPsfBN5VVR9eMmc7sL3tvgP43oBT\nng78cMDtjze9PV7wMffCx/za/G5VzRxr0pox73xch4CNS/Y3tLFfqaqdwM7lOFmS+aqaW477Oh70\n9njBx9wLH/PKmPQloG8Cm5KcmeQk4HJgz4TXIEliws8AqurFJB8GvgScANxYVY9Ocg2SpJFJXwKi\nqu4G7p7Q6ZblUtJxpLfHCz7mXviYV8BEXwSWJL1++FUQktSpVRmA3r5uIsmNSQ4n+c601zIpSTYm\nuS/JY0keTXL1tNe00pKcnOQbSb7dHvPfT3tNk5DkhCTfSvL5aa9lUpI8meSRJA8lmV+x86y2S0Dt\n6yb+A/hTRh80+yZwRVU9NtWFraAk7wZ+CtxUVe+c9nomIckZwBlV9WCS3wb2AZet8n/OAU6pqp8m\nORH4GnB1Vd0/5aWtqCTXAHPAW6rq0mmvZxKSPAnMVdWKfvZhNT4D6O7rJqrqq8Cz017HJFXV01X1\nYNv+CbCfVf6p8hr5ads9sf2srr/gjpBkA3AJ8Nlpr2U1Wo0B8OsmOpNkFjgbeGC6K1l57XLIQ8Bh\nYG9VrfbH/Cngo8Avp72QCSvgy0n2tW9HWBGrMQDqSJI3A3cAH6mqH097PSutqv63qv6A0afoz02y\nai/5JbkUOFxV+6a9lin4o6o6h9E3J1/VLvMuu9UYgGN+3YRWh3Yd/A7g5qq6c9rrmaSqeh64D7hw\n2mtZQecD723Xw28F3pPk36a7pMmoqkPt92Hg3xld2l52qzEAft1EB9oLoruA/VX1yWmvZxKSzCRZ\n27bfyOiNDt+d7qpWTlV9vKo2VNUso3+P762qv5jyslZcklPaGxtIcgpwAbAi7/BbdQGoqheBl75u\nYj9w22r/uokktwBfB96R5GCSbdNe0wScD3yQ0V+FD7Wfi6e9qBV2BnBfkocZ/aGzt6q6eWtkR9YB\nX0vybeAbwBeq6osrcaJV9zZQSdKrs+qeAUiSXh0DIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkD\nIEmd+j9tO4uX1fDiHQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XfNKrbvrmDl6",
        "colab_type": "code",
        "outputId": "999010c0-798d-4ba1-eb4d-6792ec017e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([245.,   0., 215.,   0., 197.,   0., 257.,   0., 275., 282.]),\n",
              " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fX0eodcgiQTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYJZs7GgCy5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle  #train_data, train_label\n",
        "\n",
        "X_train, y_train = shuffle(train_data, train_label_one_hot)\n",
        "validation_data, validation_label_one_hot = validation_data, validation_label_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4WFIdiuCy53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup TensorFlow\n",
        "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "est-t83SCy55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vgad6Ny0OjqN",
        "colab_type": "code",
        "outputId": "0cfac5e9-a275-4684-d3e8-fa0583720d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "connection_probability = tf.Variable(.9999)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ZqUctyopqzZ",
        "colab_type": "code",
        "outputId": "1240aa4b-932a-49e1-e3f4-7b185802d0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "# print(G_W1.shape)\n",
        "# print(G_W2.shape)\n",
        "# print(G_W3.shape)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5881, 561)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UYVUi6tIf1mG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# len(clf.coefs_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6aNutv83RcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define the network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "stGmwMYz8vws",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "# G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "# G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "# G_w_out_h1 = tf.Variable(xavier_init([10,80]))\n",
        "# G_b_out_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "# G_w_h2_h1 = tf.Variable(xavier_init([40,80]))\n",
        "# G_b_h2_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "\n",
        "# G_w_h1_input = tf.Variable(xavier_init([80,16]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([16]))\n",
        "\n",
        "\n",
        "# G_w_input_h1_h2 = tf.Variable(xavier_init([16,40]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([40]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bajK44GmiR1",
        "colab_type": "code",
        "outputId": "42f8769d-23b0-405f-e41c-b5c99022a6ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5881, 561)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "hncjRrBbmRDY",
        "colab_type": "code",
        "outputId": "b20261dd-ad68-40ec-cfbf-61c77792c495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.coefs_[0].shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(561, 374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "-sfSdtHU3JfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x, test_mode = False):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    layer_depth = {\n",
        "        'layer_1' : 6,\n",
        "        'layer_2' : 16,\n",
        "        'layer_3' : 120,\n",
        "        'layer_f1' : 84\n",
        "    }\n",
        "\n",
        "\n",
        "    \n",
        "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
        "    x_flat = flatten(x)\n",
        "    fc1 = flatten(x)\n",
        "    fdense = fc1\n",
        "    \n",
        "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fc1_w = G_W1# tf.Variable(tf.truncated_normal(shape = (X_train.shape[1]*X_train.shape[2],300), mean = mu, stddev = sigma))\n",
        "    fc1_b = G_b1# tf.Variable(tf.zeros(300))\n",
        "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "#     # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "#     fc2_w = G_W2# tf.Variable(tf.truncated_normal(shape = (300,100), mean = mu, stddev = sigma))\n",
        "#     fc2_b = G_b2# tf.Variable(tf.zeros(100))\n",
        "#     fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
        "#     # TODO: Activation.\n",
        "#     fc2 = tf.nn.relu(fc2)\n",
        "    fc3_w = G_W2\n",
        "    fc3_b = G_b2\n",
        "    \n",
        "    logits = tf.matmul(fc1, fc3_w) + fc3_b\n",
        "    \n",
        "    #################\n",
        "    ##### Inset probability connection from x to conv2\n",
        "    fc2p_w = tf.Variable(xavier_init([X_train.shape[1],clf.coefs_[1].shape[1]]))\n",
        "    fc2p_b = tf.Variable(xavier_init([clf.coefs_[1].shape[1]]))\n",
        "    fc2_2nd_input = tf.matmul(x_flat,fc2p_w) + fc2p_b\n",
        "#     fc2_2nd_input = tf.nn.relu(fc2_2nd_input)\n",
        "    connect2 = tf.logical_and(tf.random.uniform(shape = tf.shape(connection_probability)) < connection_probability, tf.equal(test_mode,False))\n",
        "    logits = tf.cond(connect2,lambda: logits + fc2_2nd_input, lambda: logits )    \n",
        "    ################    \n",
        "#     fc3_w = G_W3\n",
        "#     fc3_b = G_b3\n",
        "    \n",
        "#     logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
        "#     print(logits.shape)\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGmN34tg3_tv",
        "colab_type": "code",
        "outputId": "b26e29e4-1a86-4d37-c69f-c3f3c20a43dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label_one_hot.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5881, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "-NX_lWUB6zue",
        "colab_type": "code",
        "outputId": "a6c39edc-cf09-42d1-86dc-9cec63b23e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 1, 0, ..., 4, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "M3U_MKYr34Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.name_scope('Input'):\n",
        "\n",
        "  x = tf.placeholder(tf.float32, (None, train_data.shape[1]), name='X')\n",
        "  y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# one_hot_y = tf.one_hot(y, train_label_one_hot.shape[1])\n",
        "is_testing= tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rtTKpeM4P8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-e6BG9DI3Jb3",
        "colab_type": "code",
        "outputId": "15a617c1-4ec0-4778-e240-872f9f7ae6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "rate = 0.001\n",
        "decay_rate = 1.0005**(X_train.shape[0]/BATCH_SIZE);\n",
        "decay_rate = 1.4\n",
        "print(decay_rate)\n",
        "logits = LeNet(x,is_testing)\n",
        "with tf.name_scope('Train'):\n",
        "#   cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
        "#   loss_operation = tf.reduce_mean(cross_entropy, name='loss')\n",
        "  loss_operation = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=LeNet(x, test_mode=False), labels=y))\n",
        "  tf.summary.scalar('loss', loss_operation)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate = rate,momentum=.9)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "# tf.train.natural_exp_decay()\n",
        "training_operation = optimizer.minimize(loss_operation)\n",
        "new_prob = connection_probability.assign(connection_probability/decay_rate)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-41-53232baabdb6>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8eQKHOw7PHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def evaluate(X_data, y_data):\n",
        "correct_pred = tf.equal(tf.argmax(LeNet(x,test_mode=True), 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tb-sFE34OGp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "# accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# saver = tf.train.Saver()\n",
        "\n",
        "# def evaluate(X_data, y_data):\n",
        "#     num_examples = len(X_data)\n",
        "#     total_accuracy = 0\n",
        "#     sess = tf.get_default_session()\n",
        "#     for offset in range(0, num_examples, BATCH_SIZE):\n",
        "#         batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "#         accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, is_testing: True})\n",
        "#         total_accuracy += (accuracy * len(batch_x))\n",
        "#     tot_acc = total_accuracy / num_examples\n",
        "#     with tf.name_scope('Accuracy'):\n",
        "#       tf.summary.scalar('accuracy', tot_acc)\n",
        "#     return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCovfr0E4oJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the mode"
      ]
    },
    {
      "metadata": {
        "id": "NGF2PRgw9nLL",
        "colab_type": "code",
        "outputId": "d27eb4b1-0381-4ae9-b871-81b6978f34bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "2056*2"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cl89uCj9hjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4112"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4H5Dgkf69TOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nk5Kihg685LI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQaEeNO285BK",
        "colab_type": "code",
        "outputId": "3e992e42-d2d8-4dd7-8bb7-7c56ae8ab5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5881, 561)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "2kUXlIo82NoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VP4QWKJ4ODG",
        "colab_type": "code",
        "outputId": "cbbb7fde-6ab2-41f2-81a7-f1e427a46a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31810
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = X_train.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "epoch_track = []\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          epoch_track.append(i)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "            saver.save(sess, './DNAAdamBased')\n",
        "        \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 99.79595\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 97.62067\n",
            "0.71421427\n",
            "\n",
            "Train Accuracy = 99.76195\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 97.62067\n",
            "0.024691546\n",
            "\n",
            "Train Accuracy = 99.96599\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 98.02855\n",
            "0.00085362676\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 97.96057\n",
            "2.9511264e-05\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 98.02855\n",
            "1.0202523e-06\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 98.16452\n",
            "3.527178e-08\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 98.16452\n",
            "1.2194024e-09\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 98.09653\n",
            "4.2156716e-11\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 98.09653\n",
            "1.457426e-12\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 98.23250\n",
            "5.0385572e-14\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 98.23250\n",
            "1.7419111e-15\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 98.16452\n",
            "6.022069e-17\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.081927e-18\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 98.16452\n",
            "7.197559e-20\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.4883126e-21\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 98.23250\n",
            "8.6024985e-23\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 98.23250\n",
            "2.9740232e-24\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 98.23250\n",
            "1.02816796e-25\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 98.23250\n",
            "3.5545433e-27\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 98.23250\n",
            "1.2288633e-28\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 201 ...\n",
            "Validation Accuracy = 98.23250\n",
            "4.2483803e-30\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 211 ...\n",
            "Validation Accuracy = 98.23250\n",
            "1.4687339e-31\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 221 ...\n",
            "Validation Accuracy = 98.23250\n",
            "5.0776518e-33\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 231 ...\n",
            "Validation Accuracy = 98.23250\n",
            "1.7554266e-34\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 241 ...\n",
            "Validation Accuracy = 98.23250\n",
            "6.068794e-36\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 251 ...\n",
            "Validation Accuracy = 98.16452\n",
            "2.0980801e-37\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 261 ...\n",
            "Validation Accuracy = 98.16452\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 271 ...\n",
            "Validation Accuracy = 98.16452\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 281 ...\n",
            "Validation Accuracy = 98.23250\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 291 ...\n",
            "Validation Accuracy = 98.23250\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 301 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 311 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 321 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 331 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 341 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 351 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 361 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 371 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 381 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 391 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 401 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 411 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 421 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 431 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 441 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 451 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 461 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 471 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 481 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 491 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 501 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 511 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 521 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 531 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 541 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 551 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 561 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 571 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 581 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 591 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 601 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 611 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 621 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 631 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 641 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 651 ...\n",
            "Validation Accuracy = 98.50442\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 661 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 671 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 681 ...\n",
            "Validation Accuracy = 98.50442\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 691 ...\n",
            "Validation Accuracy = 98.50442\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 701 ...\n",
            "Validation Accuracy = 98.50442\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 711 ...\n",
            "Validation Accuracy = 98.50442\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 721 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 731 ...\n",
            "Validation Accuracy = 98.50442\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 741 ...\n",
            "Validation Accuracy = 98.50442\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 751 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 761 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 771 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 781 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 791 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 801 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 811 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 821 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 831 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 841 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 851 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 861 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 871 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 881 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 891 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 901 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 911 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 921 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 931 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 941 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 951 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 961 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 971 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 981 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 991 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1001 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1011 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1021 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1031 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1041 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1051 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1061 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1071 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1081 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1091 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1101 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1111 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1121 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1131 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1141 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1151 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1161 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1171 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1181 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1191 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1201 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1211 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1221 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1231 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1241 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1251 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1261 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1271 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1281 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1291 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1301 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1311 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1321 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1331 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1341 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1351 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1361 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1371 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1381 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1391 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1401 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1411 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1421 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1431 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1441 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1451 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1461 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1471 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1481 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1491 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1501 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1511 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1521 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1531 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1541 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1551 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1561 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1571 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1581 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1591 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1601 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1611 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1621 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1631 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1641 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1651 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1661 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1671 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1681 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1691 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1701 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1711 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1721 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1731 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1741 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1751 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1761 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1771 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1781 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1791 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1801 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1811 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1821 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1831 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1841 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1851 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1861 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1871 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1881 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1891 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1901 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1911 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1921 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1931 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1941 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1951 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1961 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1971 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1981 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1991 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2001 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2011 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2021 ...\n",
            "Validation Accuracy = 98.43644\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2031 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2041 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2051 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2061 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2071 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2081 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2091 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2101 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2111 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2121 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2131 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2141 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2151 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2161 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2171 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2181 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2191 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2201 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2211 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2221 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2231 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2241 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2251 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2261 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2271 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2281 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2291 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2301 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2311 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2321 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2331 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2341 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2351 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2361 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2371 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2381 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2391 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2401 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2411 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2421 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2431 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2441 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2451 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2461 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2471 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2481 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2491 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2501 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2511 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2521 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2531 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2541 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2551 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2561 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2571 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2581 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2591 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2601 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2611 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2621 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2631 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2641 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2651 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2661 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2671 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2681 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2691 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2701 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2711 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2721 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2731 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2741 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2751 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2761 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2771 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2781 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2791 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2801 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2811 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2821 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2831 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2841 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2851 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2861 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2871 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2881 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2891 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2901 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2911 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2921 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2931 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2941 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2951 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2961 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2971 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2981 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2991 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3001 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3011 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3021 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3031 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3041 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3051 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3061 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3071 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3081 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3091 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3101 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3111 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3121 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3131 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3141 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3151 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3161 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3171 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3181 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3191 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3201 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3211 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3221 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3231 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3241 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3251 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3261 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3271 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3281 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3291 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3301 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3311 ...\n",
            "Validation Accuracy = 98.30048\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3321 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3331 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3341 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3351 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3361 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3371 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3381 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3391 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3401 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3411 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3421 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3431 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3441 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3451 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3461 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3471 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3481 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3491 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3501 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3511 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3521 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3531 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3541 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3551 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3561 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3571 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3581 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3591 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3601 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3611 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3621 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3631 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3641 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3651 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3661 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3671 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 3681 ...\n",
            "Validation Accuracy = 98.36845\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-2665bba4e43b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_testing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# evaluate(X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy = {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalidation_label_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_testing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#evaluate(X_validation, y_validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0mvalidation_accuracy_track\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mtrain_accuracy_track\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zydEA48dDeNA",
        "colab_type": "code",
        "outputId": "7637603a-8bc0-4557-fe8c-97526b8a3c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.argmax(validation_accuracy_track)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "oDevMH2N3JZu",
        "colab_type": "code",
        "outputId": "2b7b4135-5a58-4747-b354-c202761fb48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "best_accuracy_valid"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.50442"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "uhbuJEM0-sVm",
        "colab_type": "code",
        "outputId": "a3870619-62c0-4d48-e093-00d8fd461540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './DNAAdamBased')\n",
        "    saver.save(sess, './HarFullAdam')\n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./DNAAdamBased\n",
            "Validation Accuracy = 98.504417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D5okZYa4CSqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import spline\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ou2-UqDCXZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, 4861, print_every)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6G17qZY_Pxq",
        "colab_type": "code",
        "outputId": "5c3d3105-1a7d-4e6a-d866-1eef5a4efeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(validation_accuracy_track)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "369"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "sNS-YE0XCg4s",
        "colab_type": "code",
        "outputId": "fb44007d-f383-4b42-a129-e623708178ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "# plt.plot( savgol_filter(np.asarray(validation_accuracy_track),51,1))\n",
        "plt.plot( (validation_accuracy_track))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f18ec6ea128>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nySj4RUSajIJfRKTJ/H9pfltRuf2M5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BBlFJwfn-45J",
        "colab_type": "code",
        "outputId": "dbf17522-66f1-4c15-8517-d97d74f0f571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot( (connection_probability_track))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f18ec6235f8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MCKAiyEFCg2F",
        "colab_type": "code",
        "outputId": "40562360-6711-41fa-e397-0b9476c57383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "steps_plot = epoch_track# [step for step in range(0, 2291, print_every)]\n",
        "# plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "# plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "plt.plot(steps_plot, np.asarray(train_accuracy_track))  \n",
        "plt.plot(steps_plot, validation_accuracy_track)\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEYCAYAAAAwH9PuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTUREMkWJTUREMuX/A5dZzz642yKJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u7hYf8U_CXWh",
        "colab_type": "code",
        "outputId": "5667019b-ad2a-4145-ba60-84e38c51e8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "valid_accuracy_filtered = validation_accuracy_track\n",
        "print(max(valid_accuracy_filtered))\n",
        "valid_accuracy_filtered_np = np.asarray(valid_accuracy_filtered)\n",
        "print(np.argmax(valid_accuracy_filtered))\n",
        "print(steps_plot[np.argmax(valid_accuracy_filtered)])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.50442\n",
            "65\n",
            "650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKucNn6bEZzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lrwM4UUCXTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('HarFullDatasetMomentADAM_ProbabilityBasedValid98p5.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "                                       'train_accuracy_track':train_accuracy_track,\n",
        "                                       'connection_probability_track':connection_probability_track,\n",
        "                                       'epochTrack':epoch_track, 'TestAcc':'test_accuracy',\n",
        "                                                         'BestValidation':best_accuracy_valid})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFCL8d7-CIxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now  retrain til 651 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "qoqMxUbZCIRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 1\n",
        "aside_valid_test = train_valid_combined[-aside_examples:]\n",
        "aside_valid_test_label = validation_test_label_one_hot[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = validation_test_label_one_hot[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5n7bYp9-FCn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 651"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puoBTQ3fCIOT",
        "colab_type": "code",
        "outputId": "3fedc9dc-0df6-40a7-b5a3-071a7b914437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5678
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(combined_train_valid, combined_train_valid_label)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: aside_valid_test,y:aside_valid_test_label, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "#             saver.save(sess, './PendigitSGDBased')\n",
        "    saver.save(sess, './DNAAdamBasedAllPass')   \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 94.96667\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.71421427\n",
            "\n",
            "Train Accuracy = 99.83675\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.024691546\n",
            "\n",
            "Train Accuracy = 99.93198\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.00085362676\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.9511264e-05\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.0202523e-06\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.527178e-08\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.2194024e-09\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.2156716e-11\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.457426e-12\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 100.00000\n",
            "5.0385572e-14\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.7419111e-15\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.022069e-17\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.081927e-18\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 100.00000\n",
            "7.197559e-20\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.4883126e-21\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 100.00000\n",
            "8.6024985e-23\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.9740232e-24\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.02816796e-25\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.5545433e-27\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.2288633e-28\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 201 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.2483803e-30\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 211 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.4687339e-31\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 221 ...\n",
            "Validation Accuracy = 100.00000\n",
            "5.0776518e-33\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 231 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.7554266e-34\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 241 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.068794e-36\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 251 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.0980801e-37\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 261 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 271 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 281 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 291 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 301 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 311 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 321 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 331 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 341 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 351 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 361 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 371 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 381 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 391 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 401 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 411 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 421 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 431 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 441 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 451 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 461 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 471 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 481 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 491 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 501 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 511 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 521 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 531 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 541 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 551 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 561 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 571 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 581 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 591 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 601 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 611 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 621 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 631 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 641 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 651 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ImtSTCNJF9E_",
        "colab_type": "code",
        "outputId": "7d753838-6845-425c-c8b0-d8a9f32aa14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './DNAAdamBasedAllPass')\n",
        "    saver.save(sess, './HarFullAdamAllPass')  \n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./DNAAdamBasedAllPass\n",
            "Validation Accuracy = 100.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FVMAeXpCIL8",
        "colab_type": "code",
        "outputId": "d5bae231-70a8-41de-f5b2-49c3b49dcc90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Without all pass\n",
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './HarFullAdamAllPass')\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "    print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./HarFullAdamAllPass\n",
            "Test Accuracy = 95.147614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VGUWHQR3CIJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HXCpiAuCIGT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGYzlt7KCIDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hr57zkSqCIAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q08ZXCdr-sTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Without all pass\n",
        "# with tf.Session() as sess:\n",
        "# #     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "#     saver.restore(sess, './PendigitSGDBased')\n",
        "#     test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "#     print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9z1P1DG-sQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW6V7O1e-sNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochTrack = epoch_track\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r7lCgbXR3JXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFfzfjOB3JTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}