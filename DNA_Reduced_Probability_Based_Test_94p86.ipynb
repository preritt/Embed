{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNA Reduced Probability Based Test 94p86.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/DNA_Reduced_Probability_Based_Test_94p86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qWL1XvRKt0EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yuoNYnTshaZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4FSwFTChgdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_label = np.concatenate((train_label, validation_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VloppGYRyrEX",
        "colab_type": "code",
        "outputId": "bc6a18b0-2538-4e8a-f58b-2545d96dbf63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3590
        }
      },
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(138, ), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.19113258\n",
            "Iteration 2, loss = 1.01009171\n",
            "Iteration 3, loss = 0.84970326\n",
            "Iteration 4, loss = 0.71361306\n",
            "Iteration 5, loss = 0.60549314\n",
            "Iteration 6, loss = 0.51974736\n",
            "Iteration 7, loss = 0.45079054\n",
            "Iteration 8, loss = 0.39350212\n",
            "Iteration 9, loss = 0.34670357\n",
            "Iteration 10, loss = 0.30710786\n",
            "Iteration 11, loss = 0.27474230\n",
            "Iteration 12, loss = 0.24741212\n",
            "Iteration 13, loss = 0.22443345\n",
            "Iteration 14, loss = 0.20450708\n",
            "Iteration 15, loss = 0.18779527\n",
            "Iteration 16, loss = 0.17309816\n",
            "Iteration 17, loss = 0.15999054\n",
            "Iteration 18, loss = 0.14877533\n",
            "Iteration 19, loss = 0.13840638\n",
            "Iteration 20, loss = 0.12941453\n",
            "Iteration 21, loss = 0.12128939\n",
            "Iteration 22, loss = 0.11378779\n",
            "Iteration 23, loss = 0.10715411\n",
            "Iteration 24, loss = 0.10086864\n",
            "Iteration 25, loss = 0.09544108\n",
            "Iteration 26, loss = 0.09024088\n",
            "Iteration 27, loss = 0.08550149\n",
            "Iteration 28, loss = 0.08118749\n",
            "Iteration 29, loss = 0.07720110\n",
            "Iteration 30, loss = 0.07337679\n",
            "Iteration 31, loss = 0.06994639\n",
            "Iteration 32, loss = 0.06676823\n",
            "Iteration 33, loss = 0.06372529\n",
            "Iteration 34, loss = 0.06091058\n",
            "Iteration 35, loss = 0.05829514\n",
            "Iteration 36, loss = 0.05586008\n",
            "Iteration 37, loss = 0.05359733\n",
            "Iteration 38, loss = 0.05144066\n",
            "Iteration 39, loss = 0.04940965\n",
            "Iteration 40, loss = 0.04753014\n",
            "Iteration 41, loss = 0.04574422\n",
            "Iteration 42, loss = 0.04407502\n",
            "Iteration 43, loss = 0.04248416\n",
            "Iteration 44, loss = 0.04103003\n",
            "Iteration 45, loss = 0.03955591\n",
            "Iteration 46, loss = 0.03827631\n",
            "Iteration 47, loss = 0.03700468\n",
            "Iteration 48, loss = 0.03586690\n",
            "Iteration 49, loss = 0.03471634\n",
            "Iteration 50, loss = 0.03363056\n",
            "Iteration 51, loss = 0.03262115\n",
            "Iteration 52, loss = 0.03166825\n",
            "Iteration 53, loss = 0.03073287\n",
            "Iteration 54, loss = 0.02985037\n",
            "Iteration 55, loss = 0.02903502\n",
            "Iteration 56, loss = 0.02822780\n",
            "Iteration 57, loss = 0.02746830\n",
            "Iteration 58, loss = 0.02674097\n",
            "Iteration 59, loss = 0.02603418\n",
            "Iteration 60, loss = 0.02538406\n",
            "Iteration 61, loss = 0.02475452\n",
            "Iteration 62, loss = 0.02413188\n",
            "Iteration 63, loss = 0.02355618\n",
            "Iteration 64, loss = 0.02297461\n",
            "Iteration 65, loss = 0.02245802\n",
            "Iteration 66, loss = 0.02192311\n",
            "Iteration 67, loss = 0.02144879\n",
            "Iteration 68, loss = 0.02095068\n",
            "Iteration 69, loss = 0.02049604\n",
            "Iteration 70, loss = 0.02005095\n",
            "Iteration 71, loss = 0.01961769\n",
            "Iteration 72, loss = 0.01921732\n",
            "Iteration 73, loss = 0.01881627\n",
            "Iteration 74, loss = 0.01843110\n",
            "Iteration 75, loss = 0.01806977\n",
            "Iteration 76, loss = 0.01770756\n",
            "Iteration 77, loss = 0.01735367\n",
            "Iteration 78, loss = 0.01703175\n",
            "Iteration 79, loss = 0.01670688\n",
            "Iteration 80, loss = 0.01639634\n",
            "Iteration 81, loss = 0.01609561\n",
            "Iteration 82, loss = 0.01580898\n",
            "Iteration 83, loss = 0.01552415\n",
            "Iteration 84, loss = 0.01524103\n",
            "Iteration 85, loss = 0.01497805\n",
            "Iteration 86, loss = 0.01472393\n",
            "Iteration 87, loss = 0.01447121\n",
            "Iteration 88, loss = 0.01422796\n",
            "Iteration 89, loss = 0.01398887\n",
            "Iteration 90, loss = 0.01376050\n",
            "Iteration 91, loss = 0.01354669\n",
            "Iteration 92, loss = 0.01332180\n",
            "Iteration 93, loss = 0.01311902\n",
            "Iteration 94, loss = 0.01291032\n",
            "Iteration 95, loss = 0.01271222\n",
            "Iteration 96, loss = 0.01251867\n",
            "Iteration 97, loss = 0.01232936\n",
            "Iteration 98, loss = 0.01214891\n",
            "Iteration 99, loss = 0.01196799\n",
            "Iteration 100, loss = 0.01179569\n",
            "Iteration 101, loss = 0.01162387\n",
            "Iteration 102, loss = 0.01145940\n",
            "Iteration 103, loss = 0.01130028\n",
            "Iteration 104, loss = 0.01114171\n",
            "Iteration 105, loss = 0.01099289\n",
            "Iteration 106, loss = 0.01084058\n",
            "Iteration 107, loss = 0.01069713\n",
            "Iteration 108, loss = 0.01054977\n",
            "Iteration 109, loss = 0.01041533\n",
            "Iteration 110, loss = 0.01027769\n",
            "Iteration 111, loss = 0.01014261\n",
            "Iteration 112, loss = 0.01001439\n",
            "Iteration 113, loss = 0.00988712\n",
            "Iteration 114, loss = 0.00976342\n",
            "Iteration 115, loss = 0.00964443\n",
            "Iteration 116, loss = 0.00952468\n",
            "Iteration 117, loss = 0.00940749\n",
            "Iteration 118, loss = 0.00929681\n",
            "Iteration 119, loss = 0.00918411\n",
            "Iteration 120, loss = 0.00907617\n",
            "Iteration 121, loss = 0.00896859\n",
            "Iteration 122, loss = 0.00886449\n",
            "Iteration 123, loss = 0.00876126\n",
            "Iteration 124, loss = 0.00866475\n",
            "Iteration 125, loss = 0.00856623\n",
            "Iteration 126, loss = 0.00847052\n",
            "Iteration 127, loss = 0.00837600\n",
            "Iteration 128, loss = 0.00828310\n",
            "Iteration 129, loss = 0.00819303\n",
            "Iteration 130, loss = 0.00810665\n",
            "Iteration 131, loss = 0.00801825\n",
            "Iteration 132, loss = 0.00793253\n",
            "Iteration 133, loss = 0.00784939\n",
            "Iteration 134, loss = 0.00776898\n",
            "Iteration 135, loss = 0.00768696\n",
            "Iteration 136, loss = 0.00760750\n",
            "Iteration 137, loss = 0.00753193\n",
            "Iteration 138, loss = 0.00745567\n",
            "Iteration 139, loss = 0.00738051\n",
            "Iteration 140, loss = 0.00730831\n",
            "Iteration 141, loss = 0.00723537\n",
            "Iteration 142, loss = 0.00716347\n",
            "Iteration 143, loss = 0.00709481\n",
            "Iteration 144, loss = 0.00702759\n",
            "Iteration 145, loss = 0.00695949\n",
            "Iteration 146, loss = 0.00689294\n",
            "Iteration 147, loss = 0.00682912\n",
            "Iteration 148, loss = 0.00676527\n",
            "Iteration 149, loss = 0.00670412\n",
            "Iteration 150, loss = 0.00664076\n",
            "Iteration 151, loss = 0.00658133\n",
            "Iteration 152, loss = 0.00652083\n",
            "Iteration 153, loss = 0.00646257\n",
            "Iteration 154, loss = 0.00640489\n",
            "Iteration 155, loss = 0.00634774\n",
            "Iteration 156, loss = 0.00629265\n",
            "Iteration 157, loss = 0.00623799\n",
            "Iteration 158, loss = 0.00618407\n",
            "Iteration 159, loss = 0.00613052\n",
            "Iteration 160, loss = 0.00607825\n",
            "Iteration 161, loss = 0.00602700\n",
            "Iteration 162, loss = 0.00597567\n",
            "Iteration 163, loss = 0.00592526\n",
            "Iteration 164, loss = 0.00587674\n",
            "Iteration 165, loss = 0.00582735\n",
            "Iteration 166, loss = 0.00578072\n",
            "Iteration 167, loss = 0.00573341\n",
            "Iteration 168, loss = 0.00568716\n",
            "Iteration 169, loss = 0.00564057\n",
            "Iteration 170, loss = 0.00559461\n",
            "Iteration 171, loss = 0.00555116\n",
            "Iteration 172, loss = 0.00550803\n",
            "Iteration 173, loss = 0.00546582\n",
            "Iteration 174, loss = 0.00542276\n",
            "Iteration 175, loss = 0.00538115\n",
            "Iteration 176, loss = 0.00533890\n",
            "Iteration 177, loss = 0.00529795\n",
            "Iteration 178, loss = 0.00525891\n",
            "Iteration 179, loss = 0.00521894\n",
            "Iteration 180, loss = 0.00517972\n",
            "Iteration 181, loss = 0.00514120\n",
            "Iteration 182, loss = 0.00510356\n",
            "Iteration 183, loss = 0.00506604\n",
            "Iteration 184, loss = 0.00502818\n",
            "Iteration 185, loss = 0.00499240\n",
            "Iteration 186, loss = 0.00495610\n",
            "Iteration 187, loss = 0.00492026\n",
            "Iteration 188, loss = 0.00488602\n",
            "Iteration 189, loss = 0.00485079\n",
            "Iteration 190, loss = 0.00481721\n",
            "Iteration 191, loss = 0.00478366\n",
            "Iteration 192, loss = 0.00474998\n",
            "Iteration 193, loss = 0.00471742\n",
            "Iteration 194, loss = 0.00468453\n",
            "Iteration 195, loss = 0.00465249\n",
            "Iteration 196, loss = 0.00462072\n",
            "Iteration 197, loss = 0.00459010\n",
            "Iteration 198, loss = 0.00455939\n",
            "Iteration 199, loss = 0.00452802\n",
            "Iteration 200, loss = 0.00449795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(138,), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "8m9_X9bUdZJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "# train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo_lFxdIc85n",
        "colab_type": "code",
        "outputId": "f891a755-8dd6-41ff-c07c-8b3a2d1c4411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3590
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf2 = MLPClassifier(hidden_layer_sizes=(138, ), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "clf2.fit(train_valid_combined, train_valid_label)\n",
        "# clf2.fit(train_data, train_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.15072293\n",
            "Iteration 2, loss = 0.91569572\n",
            "Iteration 3, loss = 0.72166672\n",
            "Iteration 4, loss = 0.57821542\n",
            "Iteration 5, loss = 0.47305136\n",
            "Iteration 6, loss = 0.39365008\n",
            "Iteration 7, loss = 0.33232918\n",
            "Iteration 8, loss = 0.28531490\n",
            "Iteration 9, loss = 0.24854399\n",
            "Iteration 10, loss = 0.21954920\n",
            "Iteration 11, loss = 0.19633368\n",
            "Iteration 12, loss = 0.17762887\n",
            "Iteration 13, loss = 0.16193546\n",
            "Iteration 14, loss = 0.14845612\n",
            "Iteration 15, loss = 0.13701009\n",
            "Iteration 16, loss = 0.12693760\n",
            "Iteration 17, loss = 0.11820281\n",
            "Iteration 18, loss = 0.11039802\n",
            "Iteration 19, loss = 0.10331484\n",
            "Iteration 20, loss = 0.09716385\n",
            "Iteration 21, loss = 0.09141162\n",
            "Iteration 22, loss = 0.08635659\n",
            "Iteration 23, loss = 0.08158839\n",
            "Iteration 24, loss = 0.07731552\n",
            "Iteration 25, loss = 0.07325289\n",
            "Iteration 26, loss = 0.06961857\n",
            "Iteration 27, loss = 0.06621410\n",
            "Iteration 28, loss = 0.06302236\n",
            "Iteration 29, loss = 0.06011450\n",
            "Iteration 30, loss = 0.05734502\n",
            "Iteration 31, loss = 0.05481820\n",
            "Iteration 32, loss = 0.05232316\n",
            "Iteration 33, loss = 0.05014528\n",
            "Iteration 34, loss = 0.04803267\n",
            "Iteration 35, loss = 0.04604300\n",
            "Iteration 36, loss = 0.04422010\n",
            "Iteration 37, loss = 0.04243525\n",
            "Iteration 38, loss = 0.04080024\n",
            "Iteration 39, loss = 0.03929857\n",
            "Iteration 40, loss = 0.03781607\n",
            "Iteration 41, loss = 0.03648034\n",
            "Iteration 42, loss = 0.03511426\n",
            "Iteration 43, loss = 0.03387861\n",
            "Iteration 44, loss = 0.03272863\n",
            "Iteration 45, loss = 0.03163791\n",
            "Iteration 46, loss = 0.03055580\n",
            "Iteration 47, loss = 0.02955941\n",
            "Iteration 48, loss = 0.02859447\n",
            "Iteration 49, loss = 0.02772411\n",
            "Iteration 50, loss = 0.02689200\n",
            "Iteration 51, loss = 0.02606292\n",
            "Iteration 52, loss = 0.02528093\n",
            "Iteration 53, loss = 0.02456171\n",
            "Iteration 54, loss = 0.02382916\n",
            "Iteration 55, loss = 0.02317736\n",
            "Iteration 56, loss = 0.02251817\n",
            "Iteration 57, loss = 0.02192281\n",
            "Iteration 58, loss = 0.02133733\n",
            "Iteration 59, loss = 0.02078698\n",
            "Iteration 60, loss = 0.02023008\n",
            "Iteration 61, loss = 0.01973807\n",
            "Iteration 62, loss = 0.01925119\n",
            "Iteration 63, loss = 0.01876434\n",
            "Iteration 64, loss = 0.01833849\n",
            "Iteration 65, loss = 0.01788560\n",
            "Iteration 66, loss = 0.01747378\n",
            "Iteration 67, loss = 0.01708056\n",
            "Iteration 68, loss = 0.01669004\n",
            "Iteration 69, loss = 0.01633133\n",
            "Iteration 70, loss = 0.01596694\n",
            "Iteration 71, loss = 0.01564091\n",
            "Iteration 72, loss = 0.01529899\n",
            "Iteration 73, loss = 0.01498363\n",
            "Iteration 74, loss = 0.01467515\n",
            "Iteration 75, loss = 0.01437444\n",
            "Iteration 76, loss = 0.01409830\n",
            "Iteration 77, loss = 0.01382389\n",
            "Iteration 78, loss = 0.01356077\n",
            "Iteration 79, loss = 0.01329626\n",
            "Iteration 80, loss = 0.01304650\n",
            "Iteration 81, loss = 0.01280434\n",
            "Iteration 82, loss = 0.01257339\n",
            "Iteration 83, loss = 0.01234938\n",
            "Iteration 84, loss = 0.01212282\n",
            "Iteration 85, loss = 0.01191548\n",
            "Iteration 86, loss = 0.01171598\n",
            "Iteration 87, loss = 0.01150995\n",
            "Iteration 88, loss = 0.01131526\n",
            "Iteration 89, loss = 0.01113114\n",
            "Iteration 90, loss = 0.01094831\n",
            "Iteration 91, loss = 0.01076900\n",
            "Iteration 92, loss = 0.01059831\n",
            "Iteration 93, loss = 0.01042636\n",
            "Iteration 94, loss = 0.01026719\n",
            "Iteration 95, loss = 0.01011157\n",
            "Iteration 96, loss = 0.00995243\n",
            "Iteration 97, loss = 0.00980672\n",
            "Iteration 98, loss = 0.00966030\n",
            "Iteration 99, loss = 0.00951901\n",
            "Iteration 100, loss = 0.00938138\n",
            "Iteration 101, loss = 0.00924581\n",
            "Iteration 102, loss = 0.00911423\n",
            "Iteration 103, loss = 0.00898860\n",
            "Iteration 104, loss = 0.00886067\n",
            "Iteration 105, loss = 0.00874233\n",
            "Iteration 106, loss = 0.00862296\n",
            "Iteration 107, loss = 0.00850296\n",
            "Iteration 108, loss = 0.00839312\n",
            "Iteration 109, loss = 0.00828406\n",
            "Iteration 110, loss = 0.00817623\n",
            "Iteration 111, loss = 0.00806560\n",
            "Iteration 112, loss = 0.00796645\n",
            "Iteration 113, loss = 0.00786725\n",
            "Iteration 114, loss = 0.00776954\n",
            "Iteration 115, loss = 0.00766811\n",
            "Iteration 116, loss = 0.00757761\n",
            "Iteration 117, loss = 0.00748471\n",
            "Iteration 118, loss = 0.00739419\n",
            "Iteration 119, loss = 0.00730647\n",
            "Iteration 120, loss = 0.00722004\n",
            "Iteration 121, loss = 0.00713473\n",
            "Iteration 122, loss = 0.00705382\n",
            "Iteration 123, loss = 0.00697305\n",
            "Iteration 124, loss = 0.00689551\n",
            "Iteration 125, loss = 0.00681480\n",
            "Iteration 126, loss = 0.00673992\n",
            "Iteration 127, loss = 0.00666738\n",
            "Iteration 128, loss = 0.00659245\n",
            "Iteration 129, loss = 0.00652048\n",
            "Iteration 130, loss = 0.00645145\n",
            "Iteration 131, loss = 0.00638215\n",
            "Iteration 132, loss = 0.00631395\n",
            "Iteration 133, loss = 0.00624915\n",
            "Iteration 134, loss = 0.00618380\n",
            "Iteration 135, loss = 0.00612086\n",
            "Iteration 136, loss = 0.00605653\n",
            "Iteration 137, loss = 0.00599633\n",
            "Iteration 138, loss = 0.00593693\n",
            "Iteration 139, loss = 0.00587769\n",
            "Iteration 140, loss = 0.00581912\n",
            "Iteration 141, loss = 0.00576190\n",
            "Iteration 142, loss = 0.00570472\n",
            "Iteration 143, loss = 0.00565080\n",
            "Iteration 144, loss = 0.00559714\n",
            "Iteration 145, loss = 0.00554288\n",
            "Iteration 146, loss = 0.00549058\n",
            "Iteration 147, loss = 0.00544068\n",
            "Iteration 148, loss = 0.00538986\n",
            "Iteration 149, loss = 0.00533826\n",
            "Iteration 150, loss = 0.00529110\n",
            "Iteration 151, loss = 0.00524224\n",
            "Iteration 152, loss = 0.00519506\n",
            "Iteration 153, loss = 0.00514839\n",
            "Iteration 154, loss = 0.00510350\n",
            "Iteration 155, loss = 0.00505722\n",
            "Iteration 156, loss = 0.00501388\n",
            "Iteration 157, loss = 0.00497161\n",
            "Iteration 158, loss = 0.00492742\n",
            "Iteration 159, loss = 0.00488465\n",
            "Iteration 160, loss = 0.00484313\n",
            "Iteration 161, loss = 0.00480191\n",
            "Iteration 162, loss = 0.00476236\n",
            "Iteration 163, loss = 0.00472470\n",
            "Iteration 164, loss = 0.00468314\n",
            "Iteration 165, loss = 0.00464538\n",
            "Iteration 166, loss = 0.00460693\n",
            "Iteration 167, loss = 0.00457014\n",
            "Iteration 168, loss = 0.00453250\n",
            "Iteration 169, loss = 0.00449584\n",
            "Iteration 170, loss = 0.00446096\n",
            "Iteration 171, loss = 0.00442511\n",
            "Iteration 172, loss = 0.00439031\n",
            "Iteration 173, loss = 0.00435568\n",
            "Iteration 174, loss = 0.00432279\n",
            "Iteration 175, loss = 0.00428911\n",
            "Iteration 176, loss = 0.00425587\n",
            "Iteration 177, loss = 0.00422389\n",
            "Iteration 178, loss = 0.00419245\n",
            "Iteration 179, loss = 0.00416048\n",
            "Iteration 180, loss = 0.00412985\n",
            "Iteration 181, loss = 0.00409830\n",
            "Iteration 182, loss = 0.00406817\n",
            "Iteration 183, loss = 0.00403897\n",
            "Iteration 184, loss = 0.00400991\n",
            "Iteration 185, loss = 0.00398016\n",
            "Iteration 186, loss = 0.00395228\n",
            "Iteration 187, loss = 0.00392331\n",
            "Iteration 188, loss = 0.00389573\n",
            "Iteration 189, loss = 0.00386861\n",
            "Iteration 190, loss = 0.00384142\n",
            "Iteration 191, loss = 0.00381422\n",
            "Iteration 192, loss = 0.00378828\n",
            "Iteration 193, loss = 0.00376225\n",
            "Iteration 194, loss = 0.00373597\n",
            "Iteration 195, loss = 0.00371012\n",
            "Iteration 196, loss = 0.00368630\n",
            "Iteration 197, loss = 0.00366029\n",
            "Iteration 198, loss = 0.00363631\n",
            "Iteration 199, loss = 0.00361206\n",
            "Iteration 200, loss = 0.00358804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(138,), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "Wi_0y1C6e9Er",
        "colab_type": "code",
        "outputId": "f965c17f-bcbd-42a2-d0db-24385ae51216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 138)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "oy5MNJqFys-H",
        "colab_type": "code",
        "outputId": "de9ed8c3-1155-48fc-e0ca-c8bd344e309a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "T0aiYNsdyuBR",
        "colab_type": "code",
        "outputId": "fdb10129-2d43-41bc-ae4b-0d8a9f644ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "w7DSKjQcyvL0",
        "colab_type": "code",
        "outputId": "50c11e7a-12fd-44e4-f33b-6c4269bba43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9300168634064081"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "bOJLm3y6dkCs",
        "colab_type": "code",
        "outputId": "c1210bb0-cea0-4b33-ea9a-eac3c0eb928a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(train_data,train_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "hnUFUI4rdjwi",
        "colab_type": "code",
        "outputId": "f70e5992-a761-4b8f-c0a6-be5b725eb8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(validation_data,validation_label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "1u7u5mLsirGr",
        "colab_type": "code",
        "outputId": "f041082d-2533-467c-c34a-1da5b98cc574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9300168634064081"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "BtNN_G2Bdkpr",
        "colab_type": "code",
        "outputId": "6b0c7ea6-9989-46e8-9de5-cc8b3942a943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(test_data,test_label)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9494097807757167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "63SIF9YCiJ9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9T8IoNGpkVv_",
        "colab_type": "code",
        "outputId": "f1f25bd6-5ea2-47ae-a616-075981a867be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1186, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "kxWBf1tjiJm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYHOkr0CiOxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDauxfwliQZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "\n",
        "\n",
        "# saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlClhIpViZoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Divide valid in two parts for validation and validation-test"
      ]
    },
    {
      "metadata": {
        "id": "NhtjHgUwl0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLrSUA61iQW_",
        "colab_type": "code",
        "outputId": "8ec8588c-253f-49b4-b719-5f42a1ef8780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(train_label_one_hot,axis = 1))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([331.,   0.,   0.,   0.,   0., 329.,   0.,   0.,   0., 740.]),\n",
              " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XfNKrbvrmDl6",
        "colab_type": "code",
        "outputId": "0d986930-0520-41fe-96c5-b83d29824034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([133.,   0.,   0.,   0.,   0., 156.,   0.,   0.,   0., 311.]),\n",
              " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndc6gl6TO/R8rvmB5ggrV4QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fX0eodcgiQTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYJZs7GgCy5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle  #train_data, train_label\n",
        "\n",
        "X_train, y_train = shuffle(train_data, train_label_one_hot)\n",
        "validation_data, validation_label_one_hot = validation_data, validation_label_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4WFIdiuCy53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup TensorFlow\n",
        "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "est-t83SCy55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vgad6Ny0OjqN",
        "colab_type": "code",
        "outputId": "fc4a136f-a7ea-4fd2-a406-7960e4462111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "connection_probability = tf.Variable(.9999)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ZqUctyopqzZ",
        "colab_type": "code",
        "outputId": "6e95493a-982a-4b6f-983d-9758bcda103e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "# print(G_W1.shape)\n",
        "# print(G_W2.shape)\n",
        "# print(G_W3.shape)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1400, 138)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UYVUi6tIf1mG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# len(clf.coefs_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6aNutv83RcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define the network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "stGmwMYz8vws",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "# G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "# G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "# G_w_out_h1 = tf.Variable(xavier_init([10,80]))\n",
        "# G_b_out_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "# G_w_h2_h1 = tf.Variable(xavier_init([40,80]))\n",
        "# G_b_h2_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "\n",
        "# G_w_h1_input = tf.Variable(xavier_init([80,16]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([16]))\n",
        "\n",
        "\n",
        "# G_w_input_h1_h2 = tf.Variable(xavier_init([16,40]))\n",
        "# G_b_h1_input = tf.Variable(xavier_init([40]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bajK44GmiR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "087005cc-074d-48cc-83cd-ee7dab7640fc"
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1400, 138)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "hncjRrBbmRDY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a71ea059-537b-4e7a-88b8-e2a38bd4cbee"
      },
      "cell_type": "code",
      "source": [
        "clf.coefs_[0].shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(138, 138)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "-sfSdtHU3JfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x, test_mode = False):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    layer_depth = {\n",
        "        'layer_1' : 6,\n",
        "        'layer_2' : 16,\n",
        "        'layer_3' : 120,\n",
        "        'layer_f1' : 84\n",
        "    }\n",
        "\n",
        "\n",
        "    \n",
        "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
        "    x_flat = flatten(x)\n",
        "    fc1 = flatten(x)\n",
        "    fdense = fc1\n",
        "    \n",
        "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fc1_w = G_W1# tf.Variable(tf.truncated_normal(shape = (X_train.shape[1]*X_train.shape[2],300), mean = mu, stddev = sigma))\n",
        "    fc1_b = G_b1# tf.Variable(tf.zeros(300))\n",
        "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "#     # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "#     fc2_w = G_W2# tf.Variable(tf.truncated_normal(shape = (300,100), mean = mu, stddev = sigma))\n",
        "#     fc2_b = G_b2# tf.Variable(tf.zeros(100))\n",
        "#     fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
        "#     # TODO: Activation.\n",
        "#     fc2 = tf.nn.relu(fc2)\n",
        "    fc3_w = G_W2\n",
        "    fc3_b = G_b2\n",
        "    \n",
        "    logits = tf.matmul(fc1, fc3_w) + fc3_b\n",
        "    \n",
        "    #################\n",
        "    ##### Inset probability connection from x to conv2\n",
        "    fc2p_w = tf.Variable(xavier_init([X_train.shape[1],clf.coefs_[1].shape[1]]))\n",
        "    fc2p_b = tf.Variable(xavier_init([clf.coefs_[1].shape[1]]))\n",
        "    fc2_2nd_input = tf.matmul(x_flat,fc2p_w) + fc2p_b\n",
        "#     fc2_2nd_input = tf.nn.relu(fc2_2nd_input)\n",
        "    connect2 = tf.logical_and(tf.random.uniform(shape = tf.shape(connection_probability)) < connection_probability, tf.equal(test_mode,False))\n",
        "    logits = tf.cond(connect2,lambda: logits + fc2_2nd_input, lambda: logits )    \n",
        "    ################    \n",
        "#     fc3_w = G_W3\n",
        "#     fc3_b = G_b3\n",
        "    \n",
        "#     logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
        "#     print(logits.shape)\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGmN34tg3_tv",
        "colab_type": "code",
        "outputId": "cf6fbfd6-ff39-43fa-ed01-264fc2c7780e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label_one_hot.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1400, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "-NX_lWUB6zue",
        "colab_type": "code",
        "outputId": "7bef97bc-e503-4691-aead-e30364c75f67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 2, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "M3U_MKYr34Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.name_scope('Input'):\n",
        "\n",
        "  x = tf.placeholder(tf.float32, (None, train_data.shape[1]), name='X')\n",
        "  y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# one_hot_y = tf.one_hot(y, train_label_one_hot.shape[1])\n",
        "is_testing= tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rtTKpeM4P8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-e6BG9DI3Jb3",
        "colab_type": "code",
        "outputId": "37e2fed4-fae9-4175-f607-df57b133a1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rate = 0.001\n",
        "decay_rate = 1.0005**(X_train.shape[0]/BATCH_SIZE);\n",
        "decay_rate = 1.4\n",
        "print(decay_rate)\n",
        "logits = LeNet(x,is_testing)\n",
        "with tf.name_scope('Train'):\n",
        "#   cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
        "#   loss_operation = tf.reduce_mean(cross_entropy, name='loss')\n",
        "  loss_operation = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=LeNet(x, test_mode=False), labels=y))\n",
        "  tf.summary.scalar('loss', loss_operation)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate = rate,momentum=.9)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "# tf.train.natural_exp_decay()\n",
        "training_operation = optimizer.minimize(loss_operation)\n",
        "new_prob = connection_probability.assign(connection_probability/decay_rate)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8eQKHOw7PHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def evaluate(X_data, y_data):\n",
        "correct_pred = tf.equal(tf.argmax(LeNet(x,test_mode=True), 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tb-sFE34OGp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "# accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# saver = tf.train.Saver()\n",
        "\n",
        "# def evaluate(X_data, y_data):\n",
        "#     num_examples = len(X_data)\n",
        "#     total_accuracy = 0\n",
        "#     sess = tf.get_default_session()\n",
        "#     for offset in range(0, num_examples, BATCH_SIZE):\n",
        "#         batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "#         accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, is_testing: True})\n",
        "#         total_accuracy += (accuracy * len(batch_x))\n",
        "#     tot_acc = total_accuracy / num_examples\n",
        "#     with tf.name_scope('Accuracy'):\n",
        "#       tf.summary.scalar('accuracy', tot_acc)\n",
        "#     return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCovfr0E4oJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the mode"
      ]
    },
    {
      "metadata": {
        "id": "NGF2PRgw9nLL",
        "colab_type": "code",
        "outputId": "9536c5ef-db8e-4a37-c9f1-a5df40748a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "2056*2"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cl89uCj9hjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4H5Dgkf69TOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nk5Kihg685LI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQaEeNO285BK",
        "colab_type": "code",
        "outputId": "6692d1d6-4e5b-4972-c502-fe6bc027f3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1400, 138)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "4VP4QWKJ4ODG",
        "colab_type": "code",
        "outputId": "8dc173af-a328-4d17-863d-78b5796aea1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18873
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = X_train.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "epoch_track = []\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          epoch_track.append(i)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "            saver.save(sess, './DNAAdamBased')\n",
        "        \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 93.50000\n",
            "0.71421427\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 93.00000\n",
            "0.024691546\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 93.66666\n",
            "0.00085362676\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 93.33334\n",
            "2.9511264e-05\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 93.83334\n",
            "1.0202523e-06\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 93.83334\n",
            "3.527178e-08\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 94.00000\n",
            "1.2194024e-09\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 94.33334\n",
            "4.2156716e-11\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 94.33334\n",
            "1.457426e-12\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 94.50000\n",
            "5.0385572e-14\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 94.66666\n",
            "1.7419111e-15\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 94.33334\n",
            "6.022069e-17\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 94.50000\n",
            "2.081927e-18\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 94.50000\n",
            "7.197559e-20\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 94.50000\n",
            "2.4883126e-21\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 94.66666\n",
            "8.6024985e-23\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 94.66666\n",
            "2.9740232e-24\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 94.66666\n",
            "1.02816796e-25\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 94.66666\n",
            "3.5545433e-27\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 94.66666\n",
            "1.2288633e-28\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 201 ...\n",
            "Validation Accuracy = 94.66666\n",
            "4.2483803e-30\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 211 ...\n",
            "Validation Accuracy = 94.83334\n",
            "1.4687339e-31\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 221 ...\n",
            "Validation Accuracy = 94.83334\n",
            "5.0776518e-33\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 231 ...\n",
            "Validation Accuracy = 94.83334\n",
            "1.7554266e-34\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 241 ...\n",
            "Validation Accuracy = 94.83334\n",
            "6.068794e-36\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 251 ...\n",
            "Validation Accuracy = 94.83334\n",
            "2.0980801e-37\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 261 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 271 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 281 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 291 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 301 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 311 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 321 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 331 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 341 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 351 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 361 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 371 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 381 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 391 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 401 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 411 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 421 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 431 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 441 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 451 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 461 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 471 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 481 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 491 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 501 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 511 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 521 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 531 ...\n",
            "Validation Accuracy = 95.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 541 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 551 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 561 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 571 ...\n",
            "Validation Accuracy = 94.83334\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 581 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 591 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 601 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 611 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 621 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 631 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 641 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 651 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 661 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 671 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 681 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 691 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 701 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 711 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 721 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 731 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 741 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 751 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 761 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 771 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 781 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 791 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 801 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 811 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 821 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 831 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 841 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 851 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 861 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 871 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 881 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 891 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 901 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 911 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 921 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 931 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 941 ...\n",
            "Validation Accuracy = 94.50000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 951 ...\n",
            "Validation Accuracy = 94.50000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 961 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 971 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 981 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 991 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1001 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1011 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1021 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1031 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1041 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1051 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1061 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1071 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1081 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1091 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1101 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1111 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1121 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1131 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1141 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1151 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1161 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1171 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1181 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1191 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1201 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1211 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1221 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1231 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1241 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1251 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1261 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1271 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1281 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1291 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1301 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1311 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1321 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1331 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1341 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1351 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1361 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1371 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1381 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1391 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1401 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1411 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1421 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1431 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1441 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1451 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1461 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1471 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1481 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1491 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1501 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1511 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1521 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1531 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1541 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1551 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1561 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1571 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1581 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1591 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1601 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1611 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1621 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1631 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1641 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1651 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1661 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1671 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1681 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1691 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1701 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1711 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1721 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1731 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1741 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1751 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1761 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1771 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1781 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1791 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1801 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1811 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1821 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1831 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1841 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1851 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1861 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1871 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1881 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1891 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1901 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1911 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1921 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1931 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1941 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1951 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1961 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1971 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1981 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 1991 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2001 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2011 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2021 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2031 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2041 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2051 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2061 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2071 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2081 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2091 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2101 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2111 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2121 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2131 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2141 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2151 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 2161 ...\n",
            "Validation Accuracy = 94.66666\n",
            "0.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-2665bba4e43b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_testing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# evaluate(X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy = {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalidation_label_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_testing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#evaluate(X_validation, y_validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zydEA48dDeNA",
        "colab_type": "code",
        "outputId": "7637603a-8bc0-4557-fe8c-97526b8a3c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.argmax(validation_accuracy_track)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "oDevMH2N3JZu",
        "colab_type": "code",
        "outputId": "7dfbef28-db6b-48e7-de01-dc04a11dafe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "best_accuracy_valid"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "uhbuJEM0-sVm",
        "colab_type": "code",
        "outputId": "669432d7-49b9-401a-98b6-5dea41181dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './DNAAdamBased')\n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./DNAAdamBased\n",
            "Validation Accuracy = 95.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D5okZYa4CSqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import spline\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ou2-UqDCXZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, 4861, print_every)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6G17qZY_Pxq",
        "colab_type": "code",
        "outputId": "b9b7cbe0-3b4e-432d-9af1-fa7234f2ecca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(validation_accuracy_track)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "sNS-YE0XCg4s",
        "colab_type": "code",
        "outputId": "cc3df157-87e8-46cc-839b-cffb1e485cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "# plt.plot( savgol_filter(np.asarray(validation_accuracy_track),51,1))\n",
        "plt.plot( (validation_accuracy_track))\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9c918ecc88>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA4KZmQEOCGZmljggmJkZAP8fw4HXmmGE3MIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BBlFJwfn-45J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "7f535483-cbde-4d35-b878-89aa6c3e0bf3"
      },
      "cell_type": "code",
      "source": [
        "plt.plot( (connection_probability_track))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9c918b4400>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS9ICMtwlaQH9fwgR9PNoHuj6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MCKAiyEFCg2F",
        "colab_type": "code",
        "outputId": "38fbcc52-7335-4971-edd9-f107c01feaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "steps_plot = epoch_track# [step for step in range(0, 2291, print_every)]\n",
        "# plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "# plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "plt.plot(steps_plot, np.asarray(train_accuracy_track))  \n",
        "plt.plot(steps_plot, validation_accuracy_track)\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEYCAYAAAAwH9PuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nisEmSWoUg02S1CgGmySpUQw2SVKj/H9p8GF7HgPdYAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u7hYf8U_CXWh",
        "colab_type": "code",
        "outputId": "4114d9ed-c684-442e-fdde-887264eac7f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "valid_accuracy_filtered = validation_accuracy_track\n",
        "print(max(valid_accuracy_filtered))\n",
        "valid_accuracy_filtered_np = np.asarray(valid_accuracy_filtered)\n",
        "print(np.argmax(valid_accuracy_filtered))\n",
        "print(steps_plot[np.argmax(valid_accuracy_filtered)])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95.0\n",
            "43\n",
            "430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKucNn6bEZzn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lrwM4UUCXTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('DNAReducedDatasetMomentADAM_ProbabilityBasedValid95.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "                                       'train_accuracy_track':train_accuracy_track,\n",
        "                                       'connection_probability_track':connection_probability_track,\n",
        "                                       'epochTrack':epoch_track, 'TestAcc':'test_accuracy',\n",
        "                                                         'BestValidation':best_accuracy_valid})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFCL8d7-CIxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now  retrain til 181 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "qoqMxUbZCIRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 1\n",
        "aside_valid_test = train_valid_combined[-aside_examples:]\n",
        "aside_valid_test_label = validation_test_label_one_hot[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = validation_test_label_one_hot[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5n7bYp9-FCn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puoBTQ3fCIOT",
        "colab_type": "code",
        "outputId": "2326e69e-354e-403b-c219-1395a1e6dea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4318
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(combined_train_valid, combined_train_valid_label)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: aside_valid_test,y:aside_valid_test_label, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "#             saver.save(sess, './PendigitSGDBased')\n",
        "    saver.save(sess, './DNAAdamBasedAllPass')   \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 98.54928\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.71421427\n",
            "\n",
            "Train Accuracy = 99.94997\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.024691546\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.00085362676\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.9511264e-05\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.0202523e-06\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.527178e-08\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.2194024e-09\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.2156716e-11\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.457426e-12\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 100.00000\n",
            "5.0385572e-14\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.7419111e-15\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.022069e-17\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.081927e-18\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 100.00000\n",
            "7.197559e-20\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.4883126e-21\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 100.00000\n",
            "8.6024985e-23\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.9740232e-24\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.02816796e-25\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.5545433e-27\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.2288633e-28\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 201 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.2483803e-30\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 211 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.4687339e-31\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 221 ...\n",
            "Validation Accuracy = 100.00000\n",
            "5.0776518e-33\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 231 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.7554266e-34\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 241 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.068794e-36\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 251 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.0980801e-37\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 261 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 271 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 281 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 291 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 301 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 311 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 321 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 331 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 341 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 351 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 361 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 371 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 381 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 391 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 401 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 411 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 421 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 431 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 441 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 451 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 461 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 471 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 481 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 100.00000\n",
            "EPOCH 491 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ImtSTCNJF9E_",
        "colab_type": "code",
        "outputId": "bb7e4509-0676-42f5-a75b-17462a0f61c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './DNAAdamBasedAllPass')\n",
        "    saver.save(sess, './DNAReducedAdamBasedAllPass')  \n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./DNAAdamBasedAllPass\n",
            "Validation Accuracy = 100.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FVMAeXpCIL8",
        "colab_type": "code",
        "outputId": "5b12f6d4-981b-4d1a-a192-d9e42d496bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Without all pass\n",
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './DNAReducedAdamBasedAllPass')\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "    print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./DNAReducedAdamBasedAllPass\n",
            "Test Accuracy = 94.856659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VGUWHQR3CIJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HXCpiAuCIGT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGYzlt7KCIDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hr57zkSqCIAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q08ZXCdr-sTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Without all pass\n",
        "# with tf.Session() as sess:\n",
        "# #     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "#     saver.restore(sess, './PendigitSGDBased')\n",
        "#     test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "#     print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9z1P1DG-sQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW6V7O1e-sNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochTrack = epoch_track\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r7lCgbXR3JXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFfzfjOB3JTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}