{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PendigitFullTest97p4DatasetMomentSGD_ProbabilityBased04282019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preritt/Embed/blob/master/PendigitFullTest97p4DatasetMomentSGD_ProbabilityBased04282019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qWL1XvRKt0EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4rTq2SzhyZgD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
        "# X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "# X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "# X_test, y_test             = mnist.test.images, mnist.test.labels\n",
        "\n",
        "# assert(len(X_train) == len(y_train))\n",
        "# assert(len(X_validation) == len(y_validation))\n",
        "# assert(len(X_test) == len(y_test))\n",
        "\n",
        "# print()\n",
        "# print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "# print()\n",
        "# print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "# print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "# print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lopHk729ydu5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_data = X_train.reshape(X_train.shape[0],-1)\n",
        "# train_label = y_train\n",
        "# validation_data = X_validation.reshape(X_validation.shape[0],-1)\n",
        "# validation_label = y_validation\n",
        "# test_data = X_test.reshape(X_test.shape[0],-1)\n",
        "# test_label = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yuoNYnTshaZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = np.load('X_train.npy')\n",
        "train_label = np.load('y_train.npy')\n",
        "validation_data = np.load('X_validation.npy')\n",
        "validation_label = np.load('y_validation.npy')\n",
        "test_data = np.load('X_test.npy')\n",
        "test_label = np.load('y_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4FSwFTChgdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "train_valid_label = np.concatenate((train_label, validation_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VloppGYRyrEX",
        "colab_type": "code",
        "outputId": "f0c99e69-57cb-411e-a67f-cca5051a0fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8670
        }
      },
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(80, 40, ), max_iter=500, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
        "                    learning_rate_init=.01)\n",
        "\n",
        "\n",
        "clf.fit(train_data, train_label)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.95489119\n",
            "Iteration 2, loss = 0.98170180\n",
            "Iteration 3, loss = 0.50873149\n",
            "Iteration 4, loss = 0.33459205\n",
            "Iteration 5, loss = 0.25779496\n",
            "Iteration 6, loss = 0.21278694\n",
            "Iteration 7, loss = 0.18346620\n",
            "Iteration 8, loss = 0.15914957\n",
            "Iteration 9, loss = 0.14152727\n",
            "Iteration 10, loss = 0.12701235\n",
            "Iteration 11, loss = 0.11560525\n",
            "Iteration 12, loss = 0.10628861\n",
            "Iteration 13, loss = 0.09794105\n",
            "Iteration 14, loss = 0.09102073\n",
            "Iteration 15, loss = 0.08553084\n",
            "Iteration 16, loss = 0.08047449\n",
            "Iteration 17, loss = 0.07596875\n",
            "Iteration 18, loss = 0.07220618\n",
            "Iteration 19, loss = 0.06878648\n",
            "Iteration 20, loss = 0.06588877\n",
            "Iteration 21, loss = 0.06339299\n",
            "Iteration 22, loss = 0.06088005\n",
            "Iteration 23, loss = 0.05833481\n",
            "Iteration 24, loss = 0.05645611\n",
            "Iteration 25, loss = 0.05408837\n",
            "Iteration 26, loss = 0.05232453\n",
            "Iteration 27, loss = 0.05047872\n",
            "Iteration 28, loss = 0.04933457\n",
            "Iteration 29, loss = 0.04746194\n",
            "Iteration 30, loss = 0.04640093\n",
            "Iteration 31, loss = 0.04464777\n",
            "Iteration 32, loss = 0.04361155\n",
            "Iteration 33, loss = 0.04231529\n",
            "Iteration 34, loss = 0.04125936\n",
            "Iteration 35, loss = 0.04053062\n",
            "Iteration 36, loss = 0.03940358\n",
            "Iteration 37, loss = 0.03834965\n",
            "Iteration 38, loss = 0.03753469\n",
            "Iteration 39, loss = 0.03654027\n",
            "Iteration 40, loss = 0.03572254\n",
            "Iteration 41, loss = 0.03497387\n",
            "Iteration 42, loss = 0.03433652\n",
            "Iteration 43, loss = 0.03348544\n",
            "Iteration 44, loss = 0.03283693\n",
            "Iteration 45, loss = 0.03218934\n",
            "Iteration 46, loss = 0.03149528\n",
            "Iteration 47, loss = 0.03094953\n",
            "Iteration 48, loss = 0.03041304\n",
            "Iteration 49, loss = 0.02985878\n",
            "Iteration 50, loss = 0.02926056\n",
            "Iteration 51, loss = 0.02890535\n",
            "Iteration 52, loss = 0.02831707\n",
            "Iteration 53, loss = 0.02764853\n",
            "Iteration 54, loss = 0.02736471\n",
            "Iteration 55, loss = 0.02673735\n",
            "Iteration 56, loss = 0.02637397\n",
            "Iteration 57, loss = 0.02589611\n",
            "Iteration 58, loss = 0.02558105\n",
            "Iteration 59, loss = 0.02499821\n",
            "Iteration 60, loss = 0.02478820\n",
            "Iteration 61, loss = 0.02425511\n",
            "Iteration 62, loss = 0.02397880\n",
            "Iteration 63, loss = 0.02371509\n",
            "Iteration 64, loss = 0.02320203\n",
            "Iteration 65, loss = 0.02280332\n",
            "Iteration 66, loss = 0.02251338\n",
            "Iteration 67, loss = 0.02227458\n",
            "Iteration 68, loss = 0.02191484\n",
            "Iteration 69, loss = 0.02143715\n",
            "Iteration 70, loss = 0.02130694\n",
            "Iteration 71, loss = 0.02096261\n",
            "Iteration 72, loss = 0.02074336\n",
            "Iteration 73, loss = 0.02036040\n",
            "Iteration 74, loss = 0.01999286\n",
            "Iteration 75, loss = 0.01976113\n",
            "Iteration 76, loss = 0.01964329\n",
            "Iteration 77, loss = 0.01946378\n",
            "Iteration 78, loss = 0.01919776\n",
            "Iteration 79, loss = 0.01877982\n",
            "Iteration 80, loss = 0.01851510\n",
            "Iteration 81, loss = 0.01831039\n",
            "Iteration 82, loss = 0.01810026\n",
            "Iteration 83, loss = 0.01805545\n",
            "Iteration 84, loss = 0.01765292\n",
            "Iteration 85, loss = 0.01760867\n",
            "Iteration 86, loss = 0.01721313\n",
            "Iteration 87, loss = 0.01696317\n",
            "Iteration 88, loss = 0.01687569\n",
            "Iteration 89, loss = 0.01657610\n",
            "Iteration 90, loss = 0.01636889\n",
            "Iteration 91, loss = 0.01614179\n",
            "Iteration 92, loss = 0.01606604\n",
            "Iteration 93, loss = 0.01585744\n",
            "Iteration 94, loss = 0.01567951\n",
            "Iteration 95, loss = 0.01550237\n",
            "Iteration 96, loss = 0.01521800\n",
            "Iteration 97, loss = 0.01512489\n",
            "Iteration 98, loss = 0.01492536\n",
            "Iteration 99, loss = 0.01469476\n",
            "Iteration 100, loss = 0.01461964\n",
            "Iteration 101, loss = 0.01457004\n",
            "Iteration 102, loss = 0.01431109\n",
            "Iteration 103, loss = 0.01412683\n",
            "Iteration 104, loss = 0.01398944\n",
            "Iteration 105, loss = 0.01378379\n",
            "Iteration 106, loss = 0.01364602\n",
            "Iteration 107, loss = 0.01360926\n",
            "Iteration 108, loss = 0.01347256\n",
            "Iteration 109, loss = 0.01331756\n",
            "Iteration 110, loss = 0.01312382\n",
            "Iteration 111, loss = 0.01295702\n",
            "Iteration 112, loss = 0.01286147\n",
            "Iteration 113, loss = 0.01276808\n",
            "Iteration 114, loss = 0.01260671\n",
            "Iteration 115, loss = 0.01249043\n",
            "Iteration 116, loss = 0.01247270\n",
            "Iteration 117, loss = 0.01230549\n",
            "Iteration 118, loss = 0.01221680\n",
            "Iteration 119, loss = 0.01207492\n",
            "Iteration 120, loss = 0.01196955\n",
            "Iteration 121, loss = 0.01197146\n",
            "Iteration 122, loss = 0.01170303\n",
            "Iteration 123, loss = 0.01161692\n",
            "Iteration 124, loss = 0.01153516\n",
            "Iteration 125, loss = 0.01145848\n",
            "Iteration 126, loss = 0.01133453\n",
            "Iteration 127, loss = 0.01119464\n",
            "Iteration 128, loss = 0.01112531\n",
            "Iteration 129, loss = 0.01105446\n",
            "Iteration 130, loss = 0.01099228\n",
            "Iteration 131, loss = 0.01089062\n",
            "Iteration 132, loss = 0.01071887\n",
            "Iteration 133, loss = 0.01065959\n",
            "Iteration 134, loss = 0.01060974\n",
            "Iteration 135, loss = 0.01056412\n",
            "Iteration 136, loss = 0.01042939\n",
            "Iteration 137, loss = 0.01034429\n",
            "Iteration 138, loss = 0.01023560\n",
            "Iteration 139, loss = 0.01021425\n",
            "Iteration 140, loss = 0.01011674\n",
            "Iteration 141, loss = 0.01008340\n",
            "Iteration 142, loss = 0.00993935\n",
            "Iteration 143, loss = 0.00991796\n",
            "Iteration 144, loss = 0.00977318\n",
            "Iteration 145, loss = 0.00978138\n",
            "Iteration 146, loss = 0.00967579\n",
            "Iteration 147, loss = 0.00962418\n",
            "Iteration 148, loss = 0.00965236\n",
            "Iteration 149, loss = 0.00946203\n",
            "Iteration 150, loss = 0.00942940\n",
            "Iteration 151, loss = 0.00927938\n",
            "Iteration 152, loss = 0.00921360\n",
            "Iteration 153, loss = 0.00917654\n",
            "Iteration 154, loss = 0.00909055\n",
            "Iteration 155, loss = 0.00899737\n",
            "Iteration 156, loss = 0.00901189\n",
            "Iteration 157, loss = 0.00897226\n",
            "Iteration 158, loss = 0.00892190\n",
            "Iteration 159, loss = 0.00878435\n",
            "Iteration 160, loss = 0.00877368\n",
            "Iteration 161, loss = 0.00876143\n",
            "Iteration 162, loss = 0.00861978\n",
            "Iteration 163, loss = 0.00851081\n",
            "Iteration 164, loss = 0.00851410\n",
            "Iteration 165, loss = 0.00844975\n",
            "Iteration 166, loss = 0.00838342\n",
            "Iteration 167, loss = 0.00837549\n",
            "Iteration 168, loss = 0.00828231\n",
            "Iteration 169, loss = 0.00817145\n",
            "Iteration 170, loss = 0.00816321\n",
            "Iteration 171, loss = 0.00808684\n",
            "Iteration 172, loss = 0.00806949\n",
            "Iteration 173, loss = 0.00800791\n",
            "Iteration 174, loss = 0.00796241\n",
            "Iteration 175, loss = 0.00796155\n",
            "Iteration 176, loss = 0.00788167\n",
            "Iteration 177, loss = 0.00788849\n",
            "Iteration 178, loss = 0.00779037\n",
            "Iteration 179, loss = 0.00774054\n",
            "Iteration 180, loss = 0.00771362\n",
            "Iteration 181, loss = 0.00765176\n",
            "Iteration 182, loss = 0.00760335\n",
            "Iteration 183, loss = 0.00757704\n",
            "Iteration 184, loss = 0.00752110\n",
            "Iteration 185, loss = 0.00749897\n",
            "Iteration 186, loss = 0.00748372\n",
            "Iteration 187, loss = 0.00739768\n",
            "Iteration 188, loss = 0.00737927\n",
            "Iteration 189, loss = 0.00728785\n",
            "Iteration 190, loss = 0.00731329\n",
            "Iteration 191, loss = 0.00724249\n",
            "Iteration 192, loss = 0.00719424\n",
            "Iteration 193, loss = 0.00716928\n",
            "Iteration 194, loss = 0.00713646\n",
            "Iteration 195, loss = 0.00705879\n",
            "Iteration 196, loss = 0.00699678\n",
            "Iteration 197, loss = 0.00704082\n",
            "Iteration 198, loss = 0.00694456\n",
            "Iteration 199, loss = 0.00693006\n",
            "Iteration 200, loss = 0.00703286\n",
            "Iteration 201, loss = 0.00683479\n",
            "Iteration 202, loss = 0.00679797\n",
            "Iteration 203, loss = 0.00681204\n",
            "Iteration 204, loss = 0.00674059\n",
            "Iteration 205, loss = 0.00681136\n",
            "Iteration 206, loss = 0.00673915\n",
            "Iteration 207, loss = 0.00664000\n",
            "Iteration 208, loss = 0.00673062\n",
            "Iteration 209, loss = 0.00658162\n",
            "Iteration 210, loss = 0.00653296\n",
            "Iteration 211, loss = 0.00651722\n",
            "Iteration 212, loss = 0.00649511\n",
            "Iteration 213, loss = 0.00642087\n",
            "Iteration 214, loss = 0.00637117\n",
            "Iteration 215, loss = 0.00632132\n",
            "Iteration 216, loss = 0.00625817\n",
            "Iteration 217, loss = 0.00630890\n",
            "Iteration 218, loss = 0.00628571\n",
            "Iteration 219, loss = 0.00623650\n",
            "Iteration 220, loss = 0.00622738\n",
            "Iteration 221, loss = 0.00617780\n",
            "Iteration 222, loss = 0.00612142\n",
            "Iteration 223, loss = 0.00613518\n",
            "Iteration 224, loss = 0.00606616\n",
            "Iteration 225, loss = 0.00604892\n",
            "Iteration 226, loss = 0.00604637\n",
            "Iteration 227, loss = 0.00600258\n",
            "Iteration 228, loss = 0.00594099\n",
            "Iteration 229, loss = 0.00595039\n",
            "Iteration 230, loss = 0.00590738\n",
            "Iteration 231, loss = 0.00588093\n",
            "Iteration 232, loss = 0.00584840\n",
            "Iteration 233, loss = 0.00589290\n",
            "Iteration 234, loss = 0.00582259\n",
            "Iteration 235, loss = 0.00577467\n",
            "Iteration 236, loss = 0.00573641\n",
            "Iteration 237, loss = 0.00575422\n",
            "Iteration 238, loss = 0.00570247\n",
            "Iteration 239, loss = 0.00564626\n",
            "Iteration 240, loss = 0.00564861\n",
            "Iteration 241, loss = 0.00565070\n",
            "Iteration 242, loss = 0.00571307\n",
            "Iteration 243, loss = 0.00554996\n",
            "Iteration 244, loss = 0.00558301\n",
            "Iteration 245, loss = 0.00555869\n",
            "Iteration 246, loss = 0.00549021\n",
            "Iteration 247, loss = 0.00546189\n",
            "Iteration 248, loss = 0.00544382\n",
            "Iteration 249, loss = 0.00544870\n",
            "Iteration 250, loss = 0.00549972\n",
            "Iteration 251, loss = 0.00536773\n",
            "Iteration 252, loss = 0.00537383\n",
            "Iteration 253, loss = 0.00531100\n",
            "Iteration 254, loss = 0.00527442\n",
            "Iteration 255, loss = 0.00527742\n",
            "Iteration 256, loss = 0.00528258\n",
            "Iteration 257, loss = 0.00528959\n",
            "Iteration 258, loss = 0.00523992\n",
            "Iteration 259, loss = 0.00533636\n",
            "Iteration 260, loss = 0.00516047\n",
            "Iteration 261, loss = 0.00515355\n",
            "Iteration 262, loss = 0.00514762\n",
            "Iteration 263, loss = 0.00509594\n",
            "Iteration 264, loss = 0.00514277\n",
            "Iteration 265, loss = 0.00503560\n",
            "Iteration 266, loss = 0.00511873\n",
            "Iteration 267, loss = 0.00502775\n",
            "Iteration 268, loss = 0.00505321\n",
            "Iteration 269, loss = 0.00498597\n",
            "Iteration 270, loss = 0.00496915\n",
            "Iteration 271, loss = 0.00493941\n",
            "Iteration 272, loss = 0.00493335\n",
            "Iteration 273, loss = 0.00497301\n",
            "Iteration 274, loss = 0.00490693\n",
            "Iteration 275, loss = 0.00492691\n",
            "Iteration 276, loss = 0.00486849\n",
            "Iteration 277, loss = 0.00486416\n",
            "Iteration 278, loss = 0.00481117\n",
            "Iteration 279, loss = 0.00484692\n",
            "Iteration 280, loss = 0.00488550\n",
            "Iteration 281, loss = 0.00475579\n",
            "Iteration 282, loss = 0.00474359\n",
            "Iteration 283, loss = 0.00475242\n",
            "Iteration 284, loss = 0.00473396\n",
            "Iteration 285, loss = 0.00470902\n",
            "Iteration 286, loss = 0.00468909\n",
            "Iteration 287, loss = 0.00467066\n",
            "Iteration 288, loss = 0.00466165\n",
            "Iteration 289, loss = 0.00461290\n",
            "Iteration 290, loss = 0.00462931\n",
            "Iteration 291, loss = 0.00462332\n",
            "Iteration 292, loss = 0.00455671\n",
            "Iteration 293, loss = 0.00457927\n",
            "Iteration 294, loss = 0.00451369\n",
            "Iteration 295, loss = 0.00449999\n",
            "Iteration 296, loss = 0.00447554\n",
            "Iteration 297, loss = 0.00445528\n",
            "Iteration 298, loss = 0.00452140\n",
            "Iteration 299, loss = 0.00449118\n",
            "Iteration 300, loss = 0.00442181\n",
            "Iteration 301, loss = 0.00445303\n",
            "Iteration 302, loss = 0.00443806\n",
            "Iteration 303, loss = 0.00443319\n",
            "Iteration 304, loss = 0.00437559\n",
            "Iteration 305, loss = 0.00440870\n",
            "Iteration 306, loss = 0.00434778\n",
            "Iteration 307, loss = 0.00431126\n",
            "Iteration 308, loss = 0.00430928\n",
            "Iteration 309, loss = 0.00427581\n",
            "Iteration 310, loss = 0.00427756\n",
            "Iteration 311, loss = 0.00427304\n",
            "Iteration 312, loss = 0.00426384\n",
            "Iteration 313, loss = 0.00422628\n",
            "Iteration 314, loss = 0.00420066\n",
            "Iteration 315, loss = 0.00427019\n",
            "Iteration 316, loss = 0.00419086\n",
            "Iteration 317, loss = 0.00419690\n",
            "Iteration 318, loss = 0.00416257\n",
            "Iteration 319, loss = 0.00416440\n",
            "Iteration 320, loss = 0.00416966\n",
            "Iteration 321, loss = 0.00414175\n",
            "Iteration 322, loss = 0.00412208\n",
            "Iteration 323, loss = 0.00407990\n",
            "Iteration 324, loss = 0.00405518\n",
            "Iteration 325, loss = 0.00403128\n",
            "Iteration 326, loss = 0.00404249\n",
            "Iteration 327, loss = 0.00399875\n",
            "Iteration 328, loss = 0.00399386\n",
            "Iteration 329, loss = 0.00402780\n",
            "Iteration 330, loss = 0.00400091\n",
            "Iteration 331, loss = 0.00396788\n",
            "Iteration 332, loss = 0.00399715\n",
            "Iteration 333, loss = 0.00394555\n",
            "Iteration 334, loss = 0.00396941\n",
            "Iteration 335, loss = 0.00393710\n",
            "Iteration 336, loss = 0.00397572\n",
            "Iteration 337, loss = 0.00385372\n",
            "Iteration 338, loss = 0.00396587\n",
            "Iteration 339, loss = 0.00387148\n",
            "Iteration 340, loss = 0.00388042\n",
            "Iteration 341, loss = 0.00388566\n",
            "Iteration 342, loss = 0.00385984\n",
            "Iteration 343, loss = 0.00386238\n",
            "Iteration 344, loss = 0.00382927\n",
            "Iteration 345, loss = 0.00380803\n",
            "Iteration 346, loss = 0.00381661\n",
            "Iteration 347, loss = 0.00380419\n",
            "Iteration 348, loss = 0.00378038\n",
            "Iteration 349, loss = 0.00374509\n",
            "Iteration 350, loss = 0.00372517\n",
            "Iteration 351, loss = 0.00373168\n",
            "Iteration 352, loss = 0.00374224\n",
            "Iteration 353, loss = 0.00369326\n",
            "Iteration 354, loss = 0.00372348\n",
            "Iteration 355, loss = 0.00375712\n",
            "Iteration 356, loss = 0.00368898\n",
            "Iteration 357, loss = 0.00367743\n",
            "Iteration 358, loss = 0.00366758\n",
            "Iteration 359, loss = 0.00365088\n",
            "Iteration 360, loss = 0.00368836\n",
            "Iteration 361, loss = 0.00365602\n",
            "Iteration 362, loss = 0.00362505\n",
            "Iteration 363, loss = 0.00362486\n",
            "Iteration 364, loss = 0.00356480\n",
            "Iteration 365, loss = 0.00359288\n",
            "Iteration 366, loss = 0.00363284\n",
            "Iteration 367, loss = 0.00356600\n",
            "Iteration 368, loss = 0.00358883\n",
            "Iteration 369, loss = 0.00354849\n",
            "Iteration 370, loss = 0.00356038\n",
            "Iteration 371, loss = 0.00357143\n",
            "Iteration 372, loss = 0.00350776\n",
            "Iteration 373, loss = 0.00348075\n",
            "Iteration 374, loss = 0.00347811\n",
            "Iteration 375, loss = 0.00346571\n",
            "Iteration 376, loss = 0.00348707\n",
            "Iteration 377, loss = 0.00344693\n",
            "Iteration 378, loss = 0.00347096\n",
            "Iteration 379, loss = 0.00342289\n",
            "Iteration 380, loss = 0.00342945\n",
            "Iteration 381, loss = 0.00337333\n",
            "Iteration 382, loss = 0.00343206\n",
            "Iteration 383, loss = 0.00343639\n",
            "Iteration 384, loss = 0.00339029\n",
            "Iteration 385, loss = 0.00337384\n",
            "Iteration 386, loss = 0.00338089\n",
            "Iteration 387, loss = 0.00335513\n",
            "Iteration 388, loss = 0.00338561\n",
            "Iteration 389, loss = 0.00342000\n",
            "Iteration 390, loss = 0.00333907\n",
            "Iteration 391, loss = 0.00335668\n",
            "Iteration 392, loss = 0.00330550\n",
            "Iteration 393, loss = 0.00330339\n",
            "Iteration 394, loss = 0.00332740\n",
            "Iteration 395, loss = 0.00327492\n",
            "Iteration 396, loss = 0.00331883\n",
            "Iteration 397, loss = 0.00324529\n",
            "Iteration 398, loss = 0.00329314\n",
            "Iteration 399, loss = 0.00324244\n",
            "Iteration 400, loss = 0.00324618\n",
            "Iteration 401, loss = 0.00324587\n",
            "Iteration 402, loss = 0.00323059\n",
            "Iteration 403, loss = 0.00327293\n",
            "Iteration 404, loss = 0.00322800\n",
            "Iteration 405, loss = 0.00325070\n",
            "Iteration 406, loss = 0.00319387\n",
            "Iteration 407, loss = 0.00322651\n",
            "Iteration 408, loss = 0.00318656\n",
            "Iteration 409, loss = 0.00317515\n",
            "Iteration 410, loss = 0.00320850\n",
            "Iteration 411, loss = 0.00317645\n",
            "Iteration 412, loss = 0.00316274\n",
            "Iteration 413, loss = 0.00315269\n",
            "Iteration 414, loss = 0.00314416\n",
            "Iteration 415, loss = 0.00312457\n",
            "Iteration 416, loss = 0.00312417\n",
            "Iteration 417, loss = 0.00312532\n",
            "Iteration 418, loss = 0.00312577\n",
            "Iteration 419, loss = 0.00312253\n",
            "Iteration 420, loss = 0.00311638\n",
            "Iteration 421, loss = 0.00309282\n",
            "Iteration 422, loss = 0.00306576\n",
            "Iteration 423, loss = 0.00305618\n",
            "Iteration 424, loss = 0.00309111\n",
            "Iteration 425, loss = 0.00308422\n",
            "Iteration 426, loss = 0.00304633\n",
            "Iteration 427, loss = 0.00307799\n",
            "Iteration 428, loss = 0.00302185\n",
            "Iteration 429, loss = 0.00302186\n",
            "Iteration 430, loss = 0.00303481\n",
            "Iteration 431, loss = 0.00302606\n",
            "Iteration 432, loss = 0.00299016\n",
            "Iteration 433, loss = 0.00298981\n",
            "Iteration 434, loss = 0.00297354\n",
            "Iteration 435, loss = 0.00299139\n",
            "Iteration 436, loss = 0.00297133\n",
            "Iteration 437, loss = 0.00300404\n",
            "Iteration 438, loss = 0.00295802\n",
            "Iteration 439, loss = 0.00296736\n",
            "Iteration 440, loss = 0.00291530\n",
            "Iteration 441, loss = 0.00298328\n",
            "Iteration 442, loss = 0.00292571\n",
            "Iteration 443, loss = 0.00300233\n",
            "Iteration 444, loss = 0.00291312\n",
            "Iteration 445, loss = 0.00297511\n",
            "Iteration 446, loss = 0.00294667\n",
            "Iteration 447, loss = 0.00294276\n",
            "Iteration 448, loss = 0.00288473\n",
            "Iteration 449, loss = 0.00285166\n",
            "Iteration 450, loss = 0.00290582\n",
            "Iteration 451, loss = 0.00288795\n",
            "Iteration 452, loss = 0.00293587\n",
            "Iteration 453, loss = 0.00289487\n",
            "Iteration 454, loss = 0.00285619\n",
            "Iteration 455, loss = 0.00285278\n",
            "Iteration 456, loss = 0.00285270\n",
            "Iteration 457, loss = 0.00288030\n",
            "Iteration 458, loss = 0.00280499\n",
            "Iteration 459, loss = 0.00282446\n",
            "Iteration 460, loss = 0.00284159\n",
            "Iteration 461, loss = 0.00280711\n",
            "Iteration 462, loss = 0.00280588\n",
            "Iteration 463, loss = 0.00282300\n",
            "Iteration 464, loss = 0.00280240\n",
            "Iteration 465, loss = 0.00279544\n",
            "Iteration 466, loss = 0.00275683\n",
            "Iteration 467, loss = 0.00281095\n",
            "Iteration 468, loss = 0.00284461\n",
            "Iteration 469, loss = 0.00281742\n",
            "Iteration 470, loss = 0.00273885\n",
            "Iteration 471, loss = 0.00277373\n",
            "Iteration 472, loss = 0.00272039\n",
            "Iteration 473, loss = 0.00273633\n",
            "Iteration 474, loss = 0.00272530\n",
            "Iteration 475, loss = 0.00269705\n",
            "Iteration 476, loss = 0.00273527\n",
            "Iteration 477, loss = 0.00270789\n",
            "Iteration 478, loss = 0.00270207\n",
            "Iteration 479, loss = 0.00279358\n",
            "Iteration 480, loss = 0.00278503\n",
            "Iteration 481, loss = 0.00268502\n",
            "Iteration 482, loss = 0.00267545\n",
            "Iteration 483, loss = 0.00268753\n",
            "Iteration 484, loss = 0.00271023\n",
            "Iteration 485, loss = 0.00266200\n",
            "Iteration 486, loss = 0.00264580\n",
            "Iteration 487, loss = 0.00266053\n",
            "Iteration 488, loss = 0.00267585\n",
            "Iteration 489, loss = 0.00270909\n",
            "Iteration 490, loss = 0.00267350\n",
            "Iteration 491, loss = 0.00262176\n",
            "Iteration 492, loss = 0.00261702\n",
            "Iteration 493, loss = 0.00262449\n",
            "Iteration 494, loss = 0.00258929\n",
            "Iteration 495, loss = 0.00262050\n",
            "Iteration 496, loss = 0.00258803\n",
            "Iteration 497, loss = 0.00262777\n",
            "Iteration 498, loss = 0.00256719\n",
            "Iteration 499, loss = 0.00255993\n",
            "Iteration 500, loss = 0.00257754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(80, 40), learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=1e-05,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "8m9_X9bUdZJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_valid_combined = np.concatenate((train_data, validation_data))\n",
        "# train_valid_label = np.concatenate((train_label, validation_label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo_lFxdIc85n",
        "colab_type": "code",
        "outputId": "82e85b2c-47a8-43c3-8569-d2c8e00c27a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1173
        }
      },
      "cell_type": "code",
      "source": [
        "# clf =MLPClassifier(hidden_layer_sizes=(300,100, ), max_iter=200, alpha=1e-4,\n",
        "#                     solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
        "#                     learning_rate_init=.1)\n",
        "### acc is 98.41\n",
        "clf2 = MLPClassifier(hidden_layer_sizes=(300,100,), max_iter=200, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "# Test set score: 0.950119\n",
        "\n",
        "clf2.fit(train_valid_combined, train_valid_label)\n",
        "# clf2.fit(train_data, train_label)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.54899152\n",
            "Iteration 2, loss = 0.06616612\n",
            "Iteration 3, loss = 0.04279345\n",
            "Iteration 4, loss = 0.03166336\n",
            "Iteration 5, loss = 0.02479369\n",
            "Iteration 6, loss = 0.02170618\n",
            "Iteration 7, loss = 0.01858626\n",
            "Iteration 8, loss = 0.01581559\n",
            "Iteration 9, loss = 0.01390799\n",
            "Iteration 10, loss = 0.01265809\n",
            "Iteration 11, loss = 0.01127673\n",
            "Iteration 12, loss = 0.00996930\n",
            "Iteration 13, loss = 0.00956242\n",
            "Iteration 14, loss = 0.00903562\n",
            "Iteration 15, loss = 0.00835131\n",
            "Iteration 16, loss = 0.00850045\n",
            "Iteration 17, loss = 0.00744452\n",
            "Iteration 18, loss = 0.00710270\n",
            "Iteration 19, loss = 0.00679402\n",
            "Iteration 20, loss = 0.00656596\n",
            "Iteration 21, loss = 0.00575884\n",
            "Iteration 22, loss = 0.00510601\n",
            "Iteration 23, loss = 0.00508025\n",
            "Iteration 24, loss = 0.00522847\n",
            "Iteration 25, loss = 0.00518619\n",
            "Iteration 26, loss = 0.00499809\n",
            "Iteration 27, loss = 0.00462652\n",
            "Iteration 28, loss = 0.00430399\n",
            "Iteration 29, loss = 0.00405920\n",
            "Iteration 30, loss = 0.00449292\n",
            "Iteration 31, loss = 0.00388696\n",
            "Iteration 32, loss = 0.00381280\n",
            "Iteration 33, loss = 0.00355093\n",
            "Iteration 34, loss = 0.00384297\n",
            "Iteration 35, loss = 0.00370512\n",
            "Iteration 36, loss = 0.00326534\n",
            "Iteration 37, loss = 0.00390881\n",
            "Iteration 38, loss = 0.00309825\n",
            "Iteration 39, loss = 0.00313235\n",
            "Iteration 40, loss = 0.00316818\n",
            "Iteration 41, loss = 0.00308970\n",
            "Iteration 42, loss = 0.00278589\n",
            "Iteration 43, loss = 0.00309007\n",
            "Iteration 44, loss = 0.00278668\n",
            "Iteration 45, loss = 0.00268005\n",
            "Iteration 46, loss = 0.00268217\n",
            "Iteration 47, loss = 0.00271514\n",
            "Iteration 48, loss = 0.00259554\n",
            "Iteration 49, loss = 0.00240461\n",
            "Iteration 50, loss = 0.00263695\n",
            "Iteration 51, loss = 0.00245054\n",
            "Iteration 52, loss = 0.00231362\n",
            "Iteration 53, loss = 0.00246221\n",
            "Iteration 54, loss = 0.00262239\n",
            "Iteration 55, loss = 0.00235124\n",
            "Iteration 56, loss = 0.00263333\n",
            "Iteration 57, loss = 0.00231352\n",
            "Iteration 58, loss = 0.00240500\n",
            "Iteration 59, loss = 0.00244188\n",
            "Iteration 60, loss = 0.00229447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=(300, 100), learning_rate='constant',\n",
              "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "Wi_0y1C6e9Er",
        "colab_type": "code",
        "outputId": "fa2124c1-2623-4747-bf50-db43bbf71283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_valid_combined.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7494, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "oy5MNJqFys-H",
        "colab_type": "code",
        "outputId": "5e610cba-8c8c-4515-b66f-b33dd3924861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(train_data,train_label)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9996663886572144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "T0aiYNsdyuBR",
        "colab_type": "code",
        "outputId": "abeea735-148b-41e8-e606-2d01e960dcc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(validation_data,validation_label)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9953302201467645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "w7DSKjQcyvL0",
        "colab_type": "code",
        "outputId": "5c47eb2b-7d2b-4c2e-af7a-fdf2c41e2603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf.score(test_data,test_label)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736992567181246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "bOJLm3y6dkCs",
        "colab_type": "code",
        "outputId": "aae5866c-88b0-499a-eeb4-15fef873ddeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(train_data,train_label)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9996663886572144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "hnUFUI4rdjwi",
        "colab_type": "code",
        "outputId": "0ae3aa31-c2a7-4603-c5a5-5ddf94b95864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(validation_data,validation_label)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "1u7u5mLsirGr",
        "colab_type": "code",
        "outputId": "002de8c0-6cff-4a44-e96e-9e762281bc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5995, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "BtNN_G2Bdkpr",
        "colab_type": "code",
        "outputId": "af7c60f4-c561-4ca1-d784-3d3510f5fefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf2.score(test_data,test_label)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9722698684962836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "63SIF9YCiJ9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "enc.fit(train_label.reshape(1,-1).T)\n",
        "train_label_one_hot = enc.transform(train_label.reshape(-1,1))\n",
        "test_label_one_hot = enc.transform(test_label.reshape(-1,1))\n",
        "validation_label_one_hot = enc.transform(validation_label.reshape(-1,1))\n",
        "validation_test_label_one_hot = enc.transform(train_valid_label.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9T8IoNGpkVv_",
        "colab_type": "code",
        "outputId": "9378fd98-a7d8-4b57-ac70-c6d9527bd5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label_one_hot.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3498, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "kxWBf1tjiJm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    \n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYHOkr0CiOxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev, seed=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDauxfwliQZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define weights of the layer\n",
        "\n",
        "\n",
        "\n",
        "# saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GlClhIpViZoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Divide valid in two parts for validation and validation-test"
      ]
    },
    {
      "metadata": {
        "id": "NhtjHgUwl0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLrSUA61iQW_",
        "colab_type": "code",
        "outputId": "b0bda814-a1cc-40de-90b3-22d18321b4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(train_label_one_hot,axis = 1))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([624., 623., 624., 575., 624., 576., 576., 623., 575., 575.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6CWpc/8L8ki0lvD1ev0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XfNKrbvrmDl6",
        "colab_type": "code",
        "outputId": "193d5076-f2ab-4706-ef8c-420b23e04bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(np.argmax(validation_label_one_hot,axis = 1))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([156., 156., 156., 144., 156., 144., 144., 155., 144., 144.]),\n",
              " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWqcQS9JjTPoJalxBr0kNc6gl6TG/S/I+e4LEKaj1wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fX0eodcgiQTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYJZs7GgCy5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle  #train_data, train_label\n",
        "\n",
        "X_train, y_train = shuffle(train_data, train_label_one_hot)\n",
        "validation_data, validation_label_one_hot = validation_data, validation_label_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4WFIdiuCy53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup TensorFlow\n",
        "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
        "\n",
        "You do not need to modify this section."
      ]
    },
    {
      "metadata": {
        "id": "est-t83SCy55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vgad6Ny0OjqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "connection_probability = tf.Variable(.9999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZqUctyopqzZ",
        "colab_type": "code",
        "outputId": "c4a277d3-1a02-4504-f721-9002b2991164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "# print(G_W1.shape)\n",
        "# print(G_W2.shape)\n",
        "# print(G_W3.shape)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5995, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6aNutv83RcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define the network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "stGmwMYz8vws",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "G_W1 = tf.Variable(np.float32(clf.coefs_[0]))\n",
        "G_b1 = tf.Variable(np.float32(clf.intercepts_ [0]))\n",
        "\n",
        "G_W2 =  tf.Variable(np.float32(clf.coefs_[1]))\n",
        "G_b2 = tf.Variable(np.float32(clf.intercepts_ [1]))\n",
        "\n",
        "G_W3 =  tf.Variable(np.float32(clf.coefs_[2]))\n",
        "G_b3 = tf.Variable(np.float32(clf.intercepts_ [2]))\n",
        "\n",
        "G_w_out_h1 = tf.Variable(xavier_init([10,80]))\n",
        "G_b_out_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "G_w_h2_h1 = tf.Variable(xavier_init([40,80]))\n",
        "G_b_h2_h1 = tf.Variable(xavier_init([80]))\n",
        "\n",
        "\n",
        "G_w_h1_input = tf.Variable(xavier_init([80,16]))\n",
        "G_b_h1_input = tf.Variable(xavier_init([16]))\n",
        "\n",
        "\n",
        "G_w_input_h1_h2 = tf.Variable(xavier_init([16,40]))\n",
        "G_b_h1_input = tf.Variable(xavier_init([40]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sfSdtHU3JfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x, test_mode = False):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    layer_depth = {\n",
        "        'layer_1' : 6,\n",
        "        'layer_2' : 16,\n",
        "        'layer_3' : 120,\n",
        "        'layer_f1' : 84\n",
        "    }\n",
        "\n",
        "\n",
        "    \n",
        "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
        "    x_flat = flatten(x)\n",
        "    fc1 = flatten(x)\n",
        "    fdense = fc1\n",
        "    \n",
        "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    fc1_w = G_W1# tf.Variable(tf.truncated_normal(shape = (X_train.shape[1]*X_train.shape[2],300), mean = mu, stddev = sigma))\n",
        "    fc1_b = G_b1# tf.Variable(tf.zeros(300))\n",
        "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "    fc2_w = G_W2# tf.Variable(tf.truncated_normal(shape = (300,100), mean = mu, stddev = sigma))\n",
        "    fc2_b = G_b2# tf.Variable(tf.zeros(100))\n",
        "    fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
        "    # TODO: Activation.\n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    \n",
        "    \n",
        "    #################\n",
        "    ##### Inset probability connection from x to conv2\n",
        "    fc2p_w = tf.Variable(xavier_init([X_train.shape[1],clf.coefs_[1].shape[1]]))\n",
        "    fc2p_b = tf.Variable(xavier_init([clf.coefs_[1].shape[1]]))\n",
        "    fc2_2nd_input = tf.matmul(x_flat,fc2p_w) + fc2p_b\n",
        "    fc2_2nd_input = tf.nn.relu(fc2_2nd_input)\n",
        "    connect2 = tf.logical_and(tf.random.uniform(shape = tf.shape(connection_probability)) < connection_probability, tf.equal(test_mode,False))\n",
        "    fc2 = tf.cond(connect2,lambda: fc2 + fc2_2nd_input, lambda: fc2 )    \n",
        "    ################    \n",
        "    fc3_w = G_W3\n",
        "    fc3_b = G_b3\n",
        "    \n",
        "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
        "#     print(logits.shape)\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGmN34tg3_tv",
        "colab_type": "code",
        "outputId": "6dfe81e6-31c3-414c-dbf8-4501079614ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label_one_hot.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5995, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "-NX_lWUB6zue",
        "colab_type": "code",
        "outputId": "71da8cd3-b4ca-453b-bb63-9b3e033de078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_label"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 3, ..., 6, 3, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "M3U_MKYr34Xp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.name_scope('Input'):\n",
        "\n",
        "  x = tf.placeholder(tf.float32, (None, train_data.shape[1]), name='X')\n",
        "  y = tf.placeholder(\"float\", [None, 1+np.max(train_label)])\n",
        "# one_hot_y = tf.one_hot(y, train_label_one_hot.shape[1])\n",
        "is_testing= tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rtTKpeM4P8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-e6BG9DI3Jb3",
        "colab_type": "code",
        "outputId": "dee3c050-f07a-4725-d621-f1eba34833eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rate = 0.001\n",
        "decay_rate = 1.0005**(X_train.shape[0]/BATCH_SIZE);\n",
        "decay_rate = 1.2\n",
        "print(decay_rate)\n",
        "logits = LeNet(x,is_testing)\n",
        "with tf.name_scope('Train'):\n",
        "#   cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
        "#   loss_operation = tf.reduce_mean(cross_entropy, name='loss')\n",
        "  loss_operation = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=LeNet(x, test_mode=False), labels=y))\n",
        "  tf.summary.scalar('loss', loss_operation)\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate = rate,momentum=.9)\n",
        "# optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "# tf.train.natural_exp_decay()\n",
        "training_operation = optimizer.minimize(loss_operation)\n",
        "new_prob = connection_probability.assign(connection_probability/decay_rate)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8eQKHOw7PHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def evaluate(X_data, y_data):\n",
        "correct_pred = tf.equal(tf.argmax(LeNet(x,test_mode=True), 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tb-sFE34OGp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "# accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# saver = tf.train.Saver()\n",
        "\n",
        "# def evaluate(X_data, y_data):\n",
        "#     num_examples = len(X_data)\n",
        "#     total_accuracy = 0\n",
        "#     sess = tf.get_default_session()\n",
        "#     for offset in range(0, num_examples, BATCH_SIZE):\n",
        "#         batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "#         accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, is_testing: True})\n",
        "#         total_accuracy += (accuracy * len(batch_x))\n",
        "#     tot_acc = total_accuracy / num_examples\n",
        "#     with tf.name_scope('Accuracy'):\n",
        "#       tf.summary.scalar('accuracy', tot_acc)\n",
        "#     return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCovfr0E4oJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the mode"
      ]
    },
    {
      "metadata": {
        "id": "NGF2PRgw9nLL",
        "colab_type": "code",
        "outputId": "0c9d1e30-c855-405f-a93f-4c2b44631b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "2056*2"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cl89uCj9hjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4112"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4H5Dgkf69TOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nk5Kihg685LI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQaEeNO285BK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VP4QWKJ4ODG",
        "colab_type": "code",
        "outputId": "2aecd78c-625e-49e9-c6c0-78b2eae7219f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8568
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = X_train.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "epoch_track = []\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          epoch_track.append(i)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "            saver.save(sess, './PendigitSGDBased')\n",
        "        \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.83325\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.1345745\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 99.39960\n",
            "0.021734525\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 99.39960\n",
            "0.0035102463\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 99.39960\n",
            "0.0005669242\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 99.46631\n",
            "9.156141e-05\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.4787675e-05\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.3882915e-06\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 99.53302\n",
            "3.8572227e-07\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 99.53302\n",
            "6.229629e-08\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.0061196e-08\n",
            "\n",
            "Train Accuracy = 99.94996\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.6249389e-09\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.6243663e-10\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 99.53302\n",
            "4.238497e-11\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 99.53302\n",
            "6.845409e-12\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.1055714e-12\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.7855591e-13\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.8837775e-14\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 99.53302\n",
            "4.6574606e-15\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 99.53302\n",
            "7.522058e-16\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 201 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.214854e-16\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 211 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.9620562e-17\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 221 ...\n",
            "Validation Accuracy = 99.53302\n",
            "3.1688293e-18\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 231 ...\n",
            "Validation Accuracy = 99.53302\n",
            "5.117835e-19\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 241 ...\n",
            "Validation Accuracy = 99.53302\n",
            "8.2655874e-20\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 251 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.33493824e-20\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 261 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.1559994e-21\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 271 ...\n",
            "Validation Accuracy = 99.53302\n",
            "3.4820585e-22\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 281 ...\n",
            "Validation Accuracy = 99.53302\n",
            "5.6237174e-23\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 291 ...\n",
            "Validation Accuracy = 99.53302\n",
            "9.0826155e-24\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 301 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.4668928e-24\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 311 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.369113e-25\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 321 ...\n",
            "Validation Accuracy = 99.53302\n",
            "3.8262487e-26\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 331 ...\n",
            "Validation Accuracy = 99.53302\n",
            "6.1796043e-27\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 341 ...\n",
            "Validation Accuracy = 99.53302\n",
            "9.980403e-28\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 351 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.6118906e-28\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 361 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.6032927e-29\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 371 ...\n",
            "Validation Accuracy = 99.53302\n",
            "4.2044616e-30\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 381 ...\n",
            "Validation Accuracy = 99.53302\n",
            "6.7904387e-31\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 391 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.0966934e-31\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 401 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.771221e-32\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 411 ...\n",
            "Validation Accuracy = 99.53302\n",
            "2.8606202e-33\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 421 ...\n",
            "Validation Accuracy = 99.53302\n",
            "4.6200606e-34\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 431 ...\n",
            "Validation Accuracy = 99.53302\n",
            "7.461655e-35\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 441 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.2050985e-35\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 451 ...\n",
            "Validation Accuracy = 99.53302\n",
            "1.946301e-36\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 461 ...\n",
            "Validation Accuracy = 99.53302\n",
            "3.1433837e-37\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 471 ...\n",
            "Validation Accuracy = 99.53302\n",
            "5.076739e-38\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 481 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 491 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 501 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 511 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 521 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 531 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 541 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 551 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 561 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 571 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 581 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 591 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 601 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 611 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 621 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 631 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 641 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 651 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 661 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 671 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 681 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 691 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 701 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 711 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 721 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 731 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 741 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 751 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 761 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 771 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 781 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 791 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 801 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 811 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 821 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 831 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 841 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 851 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 861 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 871 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 881 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 891 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 901 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 911 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 921 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 931 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 941 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 951 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 961 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 971 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 981 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Train Accuracy = 99.96664\n",
            "EPOCH 991 ...\n",
            "Validation Accuracy = 99.53302\n",
            "0.0\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zydEA48dDeNA",
        "colab_type": "code",
        "outputId": "cd2dc0e1-b8e0-4007-c2a3-f24bd2956514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.argmax(validation_accuracy_track)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "oDevMH2N3JZu",
        "colab_type": "code",
        "outputId": "b99f5c5a-bd78-48e5-bbd1-20be44eeff09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "best_accuracy_valid"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.53302"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "metadata": {
        "id": "uhbuJEM0-sVm",
        "colab_type": "code",
        "outputId": "9d45b697-0819-44bc-bfa7-2c9382d142c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './PendigitSGDBased')\n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./PendigitSGDBased\n",
            "Validation Accuracy = 99.533020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D5okZYa4CSqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import spline\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ou2-UqDCXZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps_plot =  [step for step in range(0, 4861, print_every)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6G17qZY_Pxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3e9d6a0-a3ad-4e5e-cc50-01df0fd4a03f"
      },
      "cell_type": "code",
      "source": [
        "len(validation_accuracy_track)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "487"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "sNS-YE0XCg4s",
        "colab_type": "code",
        "outputId": "e61bfc0c-5752-46ef-d18e-07781d303fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "# plt.plot( savgol_filter(np.asarray(validation_accuracy_track),51,1))\n",
        "plt.plot( (validation_accuracy_track))\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff2aec86240>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nywAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BBlFJwfn-45J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b51de64d-0456-40b1-d606-9b1dd47fbba5"
      },
      "cell_type": "code",
      "source": [
        "len(steps_plot)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "230"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "MCKAiyEFCg2F",
        "colab_type": "code",
        "outputId": "e58b4336-b765-4b47-c043-5d87453ab3c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "steps_plot = epoch_track# [step for step in range(0, 2291, print_every)]\n",
        "# plt.plot(steps_plot, 100*np.asarray(train_accuracy))\n",
        "# plt.plot(steps_plot, val_accuracy)\n",
        "\n",
        "plt.plot(steps_plot, np.asarray(train_accuracy_track))  \n",
        "plt.plot(steps_plot, validation_accuracy_track)\n",
        "plt.tight_layout()\n",
        "# plt.xticks(np.arange(min(steps_plot), max(steps_plot)+1, 2000))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(abs)\n",
        "plt.show()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEYCAYAAAAwH9PuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u7hYf8U_CXWh",
        "colab_type": "code",
        "outputId": "1029ba54-eaca-46be-c262-f150177cfbe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "valid_accuracy_filtered = validation_accuracy_track\n",
        "print(max(valid_accuracy_filtered))\n",
        "valid_accuracy_filtered_np = np.asarray(valid_accuracy_filtered)\n",
        "print(np.argmax(valid_accuracy_filtered))\n",
        "print(steps_plot[np.argmax(valid_accuracy_filtered)])"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99.53302\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4lrwM4UUCXTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFCL8d7-CIxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now  retrain til 1900 Epochs"
      ]
    },
    {
      "metadata": {
        "id": "qoqMxUbZCIRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keep aside \n",
        "aside_examples= 100\n",
        "aside_valid_test = train_valid_combined[-aside_examples:]\n",
        "aside_valid_test_label = validation_test_label_one_hot[-aside_examples:]\n",
        "combined_train_valid = train_valid_combined[:train_valid_combined.shape[0]-aside_examples,:]\n",
        "combined_train_valid_label = validation_test_label_one_hot[:train_valid_combined.shape[0]-aside_examples,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5n7bYp9-FCn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puoBTQ3fCIOT",
        "colab_type": "code",
        "outputId": "42b356ef-908f-47ff-f6f3-6222483ab369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "validation_accuracy_track = []\n",
        "train_accuracy_track = []\n",
        "connection_probability_track = []\n",
        "number_of_ex = combined_train_valid.shape[0]\n",
        "total_steps_for_one_pass = number_of_ex//BATCH_SIZE + 1\n",
        "\n",
        "print_every = 10\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    best_accuracy_valid = 0\n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(combined_train_valid, combined_train_valid_label)\n",
        "        for step in range(0, total_steps_for_one_pass):        \n",
        "          if step>=number_of_ex//BATCH_SIZE:\n",
        "            batch_x, batch_y = X_train[step*BATCH_SIZE:,:],y_train[step*BATCH_SIZE:]\n",
        "#             print(step,'Finishing',step*BATCH_SIZE )\n",
        "            step = 0\n",
        "\n",
        "          else:\n",
        "\n",
        "            start = step*BATCH_SIZE\n",
        "            finish = (step+1)*BATCH_SIZE\n",
        "#             print(step,'Doing', 'Start = ', start, \"Finish = \", finish)\n",
        "            batch_x, batch_y = X_train[step:finish,:],y_train[step:finish]\n",
        "#           print(batch_y.shape)\n",
        "          tr_op  = sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, is_testing : False})\n",
        "#           train_writer.add_summary(summary_tr, i)\n",
        "        prob = sess.run(new_prob)\n",
        "        if i%print_every == 0:\n",
        "          tr_accuracy = sess.run(accuracy*100, feed_dict={x: X_train,y:y_train, is_testing: True})  # evaluate(X_train, y_train)\n",
        "          print(\"Train Accuracy = {:.5f}\".format(tr_accuracy))\n",
        "          validation_accuracy = sess.run(accuracy*100, feed_dict={x: aside_valid_test,y:aside_valid_test_label, is_testing: True}) #evaluate(X_validation, y_validation)\n",
        "          validation_accuracy_track.append(validation_accuracy)\n",
        "          train_accuracy_track.append(tr_accuracy)\n",
        "          connection_probability_track.append(prob)\n",
        "          print(\"EPOCH {} ...\".format(i+1))\n",
        "          print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
        "          print(prob)\n",
        "          print()\n",
        "          if (validation_accuracy >= best_accuracy_valid):\n",
        "            best_accuracy_valid = validation_accuracy\n",
        "#             saver.save(sess, './PendigitSGDBased')\n",
        "    saver.save(sess, './PendigitAdamBasedAllPass')   \n",
        "#     saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.83325\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.1345745\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.021734525\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0035102463\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 100.00000\n",
            "0.0005669242\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 51 ...\n",
            "Validation Accuracy = 100.00000\n",
            "9.156141e-05\n",
            "\n",
            "Train Accuracy = 99.86475\n",
            "EPOCH 61 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.4787675e-05\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 71 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.3882915e-06\n",
            "\n",
            "Train Accuracy = 99.87828\n",
            "EPOCH 81 ...\n",
            "Validation Accuracy = 100.00000\n",
            "3.8572227e-07\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 91 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.229629e-08\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 101 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.0061196e-08\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 111 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.6249389e-09\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 121 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.6243663e-10\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 131 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.238497e-11\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 141 ...\n",
            "Validation Accuracy = 100.00000\n",
            "6.845409e-12\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 151 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.1055714e-12\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 161 ...\n",
            "Validation Accuracy = 100.00000\n",
            "1.7855591e-13\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 171 ...\n",
            "Validation Accuracy = 100.00000\n",
            "2.8837775e-14\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 181 ...\n",
            "Validation Accuracy = 100.00000\n",
            "4.6574606e-15\n",
            "\n",
            "Train Accuracy = 99.89181\n",
            "EPOCH 191 ...\n",
            "Validation Accuracy = 100.00000\n",
            "7.522058e-16\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ImtSTCNJF9E_",
        "colab_type": "code",
        "outputId": "e7938423-281b-48b8-e5ec-0a0a30fbc854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './PendigitAdamBasedAllPass')\n",
        "    saver.save(sess, './PendigitMomentBasedAllPass')  \n",
        "    validation_accuracy = sess.run(accuracy*100, feed_dict={x: validation_data,y:validation_label_one_hot, is_testing: True})\n",
        "    print(\"Validation Accuracy = {:.6f}\".format(validation_accuracy))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./PendigitAdamBasedAllPass\n",
            "Validation Accuracy = 99.599731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FVMAeXpCIL8",
        "colab_type": "code",
        "outputId": "f3df2789-47c4-4532-e07f-0933987e56c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Without all pass\n",
        "with tf.Session() as sess:\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    saver.restore(sess, './PendigitMomentBasedAllPass')\n",
        "    test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "    print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./PendigitMomentBasedAllPass\n",
            "Test Accuracy = 97.398514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VGUWHQR3CIJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HXCpiAuCIGT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGYzlt7KCIDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hr57zkSqCIAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q08ZXCdr-sTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Without all pass\n",
        "# with tf.Session() as sess:\n",
        "# #     saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "#     saver.restore(sess, './PendigitSGDBased')\n",
        "#     test_accuracy = sess.run(accuracy*100, feed_dict={x: test_data,y:test_label_one_hot, is_testing: True})\n",
        "#     print(\"Test Accuracy = {:.6f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9z1P1DG-sQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW6V7O1e-sNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochTrack = epoch_track\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r7lCgbXR3JXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sio.savemat('PendigitFullDatasetMoment_ProbabilityBasedValid97p4.mat', {'ValidationTracked':validation_accuracy_track,\n",
        "                                       'train_accuracy_track':train_accuracy_track,\n",
        "                                       'connection_probability_track':connection_probability_track,\n",
        "                                       'epochTrack':epochTrack, 'TestAcc':test_accuracy,\n",
        "                                                         'BestValidation':best_accuracy_valid})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFfzfjOB3JTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}